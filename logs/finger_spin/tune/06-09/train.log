{"episode_reward": 0.0, "episode": 1.0, "duration": 50.26811218261719, "step": 500}
{"episode_reward": 3.0, "episode": 2.0, "duration": 2.0272469520568848, "step": 1000}
{"episode_reward": 11.0, "episode": 3.0, "Q1 loss": 0.04506535757904203, "Q2 loss": 0.04525975558219943, "Mean Target Q": 0.05381711678917054, "Mean Q1": 0.05472707162707593, "Mean Q2": 0.054979626921051024, "critic_loss": 0.09622967966226861, "batch_reward": 0.0155390625, "actor_loss": -0.17380485222581774, "actor_target_entropy": -2.0, "actor_entropy": 2.5222743701934816, "alpha_loss": 0.3209393385797739, "alpha_value": 0.09876965060254794, "duration": 2502.0700104236603, "step": 1500}
{"episode_reward": 6.0, "episode": 4.0, "Q1 loss": 0.024695092265093047, "Q2 loss": 0.024359005557906932, "Mean Target Q": 0.39920469411622256, "Mean Q1": 0.39523189418497784, "Mean Q2": 0.3959027337463063, "critic_loss": 0.051197200732305645, "batch_reward": 0.013171875, "actor_loss": -0.5515376036167144, "actor_target_entropy": -2.0, "actor_entropy": 2.6008409996032715, "alpha_loss": 0.3228040755987167, "alpha_value": 0.09632141048120489, "duration": 1566.8312630653381, "step": 2000}
{"episode_reward": 4.0, "episode": 5.0, "Q1 loss": 0.02003145659488759, "Q2 loss": 0.019584270064536082, "Mean Target Q": 0.6933919718435833, "Mean Q1": 0.6920647064532552, "Mean Q2": 0.6933178646905082, "critic_loss": 0.04333456573216245, "batch_reward": 0.0105546875, "actor_loss": -0.8315003087520599, "actor_target_entropy": -2.0, "actor_entropy": 2.6018593769073486, "alpha_loss": 0.31515315878391265, "alpha_value": 0.09398321276940501, "duration": 1138.8093693256378, "step": 2500}
{"episode_reward": 0.0, "episode": 6.0, "Q1 loss": 0.01688636124057562, "Q2 loss": 0.016722599900227838, "Mean Target Q": 0.9605229899202075, "Mean Q1": 0.9602260449784142, "Mean Q2": 0.9602620750495365, "critic_loss": 0.03915949316136539, "batch_reward": 0.00889453125, "actor_loss": -1.0817384779453278, "actor_target_entropy": -2.0, "actor_entropy": 2.6210792388916015, "alpha_loss": 0.307078875541687, "alpha_value": 0.0917289302818763, "duration": 1129.666448354721, "step": 3000}
{"episode_reward": 0.0, "episode": 7.0, "Q1 loss": 0.01890656406319301, "Q2 loss": 0.01889465874466779, "Mean Target Q": 1.2066662644318171, "Mean Q1": 1.2064362813064031, "Mean Q2": 1.2065135718413762, "critic_loss": 0.04573704328015447, "batch_reward": 0.0097734375, "actor_loss": -1.324037570476532, "actor_target_entropy": -2.0, "actor_entropy": 2.6198236465454103, "alpha_loss": 0.29870303356647493, "alpha_value": 0.08955695233966765, "duration": 1123.0324430465698, "step": 3500}
{"episode_reward": 18.0, "episode": 8.0, "Q1 loss": 0.0247285222599418, "Q2 loss": 0.024876279774495056, "Mean Target Q": 1.4181334193572637, "Mean Q1": 1.4176865001958252, "Mean Q2": 1.4178107100706372, "critic_loss": 0.06440928287804126, "batch_reward": 0.01328125, "actor_loss": -1.5354712476730348, "actor_target_entropy": -2.0, "actor_entropy": 2.620813464164734, "alpha_loss": 0.29038295257091523, "alpha_value": 0.0874580923393701, "duration": 1065.5087356567383, "step": 4000}
{"episode_reward": 8.0, "episode": 9.0, "Q1 loss": 0.028707925064799685, "Q2 loss": 0.028874473699368537, "Mean Target Q": 1.6054421724478403, "Mean Q1": 1.6069496165116628, "Mean Q2": 1.6070682294766108, "critic_loss": 0.08039819091185928, "batch_reward": 0.012203125, "actor_loss": -1.7181605505943298, "actor_target_entropy": -2.0, "actor_entropy": 2.635332068443298, "alpha_loss": 0.28090690112113953, "alpha_value": 0.08543092593903075, "duration": 1080.4256806373596, "step": 4500}
{"episode_reward": 3.0, "episode": 10.0, "Q1 loss": 0.044236219650134447, "Q2 loss": 0.04446679613133892, "Mean Target Q": 1.7905494903326034, "Mean Q1": 1.7937612691720326, "Mean Q2": 1.7935033797025681, "critic_loss": 0.12451809974387289, "batch_reward": 0.0135703125, "actor_loss": -1.9100066223144532, "actor_target_entropy": -2.0, "actor_entropy": 2.6178575944900513, "alpha_loss": 0.2646641098856926, "alpha_value": 0.083498574364412, "step": 5000}
{"duration": 1509.271241426468, "step": 5000}
{"episode_reward": 19.0, "episode": 11.0, "Q1 loss": 0.07385364485139469, "Q2 loss": 0.07377602958650066, "Mean Target Q": 1.98534975303867, "Mean Q1": 1.9885395235947017, "Mean Q2": 1.988614178746881, "critic_loss": 0.18945740133523942, "batch_reward": 0.01675, "actor_loss": -2.1185014233589174, "actor_target_entropy": -2.0, "actor_entropy": 2.593144751548767, "alpha_loss": 0.24844358032941818, "alpha_value": 0.08167280571220467, "duration": 1101.4768002033234, "step": 5500}
{"episode_reward": 29.0, "episode": 12.0, "Q1 loss": 0.10254061468541623, "Q2 loss": 0.10200247838199139, "Mean Target Q": 2.2176086470603944, "Mean Q1": 2.220081973409653, "Mean Q2": 2.2201728751182555, "critic_loss": 0.25036841291189194, "batch_reward": 0.0212890625, "actor_loss": -2.3471265153884886, "actor_target_entropy": -2.0, "actor_entropy": 2.5641471099853517, "alpha_loss": 0.2343112461566925, "alpha_value": 0.07993178368481234, "duration": 1028.9637157917023, "step": 6000}
{"episode_reward": 64.0, "episode": 13.0, "Q1 loss": 0.14548641287088393, "Q2 loss": 0.1460522733569145, "Mean Target Q": 2.5489056261062624, "Mean Q1": 2.549657183456421, "Mean Q2": 2.5495285177230835, "critic_loss": 0.34168069875240326, "batch_reward": 0.0359765625, "actor_loss": -2.6781384744644163, "actor_target_entropy": -2.0, "actor_entropy": 2.484460165977478, "alpha_loss": 0.21410543459653855, "alpha_value": 0.07828348116081293, "duration": 1031.2155697345734, "step": 6500}
{"episode_reward": 115.0, "episode": 14.0, "Q1 loss": 0.1862909554183483, "Q2 loss": 0.1869442396581173, "Mean Target Q": 2.997341816329956, "Mean Q1": 2.998308800125122, "Mean Q2": 2.9981919598579405, "critic_loss": 0.43201439249515533, "batch_reward": 0.0511171875, "actor_loss": -3.1452685718536375, "actor_target_entropy": -2.0, "actor_entropy": 2.3631223640441896, "alpha_loss": 0.18352812105417252, "alpha_value": 0.07678396675209466, "duration": 1044.8135960102081, "step": 7000}
{"episode_reward": 135.0, "episode": 15.0, "Q1 loss": 0.22934961255788802, "Q2 loss": 0.2295283841729164, "Mean Target Q": 3.601582763671875, "Mean Q1": 3.60154976606369, "Mean Q2": 3.6019101870536803, "critic_loss": 0.5243721808195114, "batch_reward": 0.07087890625, "actor_loss": -3.806666793823242, "actor_target_entropy": -2.0, "actor_entropy": 2.1981960968971253, "alpha_loss": 0.14280755731463432, "alpha_value": 0.07549411147163627, "duration": 1050.9923083782196, "step": 7500}
{"episode_reward": 266.0, "episode": 16.0, "Q1 loss": 0.30985753144621847, "Q2 loss": 0.3110335204541683, "Mean Target Q": 4.415374145889282, "Mean Q1": 4.415071364402771, "Mean Q2": 4.414949977779388, "critic_loss": 0.7040600923299789, "batch_reward": 0.10962890625, "actor_loss": -4.649067113876343, "actor_target_entropy": -2.0, "actor_entropy": 2.0678640542030333, "alpha_loss": 0.10936878469586372, "alpha_value": 0.07444760426388893, "duration": 992.7099788188934, "step": 8000}
{"episode_reward": 409.0, "episode": 17.0, "Q1 loss": 0.43946436891555785, "Q2 loss": 0.43860829959511755, "Mean Target Q": 5.523304961585999, "Mean Q1": 5.523133402633667, "Mean Q2": 5.523640703010559, "critic_loss": 0.9896874336004258, "batch_reward": 0.15646484375, "actor_loss": -5.820515693664551, "actor_target_entropy": -2.0, "actor_entropy": 1.933368896007538, "alpha_loss": 0.07345160396397114, "alpha_value": 0.07361500613226425, "duration": 985.2541646957397, "step": 8500}
{"episode_reward": 436.0, "episode": 18.0, "Q1 loss": 0.5868349983215332, "Q2 loss": 0.5852168069720268, "Mean Target Q": 6.9384500087738035, "Mean Q1": 6.933466983985901, "Mean Q2": 6.933374517822266, "critic_loss": 1.3134053591489792, "batch_reward": 0.2037421875, "actor_loss": -7.2963602848052975, "actor_target_entropy": -2.0, "actor_entropy": 1.7952833366394043, "alpha_loss": 0.04168677099794149, "alpha_value": 0.07304333106816317, "duration": 989.0660333633423, "step": 9000}
{"episode_reward": 549.0, "episode": 19.0, "Q1 loss": 0.6745774613022805, "Q2 loss": 0.6742049716711044, "Mean Target Q": 8.694514198303223, "Mean Q1": 8.687803530311584, "Mean Q2": 8.68843794631958, "critic_loss": 1.5135080835819243, "batch_reward": 0.2575, "actor_loss": -9.121983079910278, "actor_target_entropy": -2.0, "actor_entropy": 1.6720281648635864, "alpha_loss": 0.019979711153544487, "alpha_value": 0.07270340428093436, "duration": 977.4583344459534, "step": 9500}
{"episode_reward": 604.0, "episode": 20.0, "Q1 loss": 0.7275632189750671, "Q2 loss": 0.7243565176248551, "Mean Target Q": 10.777152596664429, "Mean Q1": 10.770441189956665, "Mean Q2": 10.770583269500733, "critic_loss": 1.6194212517738342, "batch_reward": 0.306625, "actor_loss": -11.231145572662353, "actor_target_entropy": -2.0, "actor_entropy": 1.6004556813240052, "alpha_loss": 0.0015600674429442733, "alpha_value": 0.07258219763883679, "step": 10000}
{"duration": 857.75399518013, "step": 10000}
{"episode_reward": 594.0, "episode": 21.0, "Q1 loss": 0.7351675384163856, "Q2 loss": 0.7324693416595459, "Mean Target Q": 13.00563243522644, "Mean Q1": 12.994550394821166, "Mean Q2": 12.994818682098389, "critic_loss": 1.642804563999176, "batch_reward": 0.35176953125, "actor_loss": -13.487529731750488, "actor_target_entropy": -2.0, "actor_entropy": 1.505565890312195, "alpha_loss": -0.006347540669608861, "alpha_value": 0.07262107556662376, "duration": 674.7668845653534, "step": 10500}
{"episode_reward": 613.0, "episode": 22.0, "Q1 loss": 0.747713335108757, "Q2 loss": 0.747550833082199, "Mean Target Q": 15.322444411849975, "Mean Q1": 15.31562259979248, "Mean Q2": 15.316001044845581, "critic_loss": 1.669113677740097, "batch_reward": 0.39283984375, "actor_loss": -15.826640132904053, "actor_target_entropy": -2.0, "actor_entropy": 1.4765281085968018, "alpha_loss": -0.010951651958515867, "alpha_value": 0.07277018162936263, "duration": 674.0409469604492, "step": 11000}
{"episode_reward": 658.0, "episode": 23.0, "Q1 loss": 0.7569095312833786, "Q2 loss": 0.7560413531064987, "Mean Target Q": 17.634902487182618, "Mean Q1": 17.622986890029907, "Mean Q2": 17.62320351371765, "critic_loss": 1.700691098690033, "batch_reward": 0.43185546875, "actor_loss": -18.124416450500487, "actor_target_entropy": -2.0, "actor_entropy": 1.4423287715911866, "alpha_loss": -0.013746060580713674, "alpha_value": 0.07300049315345798, "duration": 666.3698346614838, "step": 11500}
{"episode_reward": 678.0, "episode": 24.0, "Q1 loss": 0.7605244595050812, "Q2 loss": 0.7603056004285812, "Mean Target Q": 20.00328183746338, "Mean Q1": 19.995328929138182, "Mean Q2": 19.99538196334839, "critic_loss": 1.709113174676895, "batch_reward": 0.47364453125, "actor_loss": -20.485827682495117, "actor_target_entropy": -2.0, "actor_entropy": 1.4071495900154114, "alpha_loss": -0.01706256231246516, "alpha_value": 0.07332156281306702, "duration": 680.229817867279, "step": 12000}
{"episode_reward": 695.0, "episode": 25.0, "Q1 loss": 0.7517869857430458, "Q2 loss": 0.7527728251934052, "Mean Target Q": 22.38670376358032, "Mean Q1": 22.374669465637208, "Mean Q2": 22.374360109710693, "critic_loss": 1.6935495576858521, "batch_reward": 0.5124140625, "actor_loss": -22.858500747680665, "actor_target_entropy": -2.0, "actor_entropy": 1.4014836683273315, "alpha_loss": -0.021136716457083822, "alpha_value": 0.0737885615195162, "duration": 669.405757188797, "step": 12500}
{"episode_reward": 705.0, "episode": 26.0, "Q1 loss": 0.7335423256993294, "Q2 loss": 0.7324570791840553, "Mean Target Q": 24.785873973846435, "Mean Q1": 24.77592915496826, "Mean Q2": 24.775258227539062, "critic_loss": 1.6561155555248261, "batch_reward": 0.54516015625, "actor_loss": -25.248394721984862, "actor_target_entropy": -2.0, "actor_entropy": 1.3778378772735596, "alpha_loss": -0.02323787557473406, "alpha_value": 0.07438544128593857, "duration": 669.2284209728241, "step": 13000}
{"episode_reward": 691.0, "episode": 27.0, "Q1 loss": 0.7305823790192604, "Q2 loss": 0.7292336608886719, "Mean Target Q": 27.136722677612305, "Mean Q1": 27.126007090759277, "Mean Q2": 27.125941439056398, "critic_loss": 1.6522359237670898, "batch_reward": 0.57610546875, "actor_loss": -27.57172177886963, "actor_target_entropy": -2.0, "actor_entropy": 1.3340457005500794, "alpha_loss": -0.021380256628151982, "alpha_value": 0.07505054000275933, "duration": 662.4480249881744, "step": 13500}
{"episode_reward": 725.0, "episode": 28.0, "Q1 loss": 0.731435365653038, "Q2 loss": 0.7311880616426468, "Mean Target Q": 29.463866384887694, "Mean Q1": 29.45516247406006, "Mean Q2": 29.45517664260864, "critic_loss": 1.659495747089386, "batch_reward": 0.6106953125, "actor_loss": -29.901294006347655, "actor_target_entropy": -2.0, "actor_entropy": 1.3189878597259521, "alpha_loss": -0.024863522433675827, "alpha_value": 0.0758385284674338, "duration": 732.1123044490814, "step": 14000}
{"episode_reward": 722.0, "episode": 29.0, "Q1 loss": 0.6995006330251694, "Q2 loss": 0.6995332524895668, "Mean Target Q": 31.777543574523925, "Mean Q1": 31.76435298843384, "Mean Q2": 31.764638136291502, "critic_loss": 1.5819232869148254, "batch_reward": 0.64051953125, "actor_loss": -32.215605186462405, "actor_target_entropy": -2.0, "actor_entropy": 1.3014430928230285, "alpha_loss": -0.021202567826025188, "alpha_value": 0.07671493155366536, "duration": 661.6530861854553, "step": 14500}
{"episode_reward": 734.0, "episode": 30.0, "Q1 loss": 0.7059151530742646, "Q2 loss": 0.7050429921507836, "Mean Target Q": 34.100934733581546, "Mean Q1": 34.08900714263916, "Mean Q2": 34.08906907196045, "critic_loss": 1.597024111509323, "batch_reward": 0.66833984375, "actor_loss": -34.54358116149902, "actor_target_entropy": -2.0, "actor_entropy": 1.283310224056244, "alpha_loss": -0.0223018094021827, "alpha_value": 0.0776037522650401, "step": 15000}
{"duration": 674.7287628650665, "step": 15000}
{"episode_reward": 769.0, "episode": 31.0, "Q1 loss": 0.6934999196410179, "Q2 loss": 0.6916496722579002, "Mean Target Q": 36.435376707458495, "Mean Q1": 36.423392839050294, "Mean Q2": 36.42352108612061, "critic_loss": 1.572002321958542, "batch_reward": 0.69777734375, "actor_loss": -36.86042294311523, "actor_target_entropy": -2.0, "actor_entropy": 1.251615525484085, "alpha_loss": -0.023229845979250967, "alpha_value": 0.07865419838754595, "duration": 658.9419858455658, "step": 15500}
{"episode_reward": 772.0, "episode": 32.0, "Q1 loss": 0.6806871411323547, "Q2 loss": 0.6785570868849754, "Mean Target Q": 38.727014129638675, "Mean Q1": 38.718548483276365, "Mean Q2": 38.71876564178467, "critic_loss": 1.5437476172447204, "batch_reward": 0.72559765625, "actor_loss": -39.11306327819824, "actor_target_entropy": -2.0, "actor_entropy": 1.2483022565841675, "alpha_loss": -0.023276172950398178, "alpha_value": 0.07984103562642232, "duration": 720.911411523819, "step": 16000}
{"episode_reward": 761.0, "episode": 33.0, "Q1 loss": 0.6697199618697166, "Q2 loss": 0.6700257501244545, "Mean Target Q": 41.020887057495116, "Mean Q1": 41.01182814025879, "Mean Q2": 41.01253159484863, "critic_loss": 1.5233788595199584, "batch_reward": 0.75046484375, "actor_loss": -41.39267001342773, "actor_target_entropy": -2.0, "actor_entropy": 1.2394318103790283, "alpha_loss": -0.020406997401732952, "alpha_value": 0.08103730962822968, "duration": 771.9008781909943, "step": 16500}
{"episode_reward": 766.0, "episode": 34.0, "Q1 loss": 0.6551442826509476, "Q2 loss": 0.6552885368943214, "Mean Target Q": 43.25862166595459, "Mean Q1": 43.24794158630371, "Mean Q2": 43.24757733154297, "critic_loss": 1.497040268421173, "batch_reward": 0.77250390625, "actor_loss": -43.61931475830078, "actor_target_entropy": -2.0, "actor_entropy": 1.2236738948822021, "alpha_loss": -0.01973039632057771, "alpha_value": 0.08225296646135516, "duration": 890.6843073368073, "step": 17000}
{"episode_reward": 749.0, "episode": 35.0, "Q1 loss": 0.6409346684336662, "Q2 loss": 0.6377139046669006, "Mean Target Q": 45.479620878601075, "Mean Q1": 45.472570016479494, "Mean Q2": 45.47205285491943, "critic_loss": 1.4514712443351745, "batch_reward": 0.79265234375, "actor_loss": -45.83975506591797, "actor_target_entropy": -2.0, "actor_entropy": 1.218945813179016, "alpha_loss": -0.019603757033590227, "alpha_value": 0.0835161641334528, "duration": 719.5311710834503, "step": 17500}
{"episode_reward": 778.0, "episode": 36.0, "Q1 loss": 0.6178309386014939, "Q2 loss": 0.6161940593481063, "Mean Target Q": 47.62930449829101, "Mean Q1": 47.61717381439209, "Mean Q2": 47.61868021087646, "critic_loss": 1.4147866778373719, "batch_reward": 0.81645703125, "actor_loss": -47.95429751586914, "actor_target_entropy": -2.0, "actor_entropy": 1.2009009585380555, "alpha_loss": -0.017269572705263272, "alpha_value": 0.08482122384535781, "duration": 666.4315223693848, "step": 18000}
{"episode_reward": 742.0, "episode": 37.0, "Q1 loss": 0.6126949936151505, "Q2 loss": 0.6104159772515297, "Mean Target Q": 49.7278119430542, "Mean Q1": 49.71997899932861, "Mean Q2": 49.72011510620117, "critic_loss": 1.3876874661445617, "batch_reward": 0.83185546875, "actor_loss": -50.055441925048825, "actor_target_entropy": -2.0, "actor_entropy": 1.2055906987190246, "alpha_loss": -0.016193635850213467, "alpha_value": 0.08604637937138734, "duration": 673.5189309120178, "step": 18500}
{"episode_reward": 798.0, "episode": 38.0, "Q1 loss": 0.6198070212006569, "Q2 loss": 0.6191956444382668, "Mean Target Q": 51.815036193847654, "Mean Q1": 51.80414409179688, "Mean Q2": 51.80353118896485, "critic_loss": 1.4197846472263336, "batch_reward": 0.8529609375, "actor_loss": -52.105274856567384, "actor_target_entropy": -2.0, "actor_entropy": 1.194582189798355, "alpha_loss": -0.01574042360857129, "alpha_value": 0.08734945083523046, "duration": 751.8573272228241, "step": 19000}
{"episode_reward": 761.0, "episode": 39.0, "Q1 loss": 0.589949963581562, "Q2 loss": 0.5902424782752991, "Mean Target Q": 53.870886784362796, "Mean Q1": 53.86241040039062, "Mean Q2": 53.86124402313232, "critic_loss": 1.3385672051906585, "batch_reward": 0.87033984375, "actor_loss": -54.174370681762696, "actor_target_entropy": -2.0, "actor_entropy": 1.1948504829406739, "alpha_loss": -0.011859398048371076, "alpha_value": 0.08858458493557642, "duration": 709.8940827846527, "step": 19500}
{"episode_reward": 773.0, "episode": 40.0, "Q1 loss": 0.5758980505943299, "Q2 loss": 0.5751245485663414, "Mean Target Q": 55.897590806579586, "Mean Q1": 55.889376095581056, "Mean Q2": 55.88977491912842, "critic_loss": 1.3152986335754395, "batch_reward": 0.890328125, "actor_loss": -56.18135461425781, "actor_target_entropy": -2.0, "actor_entropy": 1.1982371177673339, "alpha_loss": -0.011257103344425558, "alpha_value": 0.08960000603726417, "step": 20000}
{"duration": 930.5664582252502, "step": 20000}
{"episode_reward": 778.0, "episode": 41.0, "Q1 loss": 0.5771308309316635, "Q2 loss": 0.5749238669872284, "Mean Target Q": 57.84784027709961, "Mean Q1": 57.8381068069458, "Mean Q2": 57.83744679412842, "critic_loss": 1.3218304228782654, "batch_reward": 0.90059765625, "actor_loss": -58.127420578002926, "actor_target_entropy": -2.0, "actor_entropy": 1.2106124711036683, "alpha_loss": -0.00918268794985488, "alpha_value": 0.09069138608284137, "duration": 661.518238067627, "step": 20500}
{"episode_reward": 754.0, "episode": 42.0, "Q1 loss": 0.5751311639666558, "Q2 loss": 0.5730518646836281, "Mean Target Q": 59.76386082305908, "Mean Q1": 59.75285265960694, "Mean Q2": 59.75192160949707, "critic_loss": 1.3193921843767167, "batch_reward": 0.91998046875, "actor_loss": -59.98415205383301, "actor_target_entropy": -2.0, "actor_entropy": 1.2049427108764648, "alpha_loss": -0.007309486886020749, "alpha_value": 0.09159286426370186, "duration": 660.8061196804047, "step": 21000}
{"episode_reward": 797.0, "episode": 43.0, "Q1 loss": 0.5824932646751404, "Q2 loss": 0.5802179698467255, "Mean Target Q": 61.58510419158936, "Mean Q1": 61.575534159851074, "Mean Q2": 61.57592838897705, "critic_loss": 1.3333255298137665, "batch_reward": 0.9351875, "actor_loss": -61.856215545654294, "actor_target_entropy": -2.0, "actor_entropy": 1.181101279258728, "alpha_loss": -0.00805248671490699, "alpha_value": 0.09252589943401494, "duration": 661.5690579414368, "step": 21500}
{"episode_reward": 755.0, "episode": 44.0, "Q1 loss": 0.5565218248486519, "Q2 loss": 0.5556048446893692, "Mean Target Q": 63.43834599761963, "Mean Q1": 63.426618286132815, "Mean Q2": 63.42685725097656, "critic_loss": 1.2754990141391753, "batch_reward": 0.95008203125, "actor_loss": -63.67058949279785, "actor_target_entropy": -2.0, "actor_entropy": 1.1647527680397034, "alpha_loss": -0.0049120045606978234, "alpha_value": 0.0933987805379985, "duration": 661.3736753463745, "step": 22000}
{"episode_reward": 792.0, "episode": 45.0, "Q1 loss": 0.5661769850611686, "Q2 loss": 0.5631500290632248, "Mean Target Q": 65.21402775878906, "Mean Q1": 65.20189621582031, "Mean Q2": 65.20211522216798, "critic_loss": 1.299028668642044, "batch_reward": 0.96189453125, "actor_loss": -65.41332376098633, "actor_target_entropy": -2.0, "actor_entropy": 1.1699361612796784, "alpha_loss": -0.0012743367617949843, "alpha_value": 0.09376285872105827, "duration": 661.3800082206726, "step": 22500}
{"episode_reward": 748.0, "episode": 46.0, "Q1 loss": 0.5559680990099907, "Q2 loss": 0.5536295072197914, "Mean Target Q": 66.95868217468262, "Mean Q1": 66.9485653289795, "Mean Q2": 66.94890162963867, "critic_loss": 1.2758002705574036, "batch_reward": 0.97487109375, "actor_loss": -67.19058853149414, "actor_target_entropy": -2.0, "actor_entropy": 1.156927453517914, "alpha_loss": -0.0027342626191675665, "alpha_value": 0.09398611985667202, "duration": 662.873761177063, "step": 23000}
{"episode_reward": 774.0, "episode": 47.0, "Q1 loss": 0.5490923353791237, "Q2 loss": 0.5480507748246193, "Mean Target Q": 68.64523863220215, "Mean Q1": 68.63593710021972, "Mean Q2": 68.635335546875, "critic_loss": 1.2632828956842423, "batch_reward": 0.9898984375, "actor_loss": -68.83472500610351, "actor_target_entropy": -2.0, "actor_entropy": 1.157555042743683, "alpha_loss": -0.0031050735684111713, "alpha_value": 0.09446021278468163, "duration": 650.7963154315948, "step": 23500}
{"episode_reward": 784.0, "episode": 48.0, "Q1 loss": 0.5514654019474983, "Q2 loss": 0.5464631125688553, "Mean Target Q": 70.29731335449219, "Mean Q1": 70.28610557250977, "Mean Q2": 70.28619956359863, "critic_loss": 1.2696156774759293, "batch_reward": 0.99801953125, "actor_loss": -70.51528802490235, "actor_target_entropy": -2.0, "actor_entropy": 1.1510553698539734, "alpha_loss": -0.002800572417676449, "alpha_value": 0.09503483217392503, "duration": 651.6294224262238, "step": 24000}
{"episode_reward": 787.0, "episode": 49.0, "Q1 loss": 0.5474343212842941, "Q2 loss": 0.5462162766337395, "Mean Target Q": 71.94148160400391, "Mean Q1": 71.93110889587402, "Mean Q2": 71.93021177062988, "critic_loss": 1.2496751977205276, "batch_reward": 1.0156796875, "actor_loss": -72.15449865722657, "actor_target_entropy": -2.0, "actor_entropy": 1.1343167619705201, "alpha_loss": 0.0011400172696448862, "alpha_value": 0.09513500488451428, "duration": 652.530731678009, "step": 24500}
{"episode_reward": 805.0, "episode": 50.0, "Q1 loss": 0.540024288380146, "Q2 loss": 0.5372684794664383, "Mean Target Q": 73.5469358795166, "Mean Q1": 73.5378678314209, "Mean Q2": 73.5379168334961, "critic_loss": 1.2408169466257095, "batch_reward": 1.022953125, "actor_loss": -73.76228442382812, "actor_target_entropy": -2.0, "actor_entropy": 1.1248260312080383, "alpha_loss": -0.000804329477250576, "alpha_value": 0.09511094418878682, "step": 25000}
{"duration": 654.5112197399139, "step": 25000}
{"episode_reward": 808.0, "episode": 51.0, "Q1 loss": 0.5335426524758339, "Q2 loss": 0.5312611824274063, "Mean Target Q": 75.10685991210937, "Mean Q1": 75.09497367248535, "Mean Q2": 75.095433984375, "critic_loss": 1.230389728307724, "batch_reward": 1.03208203125, "actor_loss": -75.29352230834961, "actor_target_entropy": -2.0, "actor_entropy": 1.1223335506916046, "alpha_loss": 0.0023183845947496594, "alpha_value": 0.09498226534677613, "duration": 697.7358155250549, "step": 25500}
{"episode_reward": 797.0, "episode": 52.0, "Q1 loss": 0.5298786775827408, "Q2 loss": 0.526740436053276, "Mean Target Q": 76.6459304473877, "Mean Q1": 76.63692385253906, "Mean Q2": 76.63632127075195, "critic_loss": 1.2201458752155303, "batch_reward": 1.0473046875, "actor_loss": -76.82152734375, "actor_target_entropy": -2.0, "actor_entropy": 1.0920167021751404, "alpha_loss": 0.002344776741811074, "alpha_value": 0.09452267431799044, "duration": 651.6148352622986, "step": 26000}
{"episode_reward": 790.0, "episode": 53.0, "Q1 loss": 0.5158099373221398, "Q2 loss": 0.5123289658784866, "Mean Target Q": 78.16869924926758, "Mean Q1": 78.15733316345215, "Mean Q2": 78.15791141052246, "critic_loss": 1.1828460743427276, "batch_reward": 1.057671875, "actor_loss": -78.31975, "actor_target_entropy": -2.0, "actor_entropy": 1.093436797618866, "alpha_loss": 0.0005018501982558518, "alpha_value": 0.0942797369406444, "duration": 653.314325094223, "step": 26500}
{"episode_reward": 797.0, "episode": 54.0, "Q1 loss": 0.5213616822123528, "Q2 loss": 0.5210185505270958, "Mean Target Q": 79.6031094482422, "Mean Q1": 79.59472420043946, "Mean Q2": 79.59485930175781, "critic_loss": 1.2039355897903443, "batch_reward": 1.06288671875, "actor_loss": -79.78541418457031, "actor_target_entropy": -2.0, "actor_entropy": 1.0921076741218567, "alpha_loss": -0.0006951213583815843, "alpha_value": 0.09431148225296768, "duration": 652.8156003952026, "step": 27000}
{"episode_reward": 795.0, "episode": 55.0, "Q1 loss": 0.5085040937066079, "Q2 loss": 0.5082110808014869, "Mean Target Q": 81.10636461791992, "Mean Q1": 81.09446413574219, "Mean Q2": 81.0946668701172, "critic_loss": 1.1722911633253097, "batch_reward": 1.07510546875, "actor_loss": -81.28301577758789, "actor_target_entropy": -2.0, "actor_entropy": 1.0705531344413757, "alpha_loss": 0.0014352147122845054, "alpha_value": 0.09423426659402206, "duration": 651.867413520813, "step": 27500}
{"episode_reward": 810.0, "episode": 56.0, "Q1 loss": 0.5104267477869987, "Q2 loss": 0.5087232854127884, "Mean Target Q": 82.55296565246582, "Mean Q1": 82.5442962890625, "Mean Q2": 82.54417921447754, "critic_loss": 1.1749103133678436, "batch_reward": 1.08755078125, "actor_loss": -82.72113519287109, "actor_target_entropy": -2.0, "actor_entropy": 1.0508308212757111, "alpha_loss": 0.0001498046584893018, "alpha_value": 0.09412384707163808, "duration": 651.6342945098877, "step": 28000}
{"episode_reward": 823.0, "episode": 57.0, "Q1 loss": 0.4993665941119194, "Q2 loss": 0.4967446803689003, "Mean Target Q": 83.9447456085205, "Mean Q1": 83.93470731201172, "Mean Q2": 83.93491401977539, "critic_loss": 1.146889412045479, "batch_reward": 1.093328125, "actor_loss": -84.13249807739258, "actor_target_entropy": -2.0, "actor_entropy": 1.032704596042633, "alpha_loss": 0.001784501007758081, "alpha_value": 0.09383755155222212, "duration": 651.9614849090576, "step": 28500}
{"episode_reward": 809.0, "episode": 58.0, "Q1 loss": 0.5080658781647682, "Q2 loss": 0.5057963429927826, "Mean Target Q": 85.34231820678711, "Mean Q1": 85.33219315490723, "Mean Q2": 85.33165625, "critic_loss": 1.179429414153099, "batch_reward": 1.10283203125, "actor_loss": -85.49245272827149, "actor_target_entropy": -2.0, "actor_entropy": 1.017395482301712, "alpha_loss": 0.0005616534971632063, "alpha_value": 0.09364088416688728, "duration": 652.8192644119263, "step": 29000}
{"episode_reward": 793.0, "episode": 59.0, "Q1 loss": 0.4922297387003899, "Q2 loss": 0.49133653856515885, "Mean Target Q": 86.72292382202149, "Mean Q1": 86.71393395080567, "Mean Q2": 86.71328030395507, "critic_loss": 1.1362050931453704, "batch_reward": 1.1153984375, "actor_loss": -86.89276956176758, "actor_target_entropy": -2.0, "actor_entropy": 1.0080667753219605, "alpha_loss": 0.0010574837836902589, "alpha_value": 0.09349791048283246, "duration": 652.3877274990082, "step": 29500}
{"episode_reward": 820.0, "episode": 60.0, "Q1 loss": 0.5090873826384544, "Q2 loss": 0.5045062009334564, "Mean Target Q": 88.06802930603027, "Mean Q1": 88.05887099609375, "Mean Q2": 88.05797743530273, "critic_loss": 1.1707086814641952, "batch_reward": 1.12245703125, "actor_loss": -88.19660916137695, "actor_target_entropy": -2.0, "actor_entropy": 1.0100066547393798, "alpha_loss": 0.002751319775823504, "alpha_value": 0.09309649991468406, "step": 30000}
{"duration": 667.7820007801056, "step": 30000}
{"episode_reward": 843.0, "episode": 61.0, "Q1 loss": 0.4922593904733658, "Q2 loss": 0.48996275824308394, "Mean Target Q": 89.37454885253906, "Mean Q1": 89.36625797424317, "Mean Q2": 89.36614563293458, "critic_loss": 1.1380345433950425, "batch_reward": 1.13130078125, "actor_loss": -89.51796868896484, "actor_target_entropy": -2.0, "actor_entropy": 0.9778973615169525, "alpha_loss": 0.002391632375307381, "alpha_value": 0.09258192908652133, "duration": 652.835942029953, "step": 30500}
{"episode_reward": 832.0, "episode": 62.0, "Q1 loss": 0.47628841341733935, "Q2 loss": 0.47483711235523224, "Mean Target Q": 90.69983877563476, "Mean Q1": 90.68950515136719, "Mean Q2": 90.68931680297851, "critic_loss": 1.1101087846755981, "batch_reward": 1.139484375, "actor_loss": -90.83122161865235, "actor_target_entropy": -2.0, "actor_entropy": 0.963836888551712, "alpha_loss": -0.0007768730293028057, "alpha_value": 0.0924711507095654, "duration": 652.2720041275024, "step": 31000}
{"episode_reward": 847.0, "episode": 63.0, "Q1 loss": 0.4904054181098938, "Q2 loss": 0.4864576850533485, "Mean Target Q": 92.02504437866212, "Mean Q1": 92.01357724609375, "Mean Q2": 92.0140257598877, "critic_loss": 1.1343575428724288, "batch_reward": 1.1509921875, "actor_loss": -92.12826626586914, "actor_target_entropy": -2.0, "actor_entropy": 0.9433402149677277, "alpha_loss": -0.0008067292515188455, "alpha_value": 0.09262163090821085, "duration": 652.8332996368408, "step": 31500}
{"episode_reward": 828.0, "episode": 64.0, "Q1 loss": 0.46731520289182665, "Q2 loss": 0.46350199862718583, "Mean Target Q": 93.27204440002441, "Mean Q1": 93.26219710388183, "Mean Q2": 93.26129039611817, "critic_loss": 1.081353871703148, "batch_reward": 1.15718359375, "actor_loss": -93.40342025756836, "actor_target_entropy": -2.0, "actor_entropy": 0.9511246724128724, "alpha_loss": -0.0006761642643250524, "alpha_value": 0.09280903465776177, "duration": 652.0208234786987, "step": 32000}
{"episode_reward": 801.0, "episode": 65.0, "Q1 loss": 0.4771772920370102, "Q2 loss": 0.4755187404990196, "Mean Target Q": 94.55558841552734, "Mean Q1": 94.54387077331543, "Mean Q2": 94.54384355163575, "critic_loss": 1.0988107721805573, "batch_reward": 1.16717578125, "actor_loss": -94.67732452392578, "actor_target_entropy": -2.0, "actor_entropy": 0.9397777514457702, "alpha_loss": 0.00035144089115783574, "alpha_value": 0.09282357327088843, "duration": 651.8024778366089, "step": 32500}
{"episode_reward": 846.0, "episode": 66.0, "Q1 loss": 0.47329711973667143, "Q2 loss": 0.4715985109627247, "Mean Target Q": 95.74928336181641, "Mean Q1": 95.73888591613769, "Mean Q2": 95.7399820678711, "critic_loss": 1.1015272353887557, "batch_reward": 1.171859375, "actor_loss": -95.8301328125, "actor_target_entropy": -2.0, "actor_entropy": 0.9215724892616272, "alpha_loss": -0.001288807638688013, "alpha_value": 0.09285638720751266, "duration": 652.5053551197052, "step": 33000}
{"episode_reward": 831.0, "episode": 67.0, "Q1 loss": 0.4758569492101669, "Q2 loss": 0.47516408152580264, "Mean Target Q": 96.94723562316895, "Mean Q1": 96.93672293701172, "Mean Q2": 96.93729360961915, "critic_loss": 1.1049687530994414, "batch_reward": 1.1779921875, "actor_loss": -97.07930541992188, "actor_target_entropy": -2.0, "actor_entropy": 0.9365052514076233, "alpha_loss": 0.0013696812451817096, "alpha_value": 0.09286104790539894, "duration": 651.2451050281525, "step": 33500}
{"episode_reward": 834.0, "episode": 68.0, "Q1 loss": 0.488893835401535, "Q2 loss": 0.48545914857387545, "Mean Target Q": 98.07652010192871, "Mean Q1": 98.06727546081542, "Mean Q2": 98.06724659729004, "critic_loss": 1.1318488615751265, "batch_reward": 1.18379296875, "actor_loss": -98.15004037475586, "actor_target_entropy": -2.0, "actor_entropy": 0.8984419085979461, "alpha_loss": -0.0005294243060052395, "alpha_value": 0.0927450449625523, "duration": 652.3078362941742, "step": 34000}
{"episode_reward": 809.0, "episode": 69.0, "Q1 loss": 0.5128280315756798, "Q2 loss": 0.5076698803544044, "Mean Target Q": 99.23820228271484, "Mean Q1": 99.23099002990723, "Mean Q2": 99.22937293395997, "critic_loss": 1.1863056761026383, "batch_reward": 1.18983984375, "actor_loss": -99.31023147583008, "actor_target_entropy": -2.0, "actor_entropy": 0.913997364282608, "alpha_loss": 0.0014277696530334652, "alpha_value": 0.09260451397734079, "duration": 652.3238685131073, "step": 34500}
{"episode_reward": 839.0, "episode": 70.0, "Q1 loss": 0.4592095636367798, "Q2 loss": 0.46043469809293747, "Mean Target Q": 100.39311532287597, "Mean Q1": 100.38643416748047, "Mean Q2": 100.38640424804687, "critic_loss": 1.069984872698784, "batch_reward": 1.20403125, "actor_loss": -100.50294137573242, "actor_target_entropy": -2.0, "actor_entropy": 0.8793634235858917, "alpha_loss": -0.00032023932412266734, "alpha_value": 0.09249958642853395, "step": 35000}
{"duration": 668.6643586158752, "step": 35000}
{"episode_reward": 856.0, "episode": 71.0, "Q1 loss": 0.4708717004418373, "Q2 loss": 0.4705304193973541, "Mean Target Q": 101.49008842773438, "Mean Q1": 101.47770051574707, "Mean Q2": 101.47804327087402, "critic_loss": 1.104880230784416, "batch_reward": 1.20762109375, "actor_loss": -101.58055410766602, "actor_target_entropy": -2.0, "actor_entropy": 0.879004754781723, "alpha_loss": 0.0002463989183306694, "alpha_value": 0.09257655194017221, "duration": 652.1166684627533, "step": 35500}
{"episode_reward": 823.0, "episode": 72.0, "Q1 loss": 0.47332022894620895, "Q2 loss": 0.4694284638285637, "Mean Target Q": 102.57583898010255, "Mean Q1": 102.56655913085937, "Mean Q2": 102.5673927307129, "critic_loss": 1.1008624538183212, "batch_reward": 1.21409765625, "actor_loss": -102.65092727661133, "actor_target_entropy": -2.0, "actor_entropy": 0.8782396914958954, "alpha_loss": -0.0003142997492104769, "alpha_value": 0.09266932835682815, "duration": 653.1881155967712, "step": 36000}
{"episode_reward": 852.0, "episode": 73.0, "Q1 loss": 0.47592128130197525, "Q2 loss": 0.4780052190542221, "Mean Target Q": 103.62499383239746, "Mean Q1": 103.61466053161621, "Mean Q2": 103.61419242553711, "critic_loss": 1.1141813765764237, "batch_reward": 1.21940625, "actor_loss": -103.69127752685547, "actor_target_entropy": -2.0, "actor_entropy": 0.8816895055770874, "alpha_loss": 0.0007521355401258915, "alpha_value": 0.092502237932052, "duration": 652.9259622097015, "step": 36500}
{"episode_reward": 856.0, "episode": 74.0, "Q1 loss": 0.4830278702020645, "Q2 loss": 0.4821900157928467, "Mean Target Q": 104.67651555786132, "Mean Q1": 104.66597987670899, "Mean Q2": 104.66610315551758, "critic_loss": 1.149577179312706, "batch_reward": 1.22644921875, "actor_loss": -104.74694900512695, "actor_target_entropy": -2.0, "actor_entropy": 0.8580262141227722, "alpha_loss": 0.001976448973175138, "alpha_value": 0.0922634184615445, "duration": 652.8029637336731, "step": 37000}
{"episode_reward": 828.0, "episode": 75.0, "Q1 loss": 0.4632886412501335, "Q2 loss": 0.461924143075943, "Mean Target Q": 105.68434671936035, "Mean Q1": 105.67334622497559, "Mean Q2": 105.67244085388184, "critic_loss": 1.0899679651260377, "batch_reward": 1.2353125, "actor_loss": -105.69543075561523, "actor_target_entropy": -2.0, "actor_entropy": 0.842076101064682, "alpha_loss": 0.0022117036865092815, "alpha_value": 0.0919445178016371, "duration": 653.6577451229095, "step": 37500}
{"episode_reward": 841.0, "episode": 76.0, "Q1 loss": 0.4425913466215134, "Q2 loss": 0.4408473270833492, "Mean Target Q": 106.68133454895019, "Mean Q1": 106.67110274963379, "Mean Q2": 106.67098720092774, "critic_loss": 1.035479815363884, "batch_reward": 1.23844140625, "actor_loss": -106.73439086914063, "actor_target_entropy": -2.0, "actor_entropy": 0.8413844974040985, "alpha_loss": 0.0001789669436402619, "alpha_value": 0.09154077847702703, "duration": 653.3172569274902, "step": 38000}
{"episode_reward": 834.0, "episode": 77.0, "Q1 loss": 0.45978093553781507, "Q2 loss": 0.4574344444155693, "Mean Target Q": 107.70294692993164, "Mean Q1": 107.69526128540039, "Mean Q2": 107.69451147766114, "critic_loss": 1.0685907762050628, "batch_reward": 1.2450234375, "actor_loss": -107.72702438354492, "actor_target_entropy": -2.0, "actor_entropy": 0.830908456325531, "alpha_loss": -0.0005205791806802153, "alpha_value": 0.09156034273885919, "duration": 653.7282915115356, "step": 38500}
{"episode_reward": 848.0, "episode": 78.0, "Q1 loss": 0.4476118145048618, "Q2 loss": 0.4458040976166725, "Mean Target Q": 108.67067570495605, "Mean Q1": 108.66069168701172, "Mean Q2": 108.66040258483886, "critic_loss": 1.0468840305805207, "batch_reward": 1.25212109375, "actor_loss": -108.66824194335938, "actor_target_entropy": -2.0, "actor_entropy": 0.8245835819244385, "alpha_loss": -0.0010987895119469614, "alpha_value": 0.09170385411429419, "duration": 652.5679121017456, "step": 39000}
{"episode_reward": 854.0, "episode": 79.0, "Q1 loss": 0.4477850229024887, "Q2 loss": 0.4449741925418377, "Mean Target Q": 109.61107890319825, "Mean Q1": 109.60204067077636, "Mean Q2": 109.60177714538574, "critic_loss": 1.0344290549755097, "batch_reward": 1.25965234375, "actor_loss": -109.63112518310547, "actor_target_entropy": -2.0, "actor_entropy": 0.8153885462284088, "alpha_loss": -0.0006458847499452531, "alpha_value": 0.09206383182358319, "duration": 654.671629190445, "step": 39500}
{"episode_reward": 857.0, "episode": 80.0, "Q1 loss": 0.4460014492928982, "Q2 loss": 0.44236442122459413, "Mean Target Q": 110.55698125, "Mean Q1": 110.54833499450683, "Mean Q2": 110.54869185180664, "critic_loss": 1.0381255345344544, "batch_reward": 1.26235546875, "actor_loss": -110.60498065185547, "actor_target_entropy": -2.0, "actor_entropy": 0.8183240079879761, "alpha_loss": 0.0016813682832289487, "alpha_value": 0.09191673812410608, "step": 40000}
{"duration": 669.1682636737823, "step": 40000}
{"episode_reward": 830.0, "episode": 81.0, "Q1 loss": 0.4562266978442669, "Q2 loss": 0.4559342891931534, "Mean Target Q": 111.4913076385498, "Mean Q1": 111.48328161010743, "Mean Q2": 111.48344713134766, "critic_loss": 1.066849755525589, "batch_reward": 1.26821484375, "actor_loss": -111.54286148071289, "actor_target_entropy": -2.0, "actor_entropy": 0.7990555408000946, "alpha_loss": 0.0025582415885291995, "alpha_value": 0.09147727342633027, "duration": 653.4486730098724, "step": 40500}
{"episode_reward": 847.0, "episode": 82.0, "Q1 loss": 0.4417821590304375, "Q2 loss": 0.4391932970762253, "Mean Target Q": 112.40184900817871, "Mean Q1": 112.3967584564209, "Mean Q2": 112.39593435058593, "critic_loss": 1.0253791456222534, "batch_reward": 1.27349609375, "actor_loss": -112.39015002441407, "actor_target_entropy": -2.0, "actor_entropy": 0.796573356628418, "alpha_loss": 0.0008778172545135021, "alpha_value": 0.09108832095092066, "duration": 653.7992615699768, "step": 41000}
{"episode_reward": 843.0, "episode": 83.0, "Q1 loss": 0.4275691775023937, "Q2 loss": 0.42502422864437106, "Mean Target Q": 113.28805118408204, "Mean Q1": 113.28139713134766, "Mean Q2": 113.28076477355957, "critic_loss": 0.9978475626707077, "batch_reward": 1.27530859375, "actor_loss": -113.28047912597657, "actor_target_entropy": -2.0, "actor_entropy": 0.796190098285675, "alpha_loss": 0.0029356047729961576, "alpha_value": 0.0906703619606881, "duration": 974.4198617935181, "step": 41500}
{"episode_reward": 841.0, "episode": 84.0, "Q1 loss": 0.4331251369357109, "Q2 loss": 0.43093537541627885, "Mean Target Q": 114.19549807739257, "Mean Q1": 114.18966126403808, "Mean Q2": 114.1896112487793, "critic_loss": 1.00728038585186, "batch_reward": 1.2818828125, "actor_loss": -114.2106591796875, "actor_target_entropy": -2.0, "actor_entropy": 0.7888026642799377, "alpha_loss": 0.0007668069149367511, "alpha_value": 0.0902282692107827, "duration": 1154.7183656692505, "step": 42000}
{"episode_reward": 853.0, "episode": 85.0, "Q1 loss": 0.4171602651953697, "Q2 loss": 0.4149472338974476, "Mean Target Q": 115.12319776000976, "Mean Q1": 115.11404667053223, "Mean Q2": 115.11476401367187, "critic_loss": 0.9733691200017929, "batch_reward": 1.2873671875, "actor_loss": -115.11742007446288, "actor_target_entropy": -2.0, "actor_entropy": 0.7943870077133178, "alpha_loss": 0.001930849427357316, "alpha_value": 0.09005879655428199, "duration": 819.1842918395996, "step": 42500}
{"episode_reward": 882.0, "episode": 86.0, "Q1 loss": 0.4264891689240932, "Q2 loss": 0.4225981737673283, "Mean Target Q": 115.94179989624024, "Mean Q1": 115.93280526428222, "Mean Q2": 115.93277955627441, "critic_loss": 1.001241949915886, "batch_reward": 1.28928125, "actor_loss": -115.9741185913086, "actor_target_entropy": -2.0, "actor_entropy": 0.78718163895607, "alpha_loss": 0.0034286890081129968, "alpha_value": 0.08950084001261445, "duration": 739.7822732925415, "step": 43000}
{"episode_reward": 866.0, "episode": 87.0, "Q1 loss": 0.44790876721143724, "Q2 loss": 0.4490164386868477, "Mean Target Q": 116.74265341186523, "Mean Q1": 116.72993546447753, "Mean Q2": 116.72970422363281, "critic_loss": 1.0733052622079848, "batch_reward": 1.29784765625, "actor_loss": -116.83417150878907, "actor_target_entropy": -2.0, "actor_entropy": 0.7964859580993653, "alpha_loss": 0.0006045320024713874, "alpha_value": 0.08902159329326843, "duration": 667.3551781177521, "step": 43500}
{"episode_reward": 850.0, "episode": 88.0, "Q1 loss": 0.4246873600780964, "Q2 loss": 0.42297803557515146, "Mean Target Q": 117.58084300231934, "Mean Q1": 117.57077422180176, "Mean Q2": 117.57042214660645, "critic_loss": 0.9973516647815704, "batch_reward": 1.30226171875, "actor_loss": -117.60920431518555, "actor_target_entropy": -2.0, "actor_entropy": 0.7742374591827392, "alpha_loss": -0.0005334414993412793, "alpha_value": 0.08913075898414406, "duration": 666.3807559013367, "step": 44000}
{"episode_reward": 866.0, "episode": 89.0, "Q1 loss": 0.43067819662094114, "Q2 loss": 0.4271544906914234, "Mean Target Q": 118.3873514038086, "Mean Q1": 118.37845325012206, "Mean Q2": 118.37832767028809, "critic_loss": 1.013168793320656, "batch_reward": 1.30800390625, "actor_loss": -118.4098773803711, "actor_target_entropy": -2.0, "actor_entropy": 0.7639121530056, "alpha_loss": 0.0003363704713992775, "alpha_value": 0.0890412356774612, "duration": 664.5198776721954, "step": 44500}
{"episode_reward": 859.0, "episode": 90.0, "Q1 loss": 0.43931188443899155, "Q2 loss": 0.4345010850965977, "Mean Target Q": 119.18988698730469, "Mean Q1": 119.18036944580078, "Mean Q2": 119.18039648132324, "critic_loss": 1.0206775097846985, "batch_reward": 1.3115234375, "actor_loss": -119.2103996887207, "actor_target_entropy": -2.0, "actor_entropy": 0.7691989505290985, "alpha_loss": 0.0014134971327148377, "alpha_value": 0.08896828415852659, "step": 45000}
{"duration": 682.9081456661224, "step": 45000}
{"episode_reward": 826.0, "episode": 91.0, "Q1 loss": 0.4115801115870476, "Q2 loss": 0.40996566277742386, "Mean Target Q": 119.95433629760743, "Mean Q1": 119.94351224060058, "Mean Q2": 119.94320143737794, "critic_loss": 0.968983912229538, "batch_reward": 1.315125, "actor_loss": -119.96193539428711, "actor_target_entropy": -2.0, "actor_entropy": 0.7717507538795472, "alpha_loss": 0.0011011795764788986, "alpha_value": 0.08865248140397392, "duration": 665.021133184433, "step": 45500}
{"episode_reward": 840.0, "episode": 92.0, "Q1 loss": 0.424656809169054, "Q2 loss": 0.4202273173570633, "Mean Target Q": 120.69755750732422, "Mean Q1": 120.69161965332032, "Mean Q2": 120.69114685974121, "critic_loss": 0.9881555668115616, "batch_reward": 1.32003515625, "actor_loss": -120.68372052001953, "actor_target_entropy": -2.0, "actor_entropy": 0.7641834695339202, "alpha_loss": 0.001363332818960771, "alpha_value": 0.08837671603302491, "duration": 667.6379244327545, "step": 46000}
{"episode_reward": 856.0, "episode": 93.0, "Q1 loss": 0.42264482179284096, "Q2 loss": 0.414008218985796, "Mean Target Q": 121.44329211425782, "Mean Q1": 121.43425234375, "Mean Q2": 121.43480705871582, "critic_loss": 0.9755806893110275, "batch_reward": 1.32379296875, "actor_loss": -121.47657412719727, "actor_target_entropy": -2.0, "actor_entropy": 0.7651379556655884, "alpha_loss": 0.0020621113274246453, "alpha_value": 0.08797927591274929, "duration": 665.1871514320374, "step": 46500}
{"episode_reward": 857.0, "episode": 94.0, "Q1 loss": 0.4146450484871864, "Q2 loss": 0.4143209333002567, "Mean Target Q": 122.18097630615235, "Mean Q1": 122.17395289916992, "Mean Q2": 122.17282074279785, "critic_loss": 0.9720966812372208, "batch_reward": 1.3272421875, "actor_loss": -122.17710766601563, "actor_target_entropy": -2.0, "actor_entropy": 0.7444523167610169, "alpha_loss": -0.0002213383736088872, "alpha_value": 0.08790328727318829, "duration": 658.4041979312897, "step": 47000}
{"episode_reward": 862.0, "episode": 95.0, "Q1 loss": 0.4199415325820446, "Q2 loss": 0.4166568805217743, "Mean Target Q": 122.92837687683105, "Mean Q1": 122.92036598510742, "Mean Q2": 122.91995546875, "critic_loss": 0.9741013975143432, "batch_reward": 1.33236328125, "actor_loss": -122.99176760864258, "actor_target_entropy": -2.0, "actor_entropy": 0.7382779240608215, "alpha_loss": 0.0010217805972788483, "alpha_value": 0.08777000378541829, "duration": 653.9166605472565, "step": 47500}
{"episode_reward": 867.0, "episode": 96.0, "Q1 loss": 0.3931274394989014, "Q2 loss": 0.39042818548679353, "Mean Target Q": 123.66125384521484, "Mean Q1": 123.65686973571778, "Mean Q2": 123.65501923522949, "critic_loss": 0.9136124436855316, "batch_reward": 1.33982421875, "actor_loss": -123.64221322631836, "actor_target_entropy": -2.0, "actor_entropy": 0.7289037709236145, "alpha_loss": 0.0021474219681695103, "alpha_value": 0.08751607047210543, "duration": 655.5174293518066, "step": 48000}
{"episode_reward": 865.0, "episode": 97.0, "Q1 loss": 0.41189974703788756, "Q2 loss": 0.40877155396938325, "Mean Target Q": 124.35119598388673, "Mean Q1": 124.33976153564453, "Mean Q2": 124.3395274230957, "critic_loss": 0.9698003307580948, "batch_reward": 1.34387109375, "actor_loss": -124.32498089599609, "actor_target_entropy": -2.0, "actor_entropy": 0.7304853162765503, "alpha_loss": 0.0013003717896062882, "alpha_value": 0.08714769360078821, "duration": 654.1636731624603, "step": 48500}
{"episode_reward": 888.0, "episode": 98.0, "Q1 loss": 0.4190491910338402, "Q2 loss": 0.41484802933931353, "Mean Target Q": 125.01400240478516, "Mean Q1": 125.00470884094238, "Mean Q2": 125.00496772766114, "critic_loss": 0.9869977223873139, "batch_reward": 1.34306640625, "actor_loss": -124.98352163696289, "actor_target_entropy": -2.0, "actor_entropy": 0.7083985831737518, "alpha_loss": -0.0006487721260637044, "alpha_value": 0.08698925852274043, "duration": 653.2243745326996, "step": 49000}
{"episode_reward": 875.0, "episode": 99.0, "Q1 loss": 0.4212522642850876, "Q2 loss": 0.41817942721247675, "Mean Target Q": 125.70009956359863, "Mean Q1": 125.69165522766113, "Mean Q2": 125.69275145874023, "critic_loss": 0.9896941128969192, "batch_reward": 1.34909765625, "actor_loss": -125.72833499145507, "actor_target_entropy": -2.0, "actor_entropy": 0.7097540674209595, "alpha_loss": 0.0005145623455755413, "alpha_value": 0.08705866675524242, "duration": 655.8921885490417, "step": 49500}
{"episode_reward": 866.0, "episode": 100.0, "Q1 loss": 0.40601489595770834, "Q2 loss": 0.40436328011751177, "Mean Target Q": 126.37164391784668, "Mean Q1": 126.36187640991211, "Mean Q2": 126.36136657409668, "critic_loss": 0.9560194506645202, "batch_reward": 1.3561875, "actor_loss": -126.40845223999024, "actor_target_entropy": -2.0, "actor_entropy": 0.6929177370071411, "alpha_loss": 0.0014269316447898745, "alpha_value": 0.0869218619054528, "step": 50000}
{"duration": 784.4065628051758, "step": 50000}
{"episode_reward": 862.0, "episode": 101.0, "Q1 loss": 0.4095790918588638, "Q2 loss": 0.4044390554368496, "Mean Target Q": 127.03775166015625, "Mean Q1": 127.03098035583496, "Mean Q2": 127.03021022338868, "critic_loss": 0.9538636634349823, "batch_reward": 1.3586484375, "actor_loss": -127.04117272949219, "actor_target_entropy": -2.0, "actor_entropy": 0.6909523870944977, "alpha_loss": 0.0023302815288770945, "alpha_value": 0.08645292758296547, "duration": 1021.5849068164825, "step": 50500}
{"episode_reward": 874.0, "episode": 102.0, "Q1 loss": 0.4098698094427586, "Q2 loss": 0.4055920238852501, "Mean Target Q": 127.68594519958496, "Mean Q1": 127.68064154968262, "Mean Q2": 127.68008196716309, "critic_loss": 0.9549127559661865, "batch_reward": 1.358984375, "actor_loss": -127.66300900268554, "actor_target_entropy": -2.0, "actor_entropy": 0.7187884142398834, "alpha_loss": 0.0021057442547753455, "alpha_value": 0.08593088725042838, "duration": 1234.7669305801392, "step": 51000}
{"episode_reward": 867.0, "episode": 103.0, "Q1 loss": 0.3850133573591709, "Q2 loss": 0.38335469701886177, "Mean Target Q": 128.36501184692384, "Mean Q1": 128.35500111083985, "Mean Q2": 128.35455829772948, "critic_loss": 0.9152178792953491, "batch_reward": 1.3644375, "actor_loss": -128.29979751586913, "actor_target_entropy": -2.0, "actor_entropy": 0.6897892138957977, "alpha_loss": 0.0016892033708281815, "alpha_value": 0.08556267606836217, "duration": 953.0130677223206, "step": 51500}
{"episode_reward": 874.0, "episode": 104.0, "Q1 loss": 0.40451800196170806, "Q2 loss": 0.39763710143566133, "Mean Target Q": 128.99508511047364, "Mean Q1": 128.98923268127442, "Mean Q2": 128.98910194091798, "critic_loss": 0.9385654336214065, "batch_reward": 1.3670703125, "actor_loss": -129.07676580810548, "actor_target_entropy": -2.0, "actor_entropy": 0.7018417401313781, "alpha_loss": 0.0013469013206195085, "alpha_value": 0.0852802446228125, "duration": 965.6430728435516, "step": 52000}
{"episode_reward": 861.0, "episode": 105.0, "Q1 loss": 0.37713658929467203, "Q2 loss": 0.3746123920440674, "Mean Target Q": 129.61787302856445, "Mean Q1": 129.6112344116211, "Mean Q2": 129.61028835449218, "critic_loss": 0.8848158259391785, "batch_reward": 1.3674140625, "actor_loss": -129.64989770507813, "actor_target_entropy": -2.0, "actor_entropy": 0.7129423887729645, "alpha_loss": 0.0015402414151467383, "alpha_value": 0.08492825505729122, "duration": 689.8942284584045, "step": 52500}
{"episode_reward": 875.0, "episode": 106.0, "Q1 loss": 0.39425008163452147, "Q2 loss": 0.3881119758963585, "Mean Target Q": 130.22741911010743, "Mean Q1": 130.2194657470703, "Mean Q2": 130.2196408203125, "critic_loss": 0.9208171744346618, "batch_reward": 1.37387890625, "actor_loss": -130.2046689453125, "actor_target_entropy": -2.0, "actor_entropy": 0.686241868019104, "alpha_loss": 0.0009222948877140879, "alpha_value": 0.08468671497227319, "duration": 691.7700052261353, "step": 53000}
{"episode_reward": 866.0, "episode": 107.0, "Q1 loss": 0.38578574051856995, "Q2 loss": 0.3816081276118755, "Mean Target Q": 130.81079752197266, "Mean Q1": 130.80449060058595, "Mean Q2": 130.8036696105957, "critic_loss": 0.9077011556625366, "batch_reward": 1.3797421875, "actor_loss": -130.78279833984374, "actor_target_entropy": -2.0, "actor_entropy": 0.7003548469543457, "alpha_loss": 0.00019168198644183576, "alpha_value": 0.08444874688379822, "duration": 718.8947670459747, "step": 53500}
{"episode_reward": 881.0, "episode": 108.0, "Q1 loss": 0.3956178198993206, "Q2 loss": 0.3919184136271477, "Mean Target Q": 131.364633013916, "Mean Q1": 131.359069140625, "Mean Q2": 131.35851604614257, "critic_loss": 0.9317865121364594, "batch_reward": 1.38180078125, "actor_loss": -131.3130205078125, "actor_target_entropy": -2.0, "actor_entropy": 0.6768287844657898, "alpha_loss": -0.0007926570395939052, "alpha_value": 0.08451344474142719, "duration": 689.1430013179779, "step": 54000}
{"episode_reward": 872.0, "episode": 109.0, "Q1 loss": 0.3823550213217735, "Q2 loss": 0.3788093861877918, "Mean Target Q": 131.91954528808594, "Mean Q1": 131.90988240966797, "Mean Q2": 131.90989462280274, "critic_loss": 0.9037885942459106, "batch_reward": 1.38846484375, "actor_loss": -131.89716723632813, "actor_target_entropy": -2.0, "actor_entropy": 0.6758359998464585, "alpha_loss": 0.0007826728783547879, "alpha_value": 0.08459343250611673, "duration": 691.2987732887268, "step": 54500}
{"episode_reward": 880.0, "episode": 110.0, "Q1 loss": 0.3862649870991707, "Q2 loss": 0.3820937985062599, "Mean Target Q": 132.48320568237304, "Mean Q1": 132.47825637207032, "Mean Q2": 132.47776873168945, "critic_loss": 0.9006057051420212, "batch_reward": 1.38866796875, "actor_loss": -132.47445843505858, "actor_target_entropy": -2.0, "actor_entropy": 0.686207367181778, "alpha_loss": 0.002368724576663226, "alpha_value": 0.08420233565525805, "step": 55000}
{"duration": 711.8398377895355, "step": 55000}
{"episode_reward": 885.0, "episode": 111.0, "Q1 loss": 0.372293325817585, "Q2 loss": 0.3691463013827801, "Mean Target Q": 133.04215947875977, "Mean Q1": 133.0332498413086, "Mean Q2": 133.03301583251954, "critic_loss": 0.8730799565315247, "batch_reward": 1.39419140625, "actor_loss": -133.0171561279297, "actor_target_entropy": -2.0, "actor_entropy": 0.6775868406295776, "alpha_loss": 0.0007835674402303993, "alpha_value": 0.0838343234140419, "duration": 694.8186094760895, "step": 55500}
{"episode_reward": 889.0, "episode": 112.0, "Q1 loss": 0.3788943808019161, "Q2 loss": 0.37386090066432953, "Mean Target Q": 133.5853558959961, "Mean Q1": 133.57727208862303, "Mean Q2": 133.57740927124024, "critic_loss": 0.8948735238313675, "batch_reward": 1.39842578125, "actor_loss": -133.58681756591798, "actor_target_entropy": -2.0, "actor_entropy": 0.6729869434833526, "alpha_loss": 0.002330059095285833, "alpha_value": 0.08365381505928855, "duration": 693.0008165836334, "step": 56000}
{"episode_reward": 851.0, "episode": 113.0, "Q1 loss": 0.38780314195156096, "Q2 loss": 0.3834818005204201, "Mean Target Q": 134.11579134521483, "Mean Q1": 134.1095723815918, "Mean Q2": 134.10934407348634, "critic_loss": 0.906256293296814, "batch_reward": 1.40043359375, "actor_loss": -134.11469598388672, "actor_target_entropy": -2.0, "actor_entropy": 0.662979276061058, "alpha_loss": 0.0008145356276072562, "alpha_value": 0.08328829214510262, "duration": 701.7929086685181, "step": 56500}
{"episode_reward": 879.0, "episode": 114.0, "Q1 loss": 0.36987921735048296, "Q2 loss": 0.36674785870909693, "Mean Target Q": 134.59534895629884, "Mean Q1": 134.588074609375, "Mean Q2": 134.5884072265625, "critic_loss": 0.8749003950357437, "batch_reward": 1.3998359375, "actor_loss": -134.5409600830078, "actor_target_entropy": -2.0, "actor_entropy": 0.6651017186641693, "alpha_loss": 0.0004996817903593182, "alpha_value": 0.08301894638270088, "duration": 692.508228302002, "step": 57000}
{"episode_reward": 855.0, "episode": 115.0, "Q1 loss": 0.3726293946743012, "Q2 loss": 0.37006826842427254, "Mean Target Q": 135.11314529418945, "Mean Q1": 135.10463162231446, "Mean Q2": 135.10349251098634, "critic_loss": 0.8744923919439316, "batch_reward": 1.40448828125, "actor_loss": -135.08774609375, "actor_target_entropy": -2.0, "actor_entropy": 0.6814740419387817, "alpha_loss": 0.0027954109595157207, "alpha_value": 0.08277075100432615, "duration": 708.1609933376312, "step": 57500}
{"episode_reward": 886.0, "episode": 116.0, "Q1 loss": 0.3681610931575298, "Q2 loss": 0.3640824860870838, "Mean Target Q": 135.6200464538574, "Mean Q1": 135.61185990600586, "Mean Q2": 135.61127200317384, "critic_loss": 0.8707926573753357, "batch_reward": 1.40321484375, "actor_loss": -135.5227022705078, "actor_target_entropy": -2.0, "actor_entropy": 0.6507376130819321, "alpha_loss": 0.0023697517719119786, "alpha_value": 0.08215912661492564, "duration": 711.156733751297, "step": 58000}
{"episode_reward": 857.0, "episode": 117.0, "Q1 loss": 0.3693258984684944, "Q2 loss": 0.36797952196002004, "Mean Target Q": 136.16020853271485, "Mean Q1": 136.15009004516602, "Mean Q2": 136.150102734375, "critic_loss": 0.8752133283615112, "batch_reward": 1.41058203125, "actor_loss": -136.1062520751953, "actor_target_entropy": -2.0, "actor_entropy": 0.6628474003076553, "alpha_loss": 0.002094310613581911, "alpha_value": 0.081800979787044, "duration": 725.0224945545197, "step": 58500}
{"episode_reward": 896.0, "episode": 118.0, "Q1 loss": 0.36552569265365603, "Q2 loss": 0.36195802828073503, "Mean Target Q": 136.6227578186035, "Mean Q1": 136.61463834838867, "Mean Q2": 136.61549197998048, "critic_loss": 0.85389446413517, "batch_reward": 1.41398046875, "actor_loss": -136.60393078613282, "actor_target_entropy": -2.0, "actor_entropy": 0.6569443175792694, "alpha_loss": 0.0008359151338227093, "alpha_value": 0.08147099753303058, "duration": 720.2728550434113, "step": 59000}
{"episode_reward": 861.0, "episode": 119.0, "Q1 loss": 0.37145386688113213, "Q2 loss": 0.3704956477403641, "Mean Target Q": 137.12846348876954, "Mean Q1": 137.12241922607421, "Mean Q2": 137.12196557617187, "critic_loss": 0.8724787244796753, "batch_reward": 1.42178515625, "actor_loss": -137.1034267578125, "actor_target_entropy": -2.0, "actor_entropy": 0.6605178209543228, "alpha_loss": 0.0023090003267861904, "alpha_value": 0.081114790877512, "duration": 697.7176206111908, "step": 59500}
{"episode_reward": 903.0, "episode": 120.0, "Q1 loss": 0.36597293739914893, "Q2 loss": 0.36171692150831225, "Mean Target Q": 137.5688341308594, "Mean Q1": 137.5600636291504, "Mean Q2": 137.5602166381836, "critic_loss": 0.8582228240966797, "batch_reward": 1.415984375, "actor_loss": -137.5372626953125, "actor_target_entropy": -2.0, "actor_entropy": 0.63799842441082, "alpha_loss": -0.0003048597031738609, "alpha_value": 0.08094160353352997, "step": 60000}
{"duration": 686.3704662322998, "step": 60000}
{"episode_reward": 891.0, "episode": 121.0, "Q1 loss": 0.36121544443368914, "Q2 loss": 0.35717965442538263, "Mean Target Q": 138.04464049072266, "Mean Q1": 138.03657830200194, "Mean Q2": 138.03644676513673, "critic_loss": 0.8547808723449707, "batch_reward": 1.420140625, "actor_loss": -138.07898504638672, "actor_target_entropy": -2.0, "actor_entropy": 0.6463307300806046, "alpha_loss": 0.00017574416333809495, "alpha_value": 0.08101074771888334, "duration": 696.2298040390015, "step": 60500}
{"episode_reward": 871.0, "episode": 122.0, "Q1 loss": 0.35287214859127997, "Q2 loss": 0.3512164128124714, "Mean Target Q": 138.48429313964843, "Mean Q1": 138.47908684082032, "Mean Q2": 138.47812303466796, "critic_loss": 0.8323224591016769, "batch_reward": 1.42359765625, "actor_loss": -138.4415209350586, "actor_target_entropy": -2.0, "actor_entropy": 0.6264445185661316, "alpha_loss": 9.860853385180235e-05, "alpha_value": 0.08096267565514158, "duration": 709.1901879310608, "step": 61000}
{"episode_reward": 873.0, "episode": 123.0, "Q1 loss": 0.37017301509976386, "Q2 loss": 0.36446419296860694, "Mean Target Q": 138.9315037475586, "Mean Q1": 138.92455858154298, "Mean Q2": 138.92439419555663, "critic_loss": 0.8585906826257705, "batch_reward": 1.42657421875, "actor_loss": -138.94079180908204, "actor_target_entropy": -2.0, "actor_entropy": 0.6406502664089203, "alpha_loss": 0.002562186065129936, "alpha_value": 0.08057607403254424, "duration": 680.2736685276031, "step": 61500}
{"episode_reward": 893.0, "episode": 124.0, "Q1 loss": 0.34276149171590803, "Q2 loss": 0.3432191526830196, "Mean Target Q": 139.36173157958984, "Mean Q1": 139.35396041870118, "Mean Q2": 139.354305859375, "critic_loss": 0.8147000375986099, "batch_reward": 1.4311875, "actor_loss": -139.30034881591797, "actor_target_entropy": -2.0, "actor_entropy": 0.6219032986164093, "alpha_loss": 0.001428403225261718, "alpha_value": 0.08026283147930115, "duration": 706.6389784812927, "step": 62000}
{"episode_reward": 888.0, "episode": 125.0, "Q1 loss": 0.3652667381644249, "Q2 loss": 0.36148309428691866, "Mean Target Q": 139.76238129882813, "Mean Q1": 139.75060421142578, "Mean Q2": 139.7514733581543, "critic_loss": 0.8735852653980255, "batch_reward": 1.4332578125, "actor_loss": -139.55784313964844, "actor_target_entropy": -2.0, "actor_entropy": 0.6461585760116577, "alpha_loss": -0.0003484048889949918, "alpha_value": 0.08011168091436777, "duration": 696.8810956478119, "step": 62500}
{"episode_reward": 892.0, "episode": 126.0, "Q1 loss": 0.35423021690249445, "Q2 loss": 0.3490662046730518, "Mean Target Q": 140.2038535522461, "Mean Q1": 140.1983046081543, "Mean Q2": 140.19782030029296, "critic_loss": 0.8269288599491119, "batch_reward": 1.43469140625, "actor_loss": -140.1145067138672, "actor_target_entropy": -2.0, "actor_entropy": 0.5922651314735412, "alpha_loss": -0.0007664941176772118, "alpha_value": 0.08021604573684703, "duration": 719.3125288486481, "step": 63000}
{"episode_reward": 895.0, "episode": 127.0, "Q1 loss": 0.35908173483014105, "Q2 loss": 0.35716457270383833, "Mean Target Q": 140.63519457397462, "Mean Q1": 140.62774159545899, "Mean Q2": 140.62619826049806, "critic_loss": 0.8522510288953781, "batch_reward": 1.43906640625, "actor_loss": -140.5899503173828, "actor_target_entropy": -2.0, "actor_entropy": 0.6025300022363663, "alpha_loss": 0.00022251102048903703, "alpha_value": 0.08026684254906338, "duration": 699.3603134155273, "step": 63500}
{"episode_reward": 876.0, "episode": 128.0, "Q1 loss": 0.35514708119034766, "Q2 loss": 0.3497569897770882, "Mean Target Q": 141.04668411254883, "Mean Q1": 141.03963307495118, "Mean Q2": 141.03901737060548, "critic_loss": 0.8340812777280807, "batch_reward": 1.44008203125, "actor_loss": -141.04829602050782, "actor_target_entropy": -2.0, "actor_entropy": 0.5839067713022232, "alpha_loss": -0.0003760511048603803, "alpha_value": 0.08026821509528514, "duration": 718.0658783912659, "step": 64000}
{"episode_reward": 893.0, "episode": 129.0, "Q1 loss": 0.35649229514598846, "Q2 loss": 0.3527553755044937, "Mean Target Q": 141.4315726135254, "Mean Q1": 141.42557121582033, "Mean Q2": 141.42538236083985, "critic_loss": 0.8399208178520202, "batch_reward": 1.44433984375, "actor_loss": -141.4002778930664, "actor_target_entropy": -2.0, "actor_entropy": 0.6009598842859268, "alpha_loss": 0.00024614368565380575, "alpha_value": 0.08037447953728832, "duration": 710.1088833808899, "step": 64500}
{"episode_reward": 880.0, "episode": 130.0, "Q1 loss": 0.3432390984594822, "Q2 loss": 0.34090272090435025, "Mean Target Q": 141.8673771118164, "Mean Q1": 141.8594099609375, "Mean Q2": 141.86014401245117, "critic_loss": 0.8094350259304046, "batch_reward": 1.44568359375, "actor_loss": -141.96727532958985, "actor_target_entropy": -2.0, "actor_entropy": 0.616173926115036, "alpha_loss": 0.002988338065566495, "alpha_value": 0.07999327406413488, "step": 65000}
{"duration": 725.7294075489044, "step": 65000}
{"episode_reward": 869.0, "episode": 131.0, "Q1 loss": 0.3433479790508747, "Q2 loss": 0.33890868562459947, "Mean Target Q": 142.25496997070312, "Mean Q1": 142.24759807739258, "Mean Q2": 142.2476348815918, "critic_loss": 0.8072114812135697, "batch_reward": 1.449, "actor_loss": -142.27201892089843, "actor_target_entropy": -2.0, "actor_entropy": 0.6237937725782394, "alpha_loss": 0.0015284719779156148, "alpha_value": 0.07951816947933724, "duration": 717.0549063682556, "step": 65500}
{"episode_reward": 885.0, "episode": 132.0, "Q1 loss": 0.3378555046141148, "Q2 loss": 0.33645245119333267, "Mean Target Q": 142.6705150695801, "Mean Q1": 142.66562458496094, "Mean Q2": 142.66606755371095, "critic_loss": 0.7997003675699234, "batch_reward": 1.45115625, "actor_loss": -142.65193676757812, "actor_target_entropy": -2.0, "actor_entropy": 0.5907241080999375, "alpha_loss": 0.0011160483029671014, "alpha_value": 0.0792369993156091, "duration": 670.1338329315186, "step": 66000}
{"episode_reward": 866.0, "episode": 133.0, "Q1 loss": 0.3528524973988533, "Q2 loss": 0.3488143909692764, "Mean Target Q": 143.0176266784668, "Mean Q1": 143.01003594360353, "Mean Q2": 143.0095562438965, "critic_loss": 0.8352315629720688, "batch_reward": 1.45233203125, "actor_loss": -142.95911126708984, "actor_target_entropy": -2.0, "actor_entropy": 0.5949136036634445, "alpha_loss": 0.002304988886695355, "alpha_value": 0.07876824018295896, "duration": 657.5608978271484, "step": 66500}
{"episode_reward": 896.0, "episode": 134.0, "Q1 loss": 0.3427883183896542, "Q2 loss": 0.33958813058137893, "Mean Target Q": 143.39549655761718, "Mean Q1": 143.38863086547852, "Mean Q2": 143.38795989379884, "critic_loss": 0.815703053355217, "batch_reward": 1.45890234375, "actor_loss": -143.34122674560547, "actor_target_entropy": -2.0, "actor_entropy": 0.5835924466848373, "alpha_loss": 0.0024329963081981985, "alpha_value": 0.0783871366105237, "duration": 658.2696945667267, "step": 67000}
{"episode_reward": 884.0, "episode": 135.0, "Q1 loss": 0.33678017033934593, "Q2 loss": 0.3335803603053093, "Mean Target Q": 143.77464172973632, "Mean Q1": 143.76733323364257, "Mean Q2": 143.7674160095215, "critic_loss": 0.8000722165107728, "batch_reward": 1.458515625, "actor_loss": -143.7861450805664, "actor_target_entropy": -2.0, "actor_entropy": 0.5952539385557175, "alpha_loss": 0.0012957216065842658, "alpha_value": 0.0779304217604944, "duration": 741.7953250408173, "step": 67500}
{"episode_reward": 891.0, "episode": 136.0, "Q1 loss": 0.33481372398734094, "Q2 loss": 0.33257566413283346, "Mean Target Q": 144.15364661865235, "Mean Q1": 144.14216076660156, "Mean Q2": 144.1426061279297, "critic_loss": 0.7934847277402878, "batch_reward": 1.46316796875, "actor_loss": -144.12160308837892, "actor_target_entropy": -2.0, "actor_entropy": 0.5718377162218093, "alpha_loss": -0.00037232009251601995, "alpha_value": 0.07790521684857471, "duration": 676.0526864528656, "step": 68000}
{"episode_reward": 879.0, "episode": 137.0, "Q1 loss": 0.34931986162662504, "Q2 loss": 0.34558034209609034, "Mean Target Q": 144.49909134521485, "Mean Q1": 144.49143554077148, "Mean Q2": 144.4923014892578, "critic_loss": 0.8218578109741211, "batch_reward": 1.46091015625, "actor_loss": -144.5097797241211, "actor_target_entropy": -2.0, "actor_entropy": 0.5946974238157272, "alpha_loss": 0.0019178117350675166, "alpha_value": 0.07768543619832712, "duration": 779.2712779045105, "step": 68500}
{"episode_reward": 900.0, "episode": 138.0, "Q1 loss": 0.33125199774503705, "Q2 loss": 0.3301074918448925, "Mean Target Q": 144.87386751098632, "Mean Q1": 144.86428680419922, "Mean Q2": 144.86432427368163, "critic_loss": 0.7873689982891082, "batch_reward": 1.4660546875, "actor_loss": -144.83585290527344, "actor_target_entropy": -2.0, "actor_entropy": 0.5740366305112838, "alpha_loss": -0.0015024823287967592, "alpha_value": 0.07761012096602639, "duration": 725.2330360412598, "step": 69000}
{"episode_reward": 886.0, "episode": 139.0, "Q1 loss": 0.3497101304888725, "Q2 loss": 0.34427093991041186, "Mean Target Q": 145.21672923583984, "Mean Q1": 145.2090827697754, "Mean Q2": 145.2085241088867, "critic_loss": 0.8236638698577881, "batch_reward": 1.46819921875, "actor_loss": -145.23570208740233, "actor_target_entropy": -2.0, "actor_entropy": 0.5741648422479629, "alpha_loss": 6.479985266923905e-05, "alpha_value": 0.07779842378964016, "duration": 724.7198202610016, "step": 69500}
{"episode_reward": 890.0, "episode": 140.0, "Q1 loss": 0.34632245859503746, "Q2 loss": 0.3427046402454376, "Mean Target Q": 145.5328271911621, "Mean Q1": 145.52537571411133, "Mean Q2": 145.5254763671875, "critic_loss": 0.8270529237985611, "batch_reward": 1.4696875, "actor_loss": -145.48887384033202, "actor_target_entropy": -2.0, "actor_entropy": 0.5675983554124833, "alpha_loss": 2.053202595561743e-06, "alpha_value": 0.07781306902173313, "step": 70000}
{"duration": 715.7652199268341, "step": 70000}
{"episode_reward": 894.0, "episode": 141.0, "Q1 loss": 0.3349304246544838, "Q2 loss": 0.3336055279493332, "Mean Target Q": 145.86766325683593, "Mean Q1": 145.86053507080078, "Mean Q2": 145.86178963012696, "critic_loss": 0.8012194041013717, "batch_reward": 1.46971875, "actor_loss": -145.85156713867187, "actor_target_entropy": -2.0, "actor_entropy": 0.5526862151622772, "alpha_loss": 0.0006325401545036584, "alpha_value": 0.07764807140446668, "duration": 656.0197510719299, "step": 70500}
{"episode_reward": 886.0, "episode": 142.0, "Q1 loss": 0.32856315999031066, "Q2 loss": 0.3235997050523758, "Mean Target Q": 146.20251864013673, "Mean Q1": 146.19486033325197, "Mean Q2": 146.1947013244629, "critic_loss": 0.7741700847148896, "batch_reward": 1.47577734375, "actor_loss": -146.13353381347656, "actor_target_entropy": -2.0, "actor_entropy": 0.5636687480211258, "alpha_loss": 0.0004681503113824874, "alpha_value": 0.07758423897683535, "duration": 656.5718717575073, "step": 71000}
{"episode_reward": 900.0, "episode": 143.0, "Q1 loss": 0.3277927244484425, "Q2 loss": 0.32865245501399043, "Mean Target Q": 146.52941282958983, "Mean Q1": 146.5209509765625, "Mean Q2": 146.5215161254883, "critic_loss": 0.7763547715544701, "batch_reward": 1.4748046875, "actor_loss": -146.51407470703126, "actor_target_entropy": -2.0, "actor_entropy": 0.5774765696525573, "alpha_loss": -0.0005530238533392549, "alpha_value": 0.07769387639726588, "duration": 658.2147626876831, "step": 71500}
{"episode_reward": 882.0, "episode": 144.0, "Q1 loss": 0.3341084964811802, "Q2 loss": 0.3313007327914238, "Mean Target Q": 146.82977894897462, "Mean Q1": 146.82330272827147, "Mean Q2": 146.82306718139648, "critic_loss": 0.784563818693161, "batch_reward": 1.47658984375, "actor_loss": -146.82176434326172, "actor_target_entropy": -2.0, "actor_entropy": 0.5744555773735046, "alpha_loss": -0.00031519128382205964, "alpha_value": 0.07767902887226176, "duration": 656.4710323810577, "step": 72000}
{"episode_reward": 902.0, "episode": 145.0, "Q1 loss": 0.32795677456855776, "Q2 loss": 0.32483021867871287, "Mean Target Q": 147.13081036376954, "Mean Q1": 147.12208170166016, "Mean Q2": 147.12269064941407, "critic_loss": 0.7767548631429673, "batch_reward": 1.47903125, "actor_loss": -147.11530798339842, "actor_target_entropy": -2.0, "actor_entropy": 0.5725980792045593, "alpha_loss": -0.0020227348840562626, "alpha_value": 0.07789824608245383, "duration": 657.2564587593079, "step": 72500}
{"episode_reward": 905.0, "episode": 146.0, "Q1 loss": 0.3355899410247803, "Q2 loss": 0.33359066333174703, "Mean Target Q": 147.44869797363282, "Mean Q1": 147.44056936645507, "Mean Q2": 147.43931154174805, "critic_loss": 0.7966733498573303, "batch_reward": 1.48176953125, "actor_loss": -147.4316703491211, "actor_target_entropy": -2.0, "actor_entropy": 0.5715343017578125, "alpha_loss": 0.00039419918251223864, "alpha_value": 0.07810560025566836, "duration": 656.5126123428345, "step": 73000}
{"episode_reward": 887.0, "episode": 147.0, "Q1 loss": 0.3366418437302113, "Q2 loss": 0.3341290260910988, "Mean Target Q": 147.7358375, "Mean Q1": 147.72898833618163, "Mean Q2": 147.72727162475587, "critic_loss": 0.799631694316864, "batch_reward": 1.4835625, "actor_loss": -147.7144594116211, "actor_target_entropy": -2.0, "actor_entropy": 0.5787774707078934, "alpha_loss": -0.0002227077663410455, "alpha_value": 0.07808360534670347, "duration": 659.083930015564, "step": 73500}
{"episode_reward": 904.0, "episode": 148.0, "Q1 loss": 0.33844783220291136, "Q2 loss": 0.3356632872581482, "Mean Target Q": 148.0226490661621, "Mean Q1": 148.0152907409668, "Mean Q2": 148.01600146484375, "critic_loss": 0.7965677785873413, "batch_reward": 1.48733984375, "actor_loss": -148.00291375732422, "actor_target_entropy": -2.0, "actor_entropy": 0.5542517578601838, "alpha_loss": 0.0001694805766455829, "alpha_value": 0.07816589875244534, "duration": 656.2971985340118, "step": 74000}
{"episode_reward": 899.0, "episode": 149.0, "Q1 loss": 0.3215571650505066, "Q2 loss": 0.3183779563307762, "Mean Target Q": 148.29918629760743, "Mean Q1": 148.2927809753418, "Mean Q2": 148.29208122558595, "critic_loss": 0.758522827744484, "batch_reward": 1.4862265625, "actor_loss": -148.30356103515626, "actor_target_entropy": -2.0, "actor_entropy": 0.5788109340667724, "alpha_loss": 0.0011368887089192867, "alpha_value": 0.07790801041614917, "duration": 657.6834156513214, "step": 74500}
{"episode_reward": 911.0, "episode": 150.0, "Q1 loss": 0.314686766576767, "Q2 loss": 0.31269092842936513, "Mean Target Q": 148.58283943481445, "Mean Q1": 148.57709775390626, "Mean Q2": 148.57687308349608, "critic_loss": 0.7363545295000077, "batch_reward": 1.4897421875, "actor_loss": -148.5580355834961, "actor_target_entropy": -2.0, "actor_entropy": 0.5670180829763413, "alpha_loss": 0.0005371584626846015, "alpha_value": 0.07782313040462878, "step": 75000}
{"duration": 672.9234688282013, "step": 75000}
{"episode_reward": 883.0, "episode": 151.0, "Q1 loss": 0.32807193146348, "Q2 loss": 0.32623143583536146, "Mean Target Q": 148.84989520874024, "Mean Q1": 148.8432346130371, "Mean Q2": 148.84317373046875, "critic_loss": 0.7792285891771317, "batch_reward": 1.49146875, "actor_loss": -148.85346575927736, "actor_target_entropy": -2.0, "actor_entropy": 0.5639291524887085, "alpha_loss": 0.0009403622359968721, "alpha_value": 0.07760561222427227, "duration": 656.4786500930786, "step": 75500}
{"episode_reward": 904.0, "episode": 152.0, "Q1 loss": 0.32490318602919577, "Q2 loss": 0.3231712523281574, "Mean Target Q": 149.1437573791504, "Mean Q1": 149.1370274291992, "Mean Q2": 149.13717215576173, "critic_loss": 0.7629253161549568, "batch_reward": 1.49695703125, "actor_loss": -149.135513671875, "actor_target_entropy": -2.0, "actor_entropy": 0.5468117390871048, "alpha_loss": -0.0008103173056151718, "alpha_value": 0.07754045051444422, "duration": 658.3918163776398, "step": 76000}
{"episode_reward": 896.0, "episode": 153.0, "Q1 loss": 0.32838908001184464, "Q2 loss": 0.32523873925209046, "Mean Target Q": 149.43832736816407, "Mean Q1": 149.4302866455078, "Mean Q2": 149.43026677856446, "critic_loss": 0.7804265867471695, "batch_reward": 1.49933984375, "actor_loss": -149.43037957763673, "actor_target_entropy": -2.0, "actor_entropy": 0.5656229214668274, "alpha_loss": 0.00020110502862371503, "alpha_value": 0.07774525613333916, "duration": 655.3935513496399, "step": 76500}
{"episode_reward": 908.0, "episode": 154.0, "Q1 loss": 0.3150779918789864, "Q2 loss": 0.3124400863826275, "Mean Target Q": 149.718187890625, "Mean Q1": 149.7099579223633, "Mean Q2": 149.70988461914064, "critic_loss": 0.749760277569294, "batch_reward": 1.5000703125, "actor_loss": -149.68132452392578, "actor_target_entropy": -2.0, "actor_entropy": 0.5602502645254135, "alpha_loss": -0.0012529262858442963, "alpha_value": 0.07773543920181945, "duration": 658.2898161411285, "step": 77000}
{"episode_reward": 891.0, "episode": 155.0, "Q1 loss": 0.3104136721789837, "Q2 loss": 0.30886070405244825, "Mean Target Q": 149.9723788696289, "Mean Q1": 149.96540993652343, "Mean Q2": 149.9658419189453, "critic_loss": 0.737267572760582, "batch_reward": 1.498703125, "actor_loss": -149.96021508789062, "actor_target_entropy": -2.0, "actor_entropy": 0.5523807674646377, "alpha_loss": -0.0004131639865227044, "alpha_value": 0.07797239893463999, "duration": 656.0443806648254, "step": 77500}
{"episode_reward": 905.0, "episode": 156.0, "Q1 loss": 0.3168158121526241, "Q2 loss": 0.3128246878504753, "Mean Target Q": 150.22265412597656, "Mean Q1": 150.21819454345703, "Mean Q2": 150.2180169128418, "critic_loss": 0.7421928563117981, "batch_reward": 1.5032265625, "actor_loss": -150.207796875, "actor_target_entropy": -2.0, "actor_entropy": 0.5547844212055206, "alpha_loss": -0.00034660901012830434, "alpha_value": 0.07810526289325481, "duration": 656.6111354827881, "step": 78000}
{"episode_reward": 898.0, "episode": 157.0, "Q1 loss": 0.31505540102124213, "Q2 loss": 0.31354390835762025, "Mean Target Q": 150.46984658813477, "Mean Q1": 150.46240571289061, "Mean Q2": 150.46258506469727, "critic_loss": 0.7544478961229324, "batch_reward": 1.505296875, "actor_loss": -150.4535654296875, "actor_target_entropy": -2.0, "actor_entropy": 0.5591101441383362, "alpha_loss": 0.001920469052158296, "alpha_value": 0.07795980076352428, "duration": 657.5642323493958, "step": 78500}
{"episode_reward": 897.0, "episode": 158.0, "Q1 loss": 0.3314976442635059, "Q2 loss": 0.32891062124967574, "Mean Target Q": 150.68227111206053, "Mean Q1": 150.66790485839843, "Mean Q2": 150.66875023803712, "critic_loss": 0.8303465505242348, "batch_reward": 1.50469921875, "actor_loss": -150.70296032714845, "actor_target_entropy": -2.0, "actor_entropy": 0.5402050777673721, "alpha_loss": -0.000923323969822377, "alpha_value": 0.0778280346208721, "duration": 655.2807981967926, "step": 79000}
{"episode_reward": 879.0, "episode": 159.0, "Q1 loss": 0.31469054639935495, "Q2 loss": 0.3105774069547653, "Mean Target Q": 150.93076314086915, "Mean Q1": 150.92170916748046, "Mean Q2": 150.92121677246095, "critic_loss": 0.7432723960876465, "batch_reward": 1.50898046875, "actor_loss": -150.9004510498047, "actor_target_entropy": -2.0, "actor_entropy": 0.5667214251756668, "alpha_loss": 0.0005110839165281504, "alpha_value": 0.07783572020371922, "duration": 643.1076717376709, "step": 79500}
{"episode_reward": 908.0, "episode": 160.0, "Q1 loss": 0.3100897771716118, "Q2 loss": 0.30766806492209436, "Mean Target Q": 151.15201419677734, "Mean Q1": 151.1431748352051, "Mean Q2": 151.14317183227539, "critic_loss": 0.7464517983794212, "batch_reward": 1.51063671875, "actor_loss": -151.1422953491211, "actor_target_entropy": -2.0, "actor_entropy": 0.5573349303007126, "alpha_loss": 2.057883539237082e-05, "alpha_value": 0.07772538261737713, "step": 80000}
{"duration": 658.276261806488, "step": 80000}
{"episode_reward": 897.0, "episode": 161.0, "Q1 loss": 0.3192264001309872, "Q2 loss": 0.3143571611464024, "Mean Target Q": 151.39146694335938, "Mean Q1": 151.38451629638672, "Mean Q2": 151.3847331665039, "critic_loss": 0.7558700653910637, "batch_reward": 1.5131484375, "actor_loss": -151.37464068603515, "actor_target_entropy": -2.0, "actor_entropy": 0.5524841132164001, "alpha_loss": -0.0002057608007453382, "alpha_value": 0.07774144038689484, "duration": 643.2680513858795, "step": 80500}
{"episode_reward": 889.0, "episode": 162.0, "Q1 loss": 0.3216857358813286, "Q2 loss": 0.31709764531850815, "Mean Target Q": 151.60800158081054, "Mean Q1": 151.6007699645996, "Mean Q2": 151.60087522583007, "critic_loss": 0.7588906890153885, "batch_reward": 1.5141640625, "actor_loss": -151.59385278320312, "actor_target_entropy": -2.0, "actor_entropy": 0.5482824593782425, "alpha_loss": -0.0015917761500459165, "alpha_value": 0.07800666597719573, "duration": 640.978342294693, "step": 81000}
{"episode_reward": 903.0, "episode": 163.0, "Q1 loss": 0.30729515864253043, "Q2 loss": 0.30478993462324144, "Mean Target Q": 151.8147934387207, "Mean Q1": 151.80700057373048, "Mean Q2": 151.80647125854492, "critic_loss": 0.7279085326194763, "batch_reward": 1.5175546875, "actor_loss": -151.74073950195313, "actor_target_entropy": -2.0, "actor_entropy": 0.5207475336790085, "alpha_loss": 0.00017784317582845687, "alpha_value": 0.07815053599795249, "duration": 641.4846351146698, "step": 81500}
{"episode_reward": 920.0, "episode": 164.0, "Q1 loss": 0.31021209515333176, "Q2 loss": 0.3080499859273434, "Mean Target Q": 152.02045212402345, "Mean Q1": 152.01233279418946, "Mean Q2": 152.01224257202148, "critic_loss": 0.7376808981895446, "batch_reward": 1.5176875, "actor_loss": -151.96271911621093, "actor_target_entropy": -2.0, "actor_entropy": 0.5350757136344909, "alpha_loss": 0.00030828882311470805, "alpha_value": 0.07805190403625455, "duration": 643.8149356842041, "step": 82000}
{"episode_reward": 897.0, "episode": 165.0, "Q1 loss": 0.31463673065900805, "Q2 loss": 0.3111418896317482, "Mean Target Q": 152.22561260986328, "Mean Q1": 152.21987879638672, "Mean Q2": 152.21948782958984, "critic_loss": 0.7447910037636757, "batch_reward": 1.51653125, "actor_loss": -152.2042727661133, "actor_target_entropy": -2.0, "actor_entropy": 0.5399440732002259, "alpha_loss": -0.00010537340957671404, "alpha_value": 0.07814892232462094, "duration": 641.6185030937195, "step": 82500}
{"episode_reward": 875.0, "episode": 166.0, "Q1 loss": 0.2985848800301552, "Q2 loss": 0.2977673914194107, "Mean Target Q": 152.4582637084961, "Mean Q1": 152.45082842407226, "Mean Q2": 152.45013673706055, "critic_loss": 0.7078356035351753, "batch_reward": 1.5241015625, "actor_loss": -152.40253271484374, "actor_target_entropy": -2.0, "actor_entropy": 0.5228881286382675, "alpha_loss": -0.0005316667584702372, "alpha_value": 0.07816296872734907, "duration": 642.637140750885, "step": 83000}
{"episode_reward": 898.0, "episode": 167.0, "Q1 loss": 0.29232810215353966, "Q2 loss": 0.2906918048918247, "Mean Target Q": 152.66788247070312, "Mean Q1": 152.66071134643553, "Mean Q2": 152.65980034179688, "critic_loss": 0.6938037680983543, "batch_reward": 1.5220859375, "actor_loss": -152.606041015625, "actor_target_entropy": -2.0, "actor_entropy": 0.5170308837890625, "alpha_loss": -0.0003712461665272713, "alpha_value": 0.0782360871222062, "duration": 641.4401519298553, "step": 83500}
{"episode_reward": 921.0, "episode": 168.0, "Q1 loss": 0.3160676705479622, "Q2 loss": 0.3118599464595318, "Mean Target Q": 152.88129060058594, "Mean Q1": 152.87465357666017, "Mean Q2": 152.87404982910155, "critic_loss": 0.7506471294760704, "batch_reward": 1.5243203125, "actor_loss": -152.8407642211914, "actor_target_entropy": -2.0, "actor_entropy": 0.5373418978452682, "alpha_loss": 0.0028908550911583004, "alpha_value": 0.07804024513405121, "duration": 643.034185886383, "step": 84000}
{"episode_reward": 889.0, "episode": 169.0, "Q1 loss": 0.3073286488473415, "Q2 loss": 0.304213315820694, "Mean Target Q": 153.08369095458985, "Mean Q1": 153.07638359985353, "Mean Q2": 153.07606076660156, "critic_loss": 0.7260270036458969, "batch_reward": 1.52497265625, "actor_loss": -153.05200079345704, "actor_target_entropy": -2.0, "actor_entropy": 0.5294453778266907, "alpha_loss": 0.0007857329829130321, "alpha_value": 0.07759104547697836, "duration": 641.8427336215973, "step": 84500}
{"episode_reward": 903.0, "episode": 170.0, "Q1 loss": 0.3047268490076065, "Q2 loss": 0.3003759722828865, "Mean Target Q": 153.26249870605469, "Mean Q1": 153.2557469848633, "Mean Q2": 153.25549215698243, "critic_loss": 0.717855956017971, "batch_reward": 1.52664453125, "actor_loss": -153.2211371459961, "actor_target_entropy": -2.0, "actor_entropy": 0.5321013598442078, "alpha_loss": 0.00038844204950146377, "alpha_value": 0.0773429518677436, "step": 85000}
{"duration": 656.1789658069611, "step": 85000}
{"episode_reward": 904.0, "episode": 171.0, "Q1 loss": 0.29714246041178705, "Q2 loss": 0.29657816670536996, "Mean Target Q": 153.4664573791504, "Mean Q1": 153.46001065063476, "Mean Q2": 153.4597114013672, "critic_loss": 0.7087221558690071, "batch_reward": 1.53055078125, "actor_loss": -153.49907458496094, "actor_target_entropy": -2.0, "actor_entropy": 0.5114435324668885, "alpha_loss": 0.0001196559895761311, "alpha_value": 0.0773971745522043, "duration": 644.642148733139, "step": 85500}
{"episode_reward": 913.0, "episode": 172.0, "Q1 loss": 0.2930503072977066, "Q2 loss": 0.2905396228849888, "Mean Target Q": 153.63791220092773, "Mean Q1": 153.63075649414063, "Mean Q2": 153.63038525390624, "critic_loss": 0.6942299897670746, "batch_reward": 1.5290390625, "actor_loss": -153.60272998046875, "actor_target_entropy": -2.0, "actor_entropy": 0.5054499506950378, "alpha_loss": -0.0004656537033151835, "alpha_value": 0.07734462667312135, "duration": 640.9028205871582, "step": 86000}
{"episode_reward": 901.0, "episode": 173.0, "Q1 loss": 0.3052530269265175, "Q2 loss": 0.30380413031578063, "Mean Target Q": 153.83954840698243, "Mean Q1": 153.83336326904296, "Mean Q2": 153.8330402648926, "critic_loss": 0.7206195941567421, "batch_reward": 1.5324375, "actor_loss": -153.83039672851564, "actor_target_entropy": -2.0, "actor_entropy": 0.5177191319465637, "alpha_loss": -0.0004482577503658831, "alpha_value": 0.07744942986983552, "duration": 643.7700519561768, "step": 86500}
{"episode_reward": 904.0, "episode": 174.0, "Q1 loss": 0.2948902686238289, "Q2 loss": 0.2929577849686146, "Mean Target Q": 154.02230951538087, "Mean Q1": 154.01379182739257, "Mean Q2": 154.0142281677246, "critic_loss": 0.699032869875431, "batch_reward": 1.5357734375, "actor_loss": -153.97835760498046, "actor_target_entropy": -2.0, "actor_entropy": 0.5095433061122894, "alpha_loss": 0.0002893093570601195, "alpha_value": 0.0775186619053212, "duration": 641.4068486690521, "step": 87000}
{"episode_reward": 890.0, "episode": 175.0, "Q1 loss": 0.3027768058776856, "Q2 loss": 0.30111958602666855, "Mean Target Q": 154.20498629760743, "Mean Q1": 154.19817819213867, "Mean Q2": 154.19815703735352, "critic_loss": 0.7247080681920052, "batch_reward": 1.53568359375, "actor_loss": -154.19761853027345, "actor_target_entropy": -2.0, "actor_entropy": 0.5181999849081039, "alpha_loss": 0.0009572325686458498, "alpha_value": 0.07744497708605634, "duration": 708.488481760025, "step": 87500}
{"episode_reward": 901.0, "episode": 176.0, "Q1 loss": 0.3036813089609146, "Q2 loss": 0.30100476680397986, "Mean Target Q": 154.3670016845703, "Mean Q1": 154.36065166625977, "Mean Q2": 154.36078584594728, "critic_loss": 0.7249146683812141, "batch_reward": 1.53628515625, "actor_loss": -154.35992724609375, "actor_target_entropy": -2.0, "actor_entropy": 0.5101448458433151, "alpha_loss": -0.0001047745980322361, "alpha_value": 0.07728445511064348, "duration": 665.2729251384735, "step": 88000}
{"episode_reward": 891.0, "episode": 177.0, "Q1 loss": 0.31215231466293336, "Q2 loss": 0.3099878857433796, "Mean Target Q": 154.5525210144043, "Mean Q1": 154.5461012878418, "Mean Q2": 154.54663661499023, "critic_loss": 0.7417940341234207, "batch_reward": 1.5367421875, "actor_loss": -154.5233823852539, "actor_target_entropy": -2.0, "actor_entropy": 0.5054421277046204, "alpha_loss": -0.0013952159371692688, "alpha_value": 0.07750987914238185, "duration": 690.9428992271423, "step": 88500}
{"episode_reward": 912.0, "episode": 178.0, "Q1 loss": 0.3060129710197449, "Q2 loss": 0.301884285312891, "Mean Target Q": 154.71972829589845, "Mean Q1": 154.71394090576172, "Mean Q2": 154.71306322631835, "critic_loss": 0.7218819816112518, "batch_reward": 1.53953515625, "actor_loss": -154.68398541259765, "actor_target_entropy": -2.0, "actor_entropy": 0.5146728670597076, "alpha_loss": 0.0014988107688259334, "alpha_value": 0.0773935004418425, "duration": 1072.2742598056793, "step": 89000}
{"episode_reward": 893.0, "episode": 179.0, "Q1 loss": 0.30754409072995187, "Q2 loss": 0.30412504145503044, "Mean Target Q": 154.91248417358398, "Mean Q1": 154.9047007446289, "Mean Q2": 154.90474185791015, "critic_loss": 0.7221438483595848, "batch_reward": 1.54056640625, "actor_loss": -154.8723564453125, "actor_target_entropy": -2.0, "actor_entropy": 0.5275194321870804, "alpha_loss": -0.0005457932992139832, "alpha_value": 0.0772448672782176, "duration": 1520.5264520645142, "step": 89500}
{"episode_reward": 889.0, "episode": 180.0, "Q1 loss": 0.2962290364742279, "Q2 loss": 0.29203344727754593, "Mean Target Q": 155.06313958740233, "Mean Q1": 155.05613825073243, "Mean Q2": 155.05692210083006, "critic_loss": 0.7036767955422402, "batch_reward": 1.54008203125, "actor_loss": -154.99235284423827, "actor_target_entropy": -2.0, "actor_entropy": 0.5105696303844451, "alpha_loss": -0.00019805759470909834, "alpha_value": 0.07747855155576767, "step": 90000}
{"duration": 839.0012056827545, "step": 90000}
{"episode_reward": 915.0, "episode": 181.0, "Q1 loss": 0.2940856483161449, "Q2 loss": 0.29108970203995704, "Mean Target Q": 155.23599547119142, "Mean Q1": 155.2296630493164, "Mean Q2": 155.22998338623046, "critic_loss": 0.6957346088886261, "batch_reward": 1.54274609375, "actor_loss": -155.1530269165039, "actor_target_entropy": -2.0, "actor_entropy": 0.5020613839626312, "alpha_loss": 0.0010993037587031722, "alpha_value": 0.07735588983837179, "duration": 642.6874120235443, "step": 90500}
{"episode_reward": 910.0, "episode": 182.0, "Q1 loss": 0.2909603430151939, "Q2 loss": 0.288920032954216, "Mean Target Q": 155.43065583496093, "Mean Q1": 155.42514864501953, "Mean Q2": 155.4260515197754, "critic_loss": 0.6886138342618943, "batch_reward": 1.54413671875, "actor_loss": -155.30579901123048, "actor_target_entropy": -2.0, "actor_entropy": 0.49644830989837646, "alpha_loss": -0.0006660640484187752, "alpha_value": 0.07731223215094736, "duration": 642.2654707431793, "step": 91000}
{"episode_reward": 922.0, "episode": 183.0, "Q1 loss": 0.2935008275508881, "Q2 loss": 0.2895985257327557, "Mean Target Q": 155.56994729003907, "Mean Q1": 155.56236724853517, "Mean Q2": 155.5625380493164, "critic_loss": 0.69644098508358, "batch_reward": 1.5462109375, "actor_loss": -155.5000191040039, "actor_target_entropy": -2.0, "actor_entropy": 0.5082096707820892, "alpha_loss": 0.00018661598744802176, "alpha_value": 0.07729520393642388, "duration": 696.588894367218, "step": 91500}
{"episode_reward": 910.0, "episode": 184.0, "Q1 loss": 0.2903908742785454, "Q2 loss": 0.2875246039450169, "Mean Target Q": 155.75694614257813, "Mean Q1": 155.75091033325197, "Mean Q2": 155.75080422973633, "critic_loss": 0.6847246160507202, "batch_reward": 1.54994140625, "actor_loss": -155.71368634033203, "actor_target_entropy": -2.0, "actor_entropy": 0.5000748964548111, "alpha_loss": -0.00026183935871813445, "alpha_value": 0.07729233933166597, "duration": 643.4100029468536, "step": 92000}
{"episode_reward": 898.0, "episode": 185.0, "Q1 loss": 0.3021089090287685, "Q2 loss": 0.29970768083930016, "Mean Target Q": 155.91001994018555, "Mean Q1": 155.90277745361328, "Mean Q2": 155.90305555419923, "critic_loss": 0.7169073755145073, "batch_reward": 1.55012890625, "actor_loss": -155.8740863647461, "actor_target_entropy": -2.0, "actor_entropy": 0.4783388306498528, "alpha_loss": -0.00045659369090572, "alpha_value": 0.07745035472928556, "duration": 645.3439955711365, "step": 92500}
{"episode_reward": 886.0, "episode": 186.0, "Q1 loss": 0.2887710204184055, "Q2 loss": 0.28637367775440215, "Mean Target Q": 156.109199609375, "Mean Q1": 156.10319891357423, "Mean Q2": 156.10349778442384, "critic_loss": 0.6848913211822509, "batch_reward": 1.55297265625, "actor_loss": -155.99260052490234, "actor_target_entropy": -2.0, "actor_entropy": 0.48360523402690886, "alpha_loss": 0.00040516629815101624, "alpha_value": 0.07736580459009526, "duration": 643.3093616962433, "step": 93000}
{"episode_reward": 905.0, "episode": 187.0, "Q1 loss": 0.2950864595234394, "Q2 loss": 0.2921182677745819, "Mean Target Q": 156.22860806884765, "Mean Q1": 156.2209414123535, "Mean Q2": 156.2207826904297, "critic_loss": 0.7004971798658371, "batch_reward": 1.55297265625, "actor_loss": -156.0883310546875, "actor_target_entropy": -2.0, "actor_entropy": 0.48671751260757445, "alpha_loss": -0.001595673740375787, "alpha_value": 0.07752660648048328, "duration": 645.5075414180756, "step": 93500}
{"episode_reward": 935.0, "episode": 188.0, "Q1 loss": 0.28821807389855386, "Q2 loss": 0.2855941748678684, "Mean Target Q": 156.39240103149413, "Mean Q1": 156.38849185791017, "Mean Q2": 156.38892670288087, "critic_loss": 0.684125694155693, "batch_reward": 1.55533984375, "actor_loss": -156.3383447265625, "actor_target_entropy": -2.0, "actor_entropy": 0.47613103926181793, "alpha_loss": 0.00019380189594812692, "alpha_value": 0.07769019695400349, "duration": 643.0779893398285, "step": 94000}
{"episode_reward": 915.0, "episode": 189.0, "Q1 loss": 0.2826723181903362, "Q2 loss": 0.27969181905388835, "Mean Target Q": 156.54909329223634, "Mean Q1": 156.54456209106445, "Mean Q2": 156.5445786682129, "critic_loss": 0.662921389400959, "batch_reward": 1.55528125, "actor_loss": -156.5311636352539, "actor_target_entropy": -2.0, "actor_entropy": 0.4921835502386093, "alpha_loss": -0.0005557350376620889, "alpha_value": 0.07762686621263364, "duration": 644.1441638469696, "step": 94500}
{"episode_reward": 910.0, "episode": 190.0, "Q1 loss": 0.2777933368563652, "Q2 loss": 0.2752924331605434, "Mean Target Q": 156.70077243652344, "Mean Q1": 156.69276411743164, "Mean Q2": 156.69250744018555, "critic_loss": 0.6604929421544075, "batch_reward": 1.5550546875, "actor_loss": -156.6300150756836, "actor_target_entropy": -2.0, "actor_entropy": 0.49587314665317533, "alpha_loss": 0.0007979522298555822, "alpha_value": 0.07765383441482344, "step": 95000}
{"duration": 660.6507384777069, "step": 95000}
{"episode_reward": 926.0, "episode": 191.0, "Q1 loss": 0.2886640415430069, "Q2 loss": 0.2861846055150032, "Mean Target Q": 156.87205291748046, "Mean Q1": 156.86845411987304, "Mean Q2": 156.86777755737305, "critic_loss": 0.6804574063420296, "batch_reward": 1.55877734375, "actor_loss": -156.81359075927733, "actor_target_entropy": -2.0, "actor_entropy": 0.4829590747356415, "alpha_loss": -0.0003055634198244661, "alpha_value": 0.07762009111543101, "duration": 646.5105111598969, "step": 95500}
{"episode_reward": 912.0, "episode": 192.0, "Q1 loss": 0.29207412841916086, "Q2 loss": 0.2878774334847927, "Mean Target Q": 157.0008021484375, "Mean Q1": 156.99339008789062, "Mean Q2": 156.99364644775392, "critic_loss": 0.6960709659457207, "batch_reward": 1.55980859375, "actor_loss": -156.95753607177735, "actor_target_entropy": -2.0, "actor_entropy": 0.4811207357645035, "alpha_loss": -0.0021883862344548107, "alpha_value": 0.07800614269951692, "duration": 647.8646686077118, "step": 96000}
{"episode_reward": 920.0, "episode": 193.0, "Q1 loss": 0.28609920017123225, "Q2 loss": 0.2824672538518906, "Mean Target Q": 157.14235205688476, "Mean Q1": 157.13446279296875, "Mean Q2": 157.1353450744629, "critic_loss": 0.6702765924930573, "batch_reward": 1.55977734375, "actor_loss": -157.12304766845702, "actor_target_entropy": -2.0, "actor_entropy": 0.47833228993415833, "alpha_loss": -0.000526868409011513, "alpha_value": 0.07827725552616727, "duration": 646.4809319972992, "step": 96500}
{"episode_reward": 918.0, "episode": 194.0, "Q1 loss": 0.2912711850821972, "Q2 loss": 0.28808149658441545, "Mean Target Q": 157.26092873535157, "Mean Q1": 157.25457115478517, "Mean Q2": 157.25444592895508, "critic_loss": 0.6920573679208756, "batch_reward": 1.56271875, "actor_loss": -157.15664892578124, "actor_target_entropy": -2.0, "actor_entropy": 0.4747718617320061, "alpha_loss": -0.0005897178875748069, "alpha_value": 0.07832865467266137, "duration": 649.2254254817963, "step": 97000}
{"episode_reward": 898.0, "episode": 195.0, "Q1 loss": 0.27653313683867453, "Q2 loss": 0.27682024723887444, "Mean Target Q": 157.4021632080078, "Mean Q1": 157.39625606689452, "Mean Q2": 157.39590649414063, "critic_loss": 0.6557728044986725, "batch_reward": 1.5633359375, "actor_loss": -157.3243130493164, "actor_target_entropy": -2.0, "actor_entropy": 0.44609711265563967, "alpha_loss": 0.00022492202022112906, "alpha_value": 0.07845053599798943, "duration": 646.7568717002869, "step": 97500}
{"episode_reward": 921.0, "episode": 196.0, "Q1 loss": 0.28555370746850967, "Q2 loss": 0.2848768828213215, "Mean Target Q": 157.55043873291015, "Mean Q1": 157.54527363891603, "Mean Q2": 157.54510313720704, "critic_loss": 0.6829418799877167, "batch_reward": 1.56537890625, "actor_loss": -157.49136529541016, "actor_target_entropy": -2.0, "actor_entropy": 0.48483759582042696, "alpha_loss": 0.00018067188421264292, "alpha_value": 0.07836670174325218, "duration": 646.5667114257812, "step": 98000}
{"episode_reward": 908.0, "episode": 197.0, "Q1 loss": 0.2841042721390724, "Q2 loss": 0.28167475831508637, "Mean Target Q": 157.6428001098633, "Mean Q1": 157.63498026123048, "Mean Q2": 157.63537947998046, "critic_loss": 0.6880847296714783, "batch_reward": 1.56501171875, "actor_loss": -157.53871710205078, "actor_target_entropy": -2.0, "actor_entropy": 0.4888302826881409, "alpha_loss": 0.0006420968766324222, "alpha_value": 0.0782837985191657, "duration": 647.83806848526, "step": 98500}
{"episode_reward": 915.0, "episode": 198.0, "Q1 loss": 0.28265858502388, "Q2 loss": 0.278399438226223, "Mean Target Q": 157.77823275146486, "Mean Q1": 157.77328990478514, "Mean Q2": 157.77246252441407, "critic_loss": 0.6800534200072289, "batch_reward": 1.57194921875, "actor_loss": -157.6294393310547, "actor_target_entropy": -2.0, "actor_entropy": 0.47341080868244173, "alpha_loss": -0.0007001123349182308, "alpha_value": 0.07824388103063308, "duration": 646.9235277175903, "step": 99000}
{"episode_reward": 907.0, "episode": 199.0, "Q1 loss": 0.2767822700798512, "Q2 loss": 0.27324266402721403, "Mean Target Q": 157.89923911743165, "Mean Q1": 157.89193923339843, "Mean Q2": 157.89120919189452, "critic_loss": 0.658844600379467, "batch_reward": 1.57309765625, "actor_loss": -157.7574553833008, "actor_target_entropy": -2.0, "actor_entropy": 0.4803514229059219, "alpha_loss": -0.0005931046148762107, "alpha_value": 0.07843845631788933, "duration": 648.2718787193298, "step": 99500}
{"episode_reward": 893.0, "episode": 200.0, "Q1 loss": 0.2915442153721869, "Q2 loss": 0.28679918445542246, "Mean Target Q": 157.99647178879243, "Mean Q1": 157.98915487224448, "Mean Q2": 157.98852984900466, "critic_loss": 0.6894459424611323, "batch_reward": 1.573341996492986, "actor_loss": -157.9402060546875, "actor_target_entropy": -2.0, "actor_entropy": 0.4968549641370773, "alpha_loss": 0.002254395151278004, "alpha_value": 0.07817706200077255, "step": 99999}
