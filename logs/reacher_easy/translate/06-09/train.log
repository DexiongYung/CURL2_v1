{"episode_reward": 0.0, "episode": 1.0, "duration": 12.111485719680786, "step": 250}
{"episode_reward": 5.0, "episode": 2.0, "duration": 0.34078550338745117, "step": 500}
{"episode_reward": 0.0, "episode": 3.0, "duration": 0.3396589756011963, "step": 750}
{"episode_reward": 0.0, "episode": 4.0, "duration": 0.3396482467651367, "step": 1000}
{"episode_reward": 201.0, "episode": 5.0, "Q1 loss": 0.49358649158477785, "Q2 loss": 0.494388341486454, "Mean Target Q": 0.6168905985727906, "Mean Q1": 0.6090975458770991, "Mean Q2": 0.6101323621682823, "critic_loss": 0.9879748336672783, "batch_reward": 0.25259375, "actor_loss": -0.707464210793376, "actor_target_entropy": -2.0, "actor_entropy": 2.456848867982626, "alpha_loss": 0.31101093637943267, "alpha_value": 0.09941311242151749, "duration": 312.1434426307678, "step": 1250}
{"episode_reward": 98.0, "episode": 6.0, "Q1 loss": 0.2508692308366299, "Q2 loss": 0.24996097084879876, "Mean Target Q": 0.9651800379753113, "Mean Q1": 0.962435709476471, "Mean Q2": 0.962546021938324, "critic_loss": 0.5008302003145217, "batch_reward": 0.261984375, "actor_loss": -1.1078795642852783, "actor_target_entropy": -2.0, "actor_entropy": 2.529283521652222, "alpha_loss": 0.31342267847061156, "alpha_value": 0.09816220164808676, "duration": 263.19909262657166, "step": 1500}
{"episode_reward": 68.0, "episode": 7.0, "Q1 loss": 0.2892904123365879, "Q2 loss": 0.2927852176129818, "Mean Target Q": 1.2590291378498077, "Mean Q1": 1.2550646426677703, "Mean Q2": 1.2554433953762054, "critic_loss": 0.5820756301283836, "batch_reward": 0.2303515625, "actor_loss": -1.476381580352783, "actor_target_entropy": -2.0, "actor_entropy": 2.494290767669678, "alpha_loss": 0.28367015647888183, "alpha_value": 0.09702332539189207, "duration": 289.20352721214294, "step": 1750}
{"episode_reward": 24.0, "episode": 8.0, "Q1 loss": 0.22669626098871232, "Q2 loss": 0.22779930576682092, "Mean Target Q": 1.6898414087295532, "Mean Q1": 1.687567702293396, "Mean Q2": 1.687658462524414, "critic_loss": 0.45449556583166123, "batch_reward": 0.2206015625, "actor_loss": -2.0312484216690065, "actor_target_entropy": -2.0, "actor_entropy": 2.452799419403076, "alpha_loss": 0.2637241051197052, "alpha_value": 0.09593288365069469, "duration": 344.907931804657, "step": 2000}
{"episode_reward": 60.0, "episode": 9.0, "Q1 loss": 0.35287985491752627, "Q2 loss": 0.35706132650375366, "Mean Target Q": 2.231822920322418, "Mean Q1": 2.2281584696769716, "Mean Q2": 2.227945339679718, "critic_loss": 0.7099411829710006, "batch_reward": 0.2108984375, "actor_loss": -2.6407317543029785, "actor_target_entropy": -2.0, "actor_entropy": 2.3934986209869384, "alpha_loss": 0.24979660069942475, "alpha_value": 0.09489996139853815, "duration": 301.2464165687561, "step": 2250}
{"episode_reward": 27.0, "episode": 10.0, "Q1 loss": 1.0499551929235458, "Q2 loss": 1.0503097232580185, "Mean Target Q": 2.87864559841156, "Mean Q1": 2.8720847816467283, "Mean Q2": 2.8719109172821047, "critic_loss": 2.100264916777611, "batch_reward": 0.2035078125, "actor_loss": -3.2767116374969483, "actor_target_entropy": -2.0, "actor_entropy": 2.3675019340515138, "alpha_loss": 0.23349177825450898, "alpha_value": 0.09388179303753735, "duration": 188.658221244812, "step": 2500}
{"episode_reward": 5.0, "episode": 11.0, "Q1 loss": 0.7978357156515121, "Q2 loss": 0.8004257863759995, "Mean Target Q": 4.6363606414794925, "Mean Q1": 4.632817043304444, "Mean Q2": 4.632957592964172, "critic_loss": 1.59826149559021, "batch_reward": 0.328234375, "actor_loss": -5.266703020095825, "actor_target_entropy": -2.0, "actor_entropy": 2.2464103660583494, "alpha_loss": 0.18437210392951967, "alpha_value": 0.09297301295829624, "duration": 84.28021430969238, "step": 2750}
{"episode_reward": 868.0, "episode": 12.0, "Q1 loss": 0.9159127345085144, "Q2 loss": 0.9177394790649414, "Mean Target Q": 6.91433497428894, "Mean Q1": 6.910049556732178, "Mean Q2": 6.910574010848999, "critic_loss": 1.8336522047519683, "batch_reward": 0.4765, "actor_loss": -7.650418285369873, "actor_target_entropy": -2.0, "actor_entropy": 2.1831642723083498, "alpha_loss": 0.15015570813417434, "alpha_value": 0.09224055868052225, "duration": 67.53420615196228, "step": 3000}
{"episode_reward": 5.0, "episode": 13.0, "Q1 loss": 1.1313400046825408, "Q2 loss": 1.1299650168418884, "Mean Target Q": 7.8961515007019045, "Mean Q1": 7.892093118667603, "Mean Q2": 7.892127620697021, "critic_loss": 2.2613050203323364, "batch_reward": 0.4388984375, "actor_loss": -8.798036350250245, "actor_target_entropy": -2.0, "actor_entropy": 2.1286077566146853, "alpha_loss": 0.15057324635982514, "alpha_value": 0.0915319211447094, "duration": 68.61812543869019, "step": 3250}
{"episode_reward": 6.0, "episode": 14.0, "Q1 loss": 1.6028957741260528, "Q2 loss": 1.6007344782352448, "Mean Target Q": 9.093725065231324, "Mean Q1": 9.089472419738769, "Mean Q2": 9.089395013809204, "critic_loss": 3.2036302433013915, "batch_reward": 0.4086484375, "actor_loss": -9.968240856170654, "actor_target_entropy": -2.0, "actor_entropy": 2.1124731760025024, "alpha_loss": 0.14788106590509414, "alpha_value": 0.0907801157562468, "duration": 106.85791659355164, "step": 3500}
{"episode_reward": 0.0, "episode": 15.0, "Q1 loss": 2.149831063747406, "Q2 loss": 2.141046571731567, "Mean Target Q": 11.080036331176759, "Mean Q1": 11.073709720611573, "Mean Q2": 11.074343654632568, "critic_loss": 4.290877639770508, "batch_reward": 0.377171875, "actor_loss": -12.151401672363281, "actor_target_entropy": -2.0, "actor_entropy": 2.0342057332992556, "alpha_loss": 0.12483136200904846, "alpha_value": 0.09005354405407937, "duration": 81.62280011177063, "step": 3750}
{"episode_reward": 13.0, "episode": 16.0, "Q1 loss": 2.32374037027359, "Q2 loss": 2.303948422431946, "Mean Target Q": 13.93485453414917, "Mean Q1": 13.92822551727295, "Mean Q2": 13.928236598968505, "critic_loss": 4.627688778877259, "batch_reward": 0.3664765625, "actor_loss": -15.20692847442627, "actor_target_entropy": -2.0, "actor_entropy": 1.8939833936691284, "alpha_loss": 0.09396353283524514, "alpha_value": 0.0894591511116413, "duration": 68.84345293045044, "step": 4000}
{"episode_reward": 6.0, "episode": 17.0, "Q1 loss": 2.1485424876213073, "Q2 loss": 2.1460652194023133, "Mean Target Q": 15.64432862854004, "Mean Q1": 15.641681850433349, "Mean Q2": 15.642379554748535, "critic_loss": 4.294607707023621, "batch_reward": 0.33803125, "actor_loss": -16.95525679779053, "actor_target_entropy": -2.0, "actor_entropy": 1.8179085016250611, "alpha_loss": 0.08433576720952987, "alpha_value": 0.0889549321211771, "duration": 67.31622838973999, "step": 4250}
{"episode_reward": 0.0, "episode": 18.0, "Q1 loss": 2.122508900642395, "Q2 loss": 2.1164593811035157, "Mean Target Q": 17.01990859222412, "Mean Q1": 17.01428276824951, "Mean Q2": 17.0145576171875, "critic_loss": 4.238968286514282, "batch_reward": 0.3165859375, "actor_loss": -18.433143463134765, "actor_target_entropy": -2.0, "actor_entropy": 1.7765588083267212, "alpha_loss": 0.06676805263757705, "alpha_value": 0.08850218361162623, "duration": 67.36071419715881, "step": 4500}
{"episode_reward": 23.0, "episode": 19.0, "Q1 loss": 1.905649405002594, "Q2 loss": 1.9000307636260987, "Mean Target Q": 18.721503635406496, "Mean Q1": 18.717803733825683, "Mean Q2": 18.718180507659913, "critic_loss": 3.8056801719665527, "batch_reward": 0.3175234375, "actor_loss": -20.38113525390625, "actor_target_entropy": -2.0, "actor_entropy": 1.7525756692886352, "alpha_loss": 0.06271187981963158, "alpha_value": 0.08807933935211779, "duration": 69.07593989372253, "step": 4750}
{"episode_reward": 298.0, "episode": 20.0, "Q1 loss": 1.8738032474517823, "Q2 loss": 1.880563862323761, "Mean Target Q": 20.06515328216553, "Mean Q1": 20.060470352172853, "Mean Q2": 20.060580513000488, "critic_loss": 3.7543671102523803, "batch_reward": 0.3552578125, "actor_loss": -21.530224182128908, "actor_target_entropy": -2.0, "actor_entropy": 1.7835809202194215, "alpha_loss": 0.0657354584634304, "alpha_value": 0.08764369119363769, "step": 5000}
{"duration": 75.63137626647949, "step": 5000}
{"episode_reward": 38.0, "episode": 21.0, "Q1 loss": 2.7764716000556944, "Q2 loss": 2.7423897218704223, "Mean Target Q": 22.856940063476564, "Mean Q1": 22.851695625305176, "Mean Q2": 22.852370681762697, "critic_loss": 5.518861315727234, "batch_reward": 0.422296875, "actor_loss": -24.350346618652345, "actor_target_entropy": -2.0, "actor_entropy": 1.7348788251876832, "alpha_loss": 0.06693779963254928, "alpha_value": 0.08715922185804037, "duration": 69.05884265899658, "step": 5250}
{"episode_reward": 913.0, "episode": 22.0, "Q1 loss": 2.14037593793869, "Q2 loss": 2.1587804474830627, "Mean Target Q": 25.65922142791748, "Mean Q1": 25.654009376525877, "Mean Q2": 25.65459288787842, "critic_loss": 4.299156386375428, "batch_reward": 0.4916015625, "actor_loss": -27.13997801208496, "actor_target_entropy": -2.0, "actor_entropy": 1.700474925994873, "alpha_loss": 0.07012233817577362, "alpha_value": 0.08662320445855513, "duration": 105.1129412651062, "step": 5500}
{"episode_reward": 5.0, "episode": 23.0, "Q1 loss": 1.832344150543213, "Q2 loss": 1.854936448097229, "Mean Target Q": 27.97740074920654, "Mean Q1": 27.974256217956544, "Mean Q2": 27.974534065246583, "critic_loss": 3.6872805948257446, "batch_reward": 0.507640625, "actor_loss": -29.33695231628418, "actor_target_entropy": -2.0, "actor_entropy": 1.7255724983215333, "alpha_loss": 0.06568607836961746, "alpha_value": 0.08608582521442286, "duration": 355.3994789123535, "step": 5750}
{"episode_reward": 434.0, "episode": 24.0, "Q1 loss": 2.0321243324279785, "Q2 loss": 2.0343103251457215, "Mean Target Q": 32.13339061737061, "Mean Q1": 32.1286897354126, "Mean Q2": 32.128802085876465, "critic_loss": 4.066434659957886, "batch_reward": 0.6143359375, "actor_loss": -33.53968124389648, "actor_target_entropy": -2.0, "actor_entropy": 1.5871691217422486, "alpha_loss": 0.05227831269800663, "alpha_value": 0.08557807254822322, "duration": 151.8775818347931, "step": 6000}
{"episode_reward": 982.0, "episode": 25.0, "Q1 loss": 3.2489189143180845, "Q2 loss": 3.246552435874939, "Mean Target Q": 36.38990106964111, "Mean Q1": 36.38572227478027, "Mean Q2": 36.3847463760376, "critic_loss": 6.495471349716187, "batch_reward": 0.72421875, "actor_loss": -37.86643453979492, "actor_target_entropy": -2.0, "actor_entropy": 1.4787902545928955, "alpha_loss": 0.036894073462113736, "alpha_value": 0.08519548485274489, "duration": 67.42803025245667, "step": 6250}
{"episode_reward": 860.0, "episode": 26.0, "Q1 loss": 1.8920777034759522, "Q2 loss": 1.8784805607795716, "Mean Target Q": 39.41237034606934, "Mean Q1": 39.40939588928223, "Mean Q2": 39.40970658874512, "critic_loss": 3.7705582685470582, "batch_reward": 0.7751015625, "actor_loss": -41.07027493286133, "actor_target_entropy": -2.0, "actor_entropy": 1.4349555387496948, "alpha_loss": 0.030795708177611233, "alpha_value": 0.08487523436519194, "duration": 72.8297049999237, "step": 6500}
{"episode_reward": 40.0, "episode": 27.0, "Q1 loss": 1.7999727907180787, "Q2 loss": 1.8035929646492004, "Mean Target Q": 41.31111344909668, "Mean Q1": 41.308552947998045, "Mean Q2": 41.309097885131834, "critic_loss": 3.6035657472610474, "batch_reward": 0.752078125, "actor_loss": -43.04888348388672, "actor_target_entropy": -2.0, "actor_entropy": 1.4694733247756957, "alpha_loss": 0.029005065171048044, "alpha_value": 0.08459578362921345, "duration": 110.70045948028564, "step": 6750}
{"episode_reward": 42.0, "episode": 28.0, "Q1 loss": 1.9213928880691529, "Q2 loss": 1.91065971326828, "Mean Target Q": 43.72256135559082, "Mean Q1": 43.7196408996582, "Mean Q2": 43.719177047729495, "critic_loss": 3.8320525999069215, "batch_reward": 0.7411875, "actor_loss": -45.2129309387207, "actor_target_entropy": -2.0, "actor_entropy": 1.4492866916656495, "alpha_loss": 0.03272824951261282, "alpha_value": 0.08426544609065081, "duration": 72.68194508552551, "step": 7000}
{"episode_reward": 183.0, "episode": 29.0, "Q1 loss": 2.0471736974716186, "Q2 loss": 2.033803518772125, "Mean Target Q": 46.72443688964844, "Mean Q1": 46.71991888427734, "Mean Q2": 46.720382263183595, "critic_loss": 4.080977235794068, "batch_reward": 0.747203125, "actor_loss": -48.23211752319336, "actor_target_entropy": -2.0, "actor_entropy": 1.4022472944259643, "alpha_loss": 0.032863246396183966, "alpha_value": 0.08391140878435456, "duration": 67.55778288841248, "step": 7250}
{"episode_reward": 144.0, "episode": 30.0, "Q1 loss": 2.138576560020447, "Q2 loss": 2.1376502408981324, "Mean Target Q": 49.08987413024902, "Mean Q1": 49.08870825195312, "Mean Q2": 49.08802410888672, "critic_loss": 4.276226788520813, "batch_reward": 0.7316875, "actor_loss": -50.600613494873045, "actor_target_entropy": -2.0, "actor_entropy": 1.343335039138794, "alpha_loss": 0.03587732351571322, "alpha_value": 0.08351687495446306, "duration": 67.4239866733551, "step": 7500}
{"episode_reward": 4.0, "episode": 31.0, "Q1 loss": 1.9731962423324585, "Q2 loss": 1.9662857995033265, "Mean Target Q": 51.84459649658203, "Mean Q1": 51.84176354980469, "Mean Q2": 51.84197981262207, "critic_loss": 3.9394820470809937, "batch_reward": 0.70284375, "actor_loss": -53.58396664428711, "actor_target_entropy": -2.0, "actor_entropy": 1.3065744676589965, "alpha_loss": 0.03936732390522957, "alpha_value": 0.08305195430587375, "duration": 67.6113133430481, "step": 7750}
{"episode_reward": 6.0, "episode": 32.0, "Q1 loss": 1.7354831056594848, "Q2 loss": 1.7441398282051086, "Mean Target Q": 53.790743103027346, "Mean Q1": 53.78722770690918, "Mean Q2": 53.78732241821289, "critic_loss": 3.47962292098999, "batch_reward": 0.6837890625, "actor_loss": -55.367299438476564, "actor_target_entropy": -2.0, "actor_entropy": 1.3300546989440918, "alpha_loss": 0.04059665904566646, "alpha_value": 0.08254125897532968, "duration": 67.4236569404602, "step": 8000}
{"episode_reward": 28.0, "episode": 33.0, "Q1 loss": 1.5982164001464845, "Q2 loss": 1.5976860966682433, "Mean Target Q": 55.98296203613281, "Mean Q1": 55.98144340515137, "Mean Q2": 55.98096383666992, "critic_loss": 3.1959025020599365, "batch_reward": 0.7189375, "actor_loss": -57.33876013183594, "actor_target_entropy": -2.0, "actor_entropy": 1.3367037191390991, "alpha_loss": 0.03974077782034874, "alpha_value": 0.08202013441723971, "duration": 68.33110904693604, "step": 8250}
{"episode_reward": 926.0, "episode": 34.0, "Q1 loss": 1.55476122713089, "Q2 loss": 1.5693613929748536, "Mean Target Q": 57.91065730285644, "Mean Q1": 57.90798922729492, "Mean Q2": 57.90841902160645, "critic_loss": 3.1241226205825807, "batch_reward": 0.75103125, "actor_loss": -59.138353149414066, "actor_target_entropy": -2.0, "actor_entropy": 1.3708290319442749, "alpha_loss": 0.04443525487184524, "alpha_value": 0.08142864215085692, "duration": 68.16048240661621, "step": 8500}
{"episode_reward": 21.0, "episode": 35.0, "Q1 loss": 1.449756826877594, "Q2 loss": 1.451603423833847, "Mean Target Q": 58.66043826293945, "Mean Q1": 58.657592224121096, "Mean Q2": 58.65712150573731, "critic_loss": 2.9013602447509768, "batch_reward": 0.7434765625, "actor_loss": -60.048737396240234, "actor_target_entropy": -2.0, "actor_entropy": 1.3877850914001464, "alpha_loss": 0.04417073249816895, "alpha_value": 0.08079436353998884, "duration": 67.6168863773346, "step": 8750}
{"episode_reward": 24.0, "episode": 36.0, "Q1 loss": 1.5144697308540345, "Q2 loss": 1.532023323535919, "Mean Target Q": 59.34731188964844, "Mean Q1": 59.344242385864256, "Mean Q2": 59.34441569519043, "critic_loss": 3.046493048667908, "batch_reward": 0.7300390625, "actor_loss": -60.74233233642578, "actor_target_entropy": -2.0, "actor_entropy": 1.4402885942459107, "alpha_loss": 0.037097445517778396, "alpha_value": 0.08020472865337272, "duration": 67.62852191925049, "step": 9000}
{"episode_reward": 248.0, "episode": 37.0, "Q1 loss": 1.511941313266754, "Q2 loss": 1.5197539620399476, "Mean Target Q": 59.918119323730465, "Mean Q1": 59.91721347045898, "Mean Q2": 59.917514297485354, "critic_loss": 3.03169527053833, "batch_reward": 0.726546875, "actor_loss": -61.371965057373046, "actor_target_entropy": -2.0, "actor_entropy": 1.3745295963287354, "alpha_loss": 0.03776422169059515, "alpha_value": 0.07963169069585088, "duration": 67.78273272514343, "step": 9250}
{"episode_reward": 431.0, "episode": 38.0, "Q1 loss": 1.6808560495376588, "Q2 loss": 1.6818334684371947, "Mean Target Q": 62.51353840637207, "Mean Q1": 62.51211404418945, "Mean Q2": 62.51167008972168, "critic_loss": 3.3626895217895507, "batch_reward": 0.793078125, "actor_loss": -63.67838317871094, "actor_target_entropy": -2.0, "actor_entropy": 1.3256084785461426, "alpha_loss": 0.03216480853036046, "alpha_value": 0.07907369389563705, "duration": 67.63571071624756, "step": 9500}
{"episode_reward": 968.0, "episode": 39.0, "Q1 loss": 1.7966829609870911, "Q2 loss": 1.7961827325820923, "Mean Target Q": 64.98695079040527, "Mean Q1": 64.98431932067871, "Mean Q2": 64.98425367736816, "critic_loss": 3.592865698814392, "batch_reward": 0.8338203125, "actor_loss": -66.09640982055664, "actor_target_entropy": -2.0, "actor_entropy": 1.2304790811538697, "alpha_loss": 0.026340763850137592, "alpha_value": 0.07860748653024076, "duration": 67.60581755638123, "step": 9750}
{"episode_reward": 11.0, "episode": 40.0, "Q1 loss": 1.9248622512817384, "Q2 loss": 1.938925669670105, "Mean Target Q": 66.16310255432128, "Mean Q1": 66.15969416809082, "Mean Q2": 66.16038067626953, "critic_loss": 3.863787919998169, "batch_reward": 0.816296875, "actor_loss": -67.63139752197266, "actor_target_entropy": -2.0, "actor_entropy": 1.1978781032562256, "alpha_loss": 0.017037427263334393, "alpha_value": 0.07823707966106094, "step": 10000}
{"duration": 76.20027709007263, "step": 10000}
{"episode_reward": 16.0, "episode": 41.0, "Q1 loss": 1.983532377243042, "Q2 loss": 1.9959972596168518, "Mean Target Q": 68.81210612487793, "Mean Q1": 68.80920901489257, "Mean Q2": 68.80983325195312, "critic_loss": 3.9795296449661253, "batch_reward": 0.8461015625, "actor_loss": -70.06387817382813, "actor_target_entropy": -2.0, "actor_entropy": 1.1514500770568847, "alpha_loss": 0.01611662852857262, "alpha_value": 0.07794607117262621, "duration": 67.57017803192139, "step": 10250}
{"episode_reward": 936.0, "episode": 42.0, "Q1 loss": 2.062127564430237, "Q2 loss": 2.0528850808143617, "Mean Target Q": 72.68377157592774, "Mean Q1": 72.68036697387696, "Mean Q2": 72.68115936279297, "critic_loss": 4.115012637138367, "batch_reward": 0.913859375, "actor_loss": -73.90939782714844, "actor_target_entropy": -2.0, "actor_entropy": 1.138074517250061, "alpha_loss": 0.010723064922727645, "alpha_value": 0.07770891803090402, "duration": 67.82789754867554, "step": 10500}
{"episode_reward": 987.0, "episode": 43.0, "Q1 loss": 2.055034379005432, "Q2 loss": 2.0584077215194703, "Mean Target Q": 76.98731408691407, "Mean Q1": 76.98477709960937, "Mean Q2": 76.98575650024414, "critic_loss": 4.11344208240509, "batch_reward": 0.977515625, "actor_loss": -78.490560546875, "actor_target_entropy": -2.0, "actor_entropy": 1.1050842399597167, "alpha_loss": 0.009736319535411894, "alpha_value": 0.07749098170819724, "duration": 67.54719519615173, "step": 10750}
{"episode_reward": 940.0, "episode": 44.0, "Q1 loss": 2.1286729679107665, "Q2 loss": 2.1465698399543762, "Mean Target Q": 79.37832348632813, "Mean Q1": 79.37644198608399, "Mean Q2": 79.3764344482422, "critic_loss": 4.275242809295654, "batch_reward": 1.002953125, "actor_loss": -80.88813403320313, "actor_target_entropy": -2.0, "actor_entropy": 1.1257694873809814, "alpha_loss": 0.010361381940543651, "alpha_value": 0.07729356475999188, "duration": 67.66418504714966, "step": 11000}
{"episode_reward": 0.0, "episode": 45.0, "Q1 loss": 2.184613521575928, "Q2 loss": 2.1914813475608828, "Mean Target Q": 81.75979830932617, "Mean Q1": 81.75780868530273, "Mean Q2": 81.75826031494141, "critic_loss": 4.376094882965088, "batch_reward": 0.9832109375, "actor_loss": -83.16612811279298, "actor_target_entropy": -2.0, "actor_entropy": 1.1581930480003357, "alpha_loss": 0.014026980035007, "alpha_value": 0.07699469457527677, "duration": 67.84280467033386, "step": 11250}
{"episode_reward": 0.0, "episode": 46.0, "Q1 loss": 2.2302306327819825, "Q2 loss": 2.2364743723869323, "Mean Target Q": 85.30303665161132, "Mean Q1": 85.30068728637696, "Mean Q2": 85.30073287963867, "critic_loss": 4.466705008506775, "batch_reward": 0.9957265625, "actor_loss": -87.04440905761719, "actor_target_entropy": -2.0, "actor_entropy": 1.1872331585884095, "alpha_loss": 0.009381991418078542, "alpha_value": 0.07673545899572436, "duration": 67.5793149471283, "step": 11500}
{"episode_reward": 987.0, "episode": 47.0, "Q1 loss": 2.3045638694763184, "Q2 loss": 2.3102094893455507, "Mean Target Q": 89.43592446899414, "Mean Q1": 89.43278768920898, "Mean Q2": 89.4332121887207, "critic_loss": 4.614773372650147, "batch_reward": 1.05753125, "actor_loss": -90.7344574584961, "actor_target_entropy": -2.0, "actor_entropy": 1.2199654760360719, "alpha_loss": 0.006778190100565552, "alpha_value": 0.07654711103516537, "duration": 67.87893748283386, "step": 11750}
{"episode_reward": 960.0, "episode": 48.0, "Q1 loss": 2.4879796004295347, "Q2 loss": 2.488175721645355, "Mean Target Q": 92.89853045654297, "Mean Q1": 92.89717541503906, "Mean Q2": 92.89620742797851, "critic_loss": 4.976155309677124, "batch_reward": 1.099609375, "actor_loss": -94.58241339111328, "actor_target_entropy": -2.0, "actor_entropy": 1.214242642402649, "alpha_loss": 0.006680650180205703, "alpha_value": 0.07634960900931474, "duration": 67.63918018341064, "step": 12000}
{"episode_reward": 623.0, "episode": 49.0, "Q1 loss": 2.5559240355491637, "Q2 loss": 2.541550215244293, "Mean Target Q": 96.37477871704101, "Mean Q1": 96.37313696289063, "Mean Q2": 96.37334002685547, "critic_loss": 5.097474232673645, "batch_reward": 1.15065625, "actor_loss": -98.24785516357421, "actor_target_entropy": -2.0, "actor_entropy": 1.1981521377563475, "alpha_loss": 0.007311673904769123, "alpha_value": 0.0762115545367261, "duration": 68.59899020195007, "step": 12250}
{"episode_reward": 943.0, "episode": 50.0, "Q1 loss": 2.8476787600517275, "Q2 loss": 2.838749628543854, "Mean Target Q": 99.24088809204102, "Mean Q1": 99.23646948242188, "Mean Q2": 99.23706936645507, "critic_loss": 5.686428398132324, "batch_reward": 1.1723203125, "actor_loss": -100.63527551269532, "actor_target_entropy": -2.0, "actor_entropy": 1.1861480197906493, "alpha_loss": 0.004970015965402127, "alpha_value": 0.0759498916845703, "duration": 68.38503861427307, "step": 12500}
{"episode_reward": 395.0, "episode": 51.0, "Q1 loss": 3.189032513618469, "Q2 loss": 3.1811563272476198, "Mean Target Q": 101.49519491577148, "Mean Q1": 101.4938765258789, "Mean Q2": 101.49362911987305, "critic_loss": 6.370188851356506, "batch_reward": 1.1746953125, "actor_loss": -103.36475286865235, "actor_target_entropy": -2.0, "actor_entropy": 1.1662014074325562, "alpha_loss": 0.0011933881789445878, "alpha_value": 0.07589540147630564, "duration": 67.65352177619934, "step": 12750}
{"episode_reward": 20.0, "episode": 52.0, "Q1 loss": 3.099872597694397, "Q2 loss": 3.0618707180023192, "Mean Target Q": 102.74438809204102, "Mean Q1": 102.74238967895508, "Mean Q2": 102.74221762084962, "critic_loss": 6.161743307113648, "batch_reward": 1.15253125, "actor_loss": -104.22361846923827, "actor_target_entropy": -2.0, "actor_entropy": 1.1200903220176697, "alpha_loss": -0.0012137905899435282, "alpha_value": 0.07587685159575469, "duration": 67.77721619606018, "step": 13000}
{"episode_reward": 10.0, "episode": 53.0, "Q1 loss": 3.133998331069946, "Q2 loss": 3.153033148765564, "Mean Target Q": 103.3471755065918, "Mean Q1": 103.34377331542969, "Mean Q2": 103.34418444824219, "critic_loss": 6.287031475067138, "batch_reward": 1.130109375, "actor_loss": -105.12607586669922, "actor_target_entropy": -2.0, "actor_entropy": 1.150209602355957, "alpha_loss": -0.00048527913447469475, "alpha_value": 0.07594730413720506, "duration": 67.98845481872559, "step": 13250}
{"episode_reward": 6.0, "episode": 54.0, "Q1 loss": 3.056296091079712, "Q2 loss": 3.055294196128845, "Mean Target Q": 104.26568682861328, "Mean Q1": 104.2642565612793, "Mean Q2": 104.26268710327149, "critic_loss": 6.1115902881622315, "batch_reward": 1.11403125, "actor_loss": -105.63607434082031, "actor_target_entropy": -2.0, "actor_entropy": 1.1804161310195922, "alpha_loss": 0.003802943492308259, "alpha_value": 0.07584791245265209, "duration": 68.01513719558716, "step": 13500}
{"episode_reward": 11.0, "episode": 55.0, "Q1 loss": 3.0023360786437987, "Q2 loss": 2.9860006294250487, "Mean Target Q": 106.6074200744629, "Mean Q1": 106.60425103759766, "Mean Q2": 106.60416439819336, "critic_loss": 5.98833671760559, "batch_reward": 1.120890625, "actor_loss": -107.89365301513672, "actor_target_entropy": -2.0, "actor_entropy": 1.1840021371841432, "alpha_loss": 0.008486437662504613, "alpha_value": 0.07564486328733076, "duration": 67.91153573989868, "step": 13750}
{"episode_reward": 896.0, "episode": 56.0, "Q1 loss": 3.4805076665878296, "Q2 loss": 3.4538626642227173, "Mean Target Q": 109.60367077636718, "Mean Q1": 109.60099301147461, "Mean Q2": 109.60120510864257, "critic_loss": 6.934370332717895, "batch_reward": 1.1571640625, "actor_loss": -110.75469226074219, "actor_target_entropy": -2.0, "actor_entropy": 1.1628233633041383, "alpha_loss": 0.006080773121211678, "alpha_value": 0.07534604571335375, "duration": 68.11063075065613, "step": 14000}
{"episode_reward": 464.0, "episode": 57.0, "Q1 loss": 3.396062924385071, "Q2 loss": 3.4018114156723023, "Mean Target Q": 110.8813016052246, "Mean Q1": 110.87812094116211, "Mean Q2": 110.8782156677246, "critic_loss": 6.797874336242676, "batch_reward": 1.152515625, "actor_loss": -112.38650311279297, "actor_target_entropy": -2.0, "actor_entropy": 1.1627225708961486, "alpha_loss": 0.0022685538125224412, "alpha_value": 0.07516187059504367, "duration": 67.88886523246765, "step": 14250}
{"episode_reward": 42.0, "episode": 58.0, "Q1 loss": 3.348533667564392, "Q2 loss": 3.347561381340027, "Mean Target Q": 112.0172946472168, "Mean Q1": 112.01501239013672, "Mean Q2": 112.01488424682617, "critic_loss": 6.696095054626465, "batch_reward": 1.13375, "actor_loss": -113.50045904541015, "actor_target_entropy": -2.0, "actor_entropy": 1.181764612197876, "alpha_loss": 0.007019161340780556, "alpha_value": 0.07497602179944816, "duration": 68.19797110557556, "step": 14500}
{"episode_reward": 20.0, "episode": 59.0, "Q1 loss": 3.169187659263611, "Q2 loss": 3.177301851272583, "Mean Target Q": 113.7221983642578, "Mean Q1": 113.72112484741211, "Mean Q2": 113.7203648071289, "critic_loss": 6.346489511489868, "batch_reward": 1.14015625, "actor_loss": -114.80128869628906, "actor_target_entropy": -2.0, "actor_entropy": 1.126888442993164, "alpha_loss": 0.006279842600692064, "alpha_value": 0.07468986549109335, "duration": 68.19400310516357, "step": 14750}
{"episode_reward": 845.0, "episode": 60.0, "Q1 loss": 3.1759844827651977, "Q2 loss": 3.175065351486206, "Mean Target Q": 115.70716870117188, "Mean Q1": 115.70425897216796, "Mean Q2": 115.70371310424805, "critic_loss": 6.351049833297729, "batch_reward": 1.15640625, "actor_loss": -116.89536224365234, "actor_target_entropy": -2.0, "actor_entropy": 1.1625844359397888, "alpha_loss": 0.00980101111996919, "alpha_value": 0.07433669528871621, "step": 15000}
{"duration": 76.0807638168335, "step": 15000}
{"episode_reward": 13.0, "episode": 61.0, "Q1 loss": 2.9809948177337646, "Q2 loss": 3.004954759597778, "Mean Target Q": 116.14350021362304, "Mean Q1": 116.14215469360352, "Mean Q2": 116.14226565551758, "critic_loss": 5.985949586868286, "batch_reward": 1.1339453125, "actor_loss": -117.4093892211914, "actor_target_entropy": -2.0, "actor_entropy": 1.1929269914627074, "alpha_loss": 0.006726372099481523, "alpha_value": 0.0739035158427295, "duration": 67.96877789497375, "step": 15250}
{"episode_reward": 0.0, "episode": 62.0, "Q1 loss": 2.906690574645996, "Q2 loss": 2.9179483914375304, "Mean Target Q": 116.31474670410157, "Mean Q1": 116.31391342163086, "Mean Q2": 116.31366168212891, "critic_loss": 5.824638956069946, "batch_reward": 1.108546875, "actor_loss": -117.46650457763671, "actor_target_entropy": -2.0, "actor_entropy": 1.1960088057518006, "alpha_loss": 0.003832813600078225, "alpha_value": 0.07366034119836705, "duration": 67.85262989997864, "step": 15500}
{"episode_reward": 3.0, "episode": 63.0, "Q1 loss": 2.9309857110977173, "Q2 loss": 2.944023052692413, "Mean Target Q": 117.7798166809082, "Mean Q1": 117.77657263183593, "Mean Q2": 117.77723022460937, "critic_loss": 5.875008775711059, "batch_reward": 1.0967578125, "actor_loss": -119.37055694580079, "actor_target_entropy": -2.0, "actor_entropy": 1.2499195003509522, "alpha_loss": 0.002181745385751128, "alpha_value": 0.07345799836922888, "duration": 68.81880640983582, "step": 15750}
{"episode_reward": 35.0, "episode": 64.0, "Q1 loss": 2.972887011528015, "Q2 loss": 3.0058229637145994, "Mean Target Q": 120.26258877563477, "Mean Q1": 120.25975906372071, "Mean Q2": 120.26071447753907, "critic_loss": 5.978710006713867, "batch_reward": 1.105046875, "actor_loss": -121.9365912475586, "actor_target_entropy": -2.0, "actor_entropy": 1.213666877746582, "alpha_loss": 0.000767853073310107, "alpha_value": 0.07340487556552655, "duration": 67.98931288719177, "step": 16000}
{"episode_reward": 854.0, "episode": 65.0, "Q1 loss": 3.0080871362686157, "Q2 loss": 3.0291703081130983, "Mean Target Q": 123.98187225341798, "Mean Q1": 123.98103909301757, "Mean Q2": 123.98055044555664, "critic_loss": 6.037257446289063, "batch_reward": 1.15128125, "actor_loss": -125.72204193115235, "actor_target_entropy": -2.0, "actor_entropy": 1.2147809162139893, "alpha_loss": -0.001711281631141901, "alpha_value": 0.07346609319571096, "duration": 68.13681483268738, "step": 16250}
{"episode_reward": 985.0, "episode": 66.0, "Q1 loss": 2.9795676279067993, "Q2 loss": 2.989800602912903, "Mean Target Q": 127.24098498535156, "Mean Q1": 127.23754943847656, "Mean Q2": 127.23764453125, "critic_loss": 5.969368234634399, "batch_reward": 1.1804765625, "actor_loss": -128.82691455078125, "actor_target_entropy": -2.0, "actor_entropy": 1.156791181564331, "alpha_loss": -0.0020575418407097457, "alpha_value": 0.07356669877745527, "duration": 68.47308731079102, "step": 16500}
{"episode_reward": 45.0, "episode": 67.0, "Q1 loss": 3.049429461479187, "Q2 loss": 3.0393471851348877, "Mean Target Q": 128.11475537109374, "Mean Q1": 128.11184588623047, "Mean Q2": 128.11271139526366, "critic_loss": 6.088776641845703, "batch_reward": 1.15621875, "actor_loss": -129.47110827636718, "actor_target_entropy": -2.0, "actor_entropy": 1.1698349013328553, "alpha_loss": -0.0015082227336242794, "alpha_value": 0.07365062591088069, "duration": 68.80297017097473, "step": 16750}
{"episode_reward": 566.0, "episode": 68.0, "Q1 loss": 3.06144029712677, "Q2 loss": 3.0713519172668455, "Mean Target Q": 131.2128265686035, "Mean Q1": 131.20995822143556, "Mean Q2": 131.2091687927246, "critic_loss": 6.132792198181153, "batch_reward": 1.1993671875, "actor_loss": -132.58925885009765, "actor_target_entropy": -2.0, "actor_entropy": 1.2032082748413087, "alpha_loss": -0.0004921908378601075, "alpha_value": 0.07374696950247848, "duration": 68.59510540962219, "step": 17000}
{"episode_reward": 987.0, "episode": 69.0, "Q1 loss": 2.9808240871429446, "Q2 loss": 3.0048027820587158, "Mean Target Q": 134.75230081176758, "Mean Q1": 134.7510944519043, "Mean Q2": 134.75108737182617, "critic_loss": 5.985626874923706, "batch_reward": 1.2369140625, "actor_loss": -136.03157086181642, "actor_target_entropy": -2.0, "actor_entropy": 1.1500505609512328, "alpha_loss": 0.0040952546745538715, "alpha_value": 0.07366308513549172, "duration": 70.88655138015747, "step": 17250}
{"episode_reward": 983.0, "episode": 70.0, "Q1 loss": 2.9695121936798095, "Q2 loss": 2.979748644351959, "Mean Target Q": 137.13630932617187, "Mean Q1": 137.13618603515624, "Mean Q2": 137.13562927246093, "critic_loss": 5.949260837554932, "batch_reward": 1.2699609375, "actor_loss": -138.45205676269532, "actor_target_entropy": -2.0, "actor_entropy": 1.1494995069503784, "alpha_loss": 0.006812712329439819, "alpha_value": 0.07324814902021286, "duration": 68.4191963672638, "step": 17500}
{"episode_reward": 971.0, "episode": 71.0, "Q1 loss": 2.9705363368988036, "Q2 loss": 2.9672225914001467, "Mean Target Q": 138.95487176513672, "Mean Q1": 138.95123049926758, "Mean Q2": 138.95043286132812, "critic_loss": 5.93775894165039, "batch_reward": 1.27521875, "actor_loss": -140.17156713867186, "actor_target_entropy": -2.0, "actor_entropy": 1.1269601330757142, "alpha_loss": -0.00033290502382442354, "alpha_value": 0.07298856510779762, "duration": 68.52512335777283, "step": 17750}
{"episode_reward": 43.0, "episode": 72.0, "Q1 loss": 2.9214566078186035, "Q2 loss": 2.8941934661865236, "Mean Target Q": 141.3640502319336, "Mean Q1": 141.36224206542968, "Mean Q2": 141.36220111083983, "critic_loss": 5.815650074005127, "batch_reward": 1.26046875, "actor_loss": -142.43999877929687, "actor_target_entropy": -2.0, "actor_entropy": 1.1127461252212525, "alpha_loss": 0.004223611317574978, "alpha_value": 0.072860266562219, "duration": 68.78246283531189, "step": 18000}
{"episode_reward": 0.0, "episode": 73.0, "Q1 loss": 3.073810764312744, "Q2 loss": 3.0865544328689576, "Mean Target Q": 143.28218701171875, "Mean Q1": 143.28033654785156, "Mean Q2": 143.2798438720703, "critic_loss": 6.160365200996399, "batch_reward": 1.2565, "actor_loss": -144.42716442871094, "actor_target_entropy": -2.0, "actor_entropy": 1.1614643464088439, "alpha_loss": -0.0012735071266070008, "alpha_value": 0.07272359214197763, "duration": 68.7115797996521, "step": 18250}
{"episode_reward": 688.0, "episode": 74.0, "Q1 loss": 3.0547767038345337, "Q2 loss": 3.0736802024841308, "Mean Target Q": 145.471564453125, "Mean Q1": 145.4703748779297, "Mean Q2": 145.46998669433594, "critic_loss": 6.128456893920898, "batch_reward": 1.29496875, "actor_loss": -146.46006774902344, "actor_target_entropy": -2.0, "actor_entropy": 1.1590047369003296, "alpha_loss": -0.005826224778778851, "alpha_value": 0.07301601132038886, "duration": 68.56307935714722, "step": 18500}
{"episode_reward": 904.0, "episode": 75.0, "Q1 loss": 3.0260586318969724, "Q2 loss": 2.9930657300949095, "Mean Target Q": 147.393431640625, "Mean Q1": 147.3917730102539, "Mean Q2": 147.39237225341796, "critic_loss": 6.019124366760254, "batch_reward": 1.3278203125, "actor_loss": -149.04682861328126, "actor_target_entropy": -2.0, "actor_entropy": 1.174552514076233, "alpha_loss": -0.0077304288279265165, "alpha_value": 0.07355139058801041, "duration": 68.69456911087036, "step": 18750}
{"episode_reward": 989.0, "episode": 76.0, "Q1 loss": 2.9311481199264526, "Q2 loss": 2.9403766002655027, "Mean Target Q": 149.96584228515624, "Mean Q1": 149.96345727539062, "Mean Q2": 149.96343646240234, "critic_loss": 5.87152472114563, "batch_reward": 1.3502578125, "actor_loss": -151.0963642578125, "actor_target_entropy": -2.0, "actor_entropy": 1.1477580294609069, "alpha_loss": -0.006221042967401445, "alpha_value": 0.07402785160375186, "duration": 68.62463140487671, "step": 19000}
{"episode_reward": 979.0, "episode": 77.0, "Q1 loss": 2.916943549156189, "Q2 loss": 2.9179692244529725, "Mean Target Q": 152.27905963134765, "Mean Q1": 152.2783681640625, "Mean Q2": 152.27909606933594, "critic_loss": 5.8349127731323245, "batch_reward": 1.3855546875, "actor_loss": -153.53834814453126, "actor_target_entropy": -2.0, "actor_entropy": 1.1337826619148255, "alpha_loss": -0.002420883973594755, "alpha_value": 0.07441058444334975, "duration": 68.65383434295654, "step": 19250}
{"episode_reward": 539.0, "episode": 78.0, "Q1 loss": 3.0546516036987303, "Q2 loss": 3.034993595123291, "Mean Target Q": 154.16635955810546, "Mean Q1": 154.16109790039062, "Mean Q2": 154.16038037109374, "critic_loss": 6.089645191192627, "batch_reward": 1.378125, "actor_loss": -155.4653974609375, "actor_target_entropy": -2.0, "actor_entropy": 1.1112634181976317, "alpha_loss": -0.0020153203518129886, "alpha_value": 0.07445379401286577, "duration": 69.08731389045715, "step": 19500}
{"episode_reward": 0.0, "episode": 79.0, "Q1 loss": 3.407967065811157, "Q2 loss": 3.4095701446533204, "Mean Target Q": 155.8439292602539, "Mean Q1": 155.84148291015626, "Mean Q2": 155.8424036254883, "critic_loss": 6.817537183761597, "batch_reward": 1.3723046875, "actor_loss": -157.5104821777344, "actor_target_entropy": -2.0, "actor_entropy": 1.0853765802383424, "alpha_loss": 0.001091240065637976, "alpha_value": 0.07455117026242092, "duration": 68.52144169807434, "step": 19750}
{"episode_reward": 586.0, "episode": 80.0, "Q1 loss": 3.4261437692642214, "Q2 loss": 3.425648310661316, "Mean Target Q": 157.9110244140625, "Mean Q1": 157.90911651611327, "Mean Q2": 157.9092510986328, "critic_loss": 6.851792053222656, "batch_reward": 1.387078125, "actor_loss": -159.16281774902345, "actor_target_entropy": -2.0, "actor_entropy": 1.0490702772140503, "alpha_loss": -0.0019749573906883595, "alpha_value": 0.07456234417270088, "step": 20000}
{"duration": 77.2716612815857, "step": 20000}
{"episode_reward": 338.0, "episode": 81.0, "Q1 loss": 3.445263165473938, "Q2 loss": 3.4273160085678103, "Mean Target Q": 159.96945471191407, "Mean Q1": 159.96873852539062, "Mean Q2": 159.9680574951172, "critic_loss": 6.872579160690307, "batch_reward": 1.3912890625, "actor_loss": -161.35535192871095, "actor_target_entropy": -2.0, "actor_entropy": 1.0391666207313537, "alpha_loss": -0.0016375141395255923, "alpha_value": 0.07477995926202984, "duration": 68.4149968624115, "step": 20250}
{"episode_reward": 1000.0, "episode": 82.0, "Q1 loss": 3.5103292999267577, "Q2 loss": 3.484873872756958, "Mean Target Q": 162.7899093017578, "Mean Q1": 162.7870764770508, "Mean Q2": 162.78596240234376, "critic_loss": 6.995203187942505, "batch_reward": 1.4314765625, "actor_loss": -164.12485815429687, "actor_target_entropy": -2.0, "actor_entropy": 0.9762074470520019, "alpha_loss": -0.003562491536140442, "alpha_value": 0.0749186418493147, "duration": 68.31384778022766, "step": 20500}
{"episode_reward": 958.0, "episode": 83.0, "Q1 loss": 3.6035368089675903, "Q2 loss": 3.6157167263031007, "Mean Target Q": 164.40909490966797, "Mean Q1": 164.40532037353515, "Mean Q2": 164.40620971679687, "critic_loss": 7.2192535400390625, "batch_reward": 1.4430859375, "actor_loss": -165.95164904785156, "actor_target_entropy": -2.0, "actor_entropy": 0.9710717840194703, "alpha_loss": -0.0052222365476191045, "alpha_value": 0.0753401095239005, "duration": 68.35719442367554, "step": 20750}
{"episode_reward": 380.0, "episode": 84.0, "Q1 loss": 3.5411575555801393, "Q2 loss": 3.5597786359786987, "Mean Target Q": 167.32103063964843, "Mean Q1": 167.3200540161133, "Mean Q2": 167.3186976928711, "critic_loss": 7.100936180114746, "batch_reward": 1.4597109375, "actor_loss": -168.4423106689453, "actor_target_entropy": -2.0, "actor_entropy": 0.979209439754486, "alpha_loss": -0.003306521131657064, "alpha_value": 0.07564599001637441, "duration": 69.04003596305847, "step": 21000}
{"episode_reward": 1000.0, "episode": 85.0, "Q1 loss": 3.462594036102295, "Q2 loss": 3.468884413719177, "Mean Target Q": 168.9316410522461, "Mean Q1": 168.92945080566406, "Mean Q2": 168.9294635620117, "critic_loss": 6.931478450775146, "batch_reward": 1.4780625, "actor_loss": -170.31646594238282, "actor_target_entropy": -2.0, "actor_entropy": 0.9653225445747375, "alpha_loss": -0.0030695835892111064, "alpha_value": 0.07596757417996754, "duration": 68.95915699005127, "step": 21250}
{"episode_reward": 671.0, "episode": 86.0, "Q1 loss": 3.6860114068984986, "Q2 loss": 3.666073595046997, "Mean Target Q": 171.29142419433595, "Mean Q1": 171.28717602539064, "Mean Q2": 171.28763110351562, "critic_loss": 7.3520850028991696, "batch_reward": 1.4998515625, "actor_loss": -172.743908203125, "actor_target_entropy": -2.0, "actor_entropy": 0.9415906229019165, "alpha_loss": -0.004884756451006979, "alpha_value": 0.0762304069396353, "duration": 68.59693765640259, "step": 21500}
{"episode_reward": 941.0, "episode": 87.0, "Q1 loss": 3.8364063062667846, "Q2 loss": 3.8320105628967287, "Mean Target Q": 173.19006433105469, "Mean Q1": 173.19041632080078, "Mean Q2": 173.19045068359375, "critic_loss": 7.66841686630249, "batch_reward": 1.5143125, "actor_loss": -174.832921875, "actor_target_entropy": -2.0, "actor_entropy": 0.8888254585266113, "alpha_loss": -0.00643564735725522, "alpha_value": 0.0767685617672833, "duration": 68.4860405921936, "step": 21750}
{"episode_reward": 803.0, "episode": 88.0, "Q1 loss": 3.6286231956481934, "Q2 loss": 3.6409410371780395, "Mean Target Q": 176.15470782470703, "Mean Q1": 176.1531024169922, "Mean Q2": 176.1526573486328, "critic_loss": 7.2695642261505125, "batch_reward": 1.5506796875, "actor_loss": -177.5243994140625, "actor_target_entropy": -2.0, "actor_entropy": 0.8808498086929322, "alpha_loss": -0.0046665523229166865, "alpha_value": 0.07727815550962583, "duration": 68.33544540405273, "step": 22000}
{"episode_reward": 971.0, "episode": 89.0, "Q1 loss": 3.7297712097167968, "Q2 loss": 3.7274976978302004, "Mean Target Q": 177.3284140625, "Mean Q1": 177.32640826416016, "Mean Q2": 177.32730822753905, "critic_loss": 7.457268932342529, "batch_reward": 1.5520546875, "actor_loss": -178.74406030273437, "actor_target_entropy": -2.0, "actor_entropy": 0.9303500938415528, "alpha_loss": -0.0044101172750815746, "alpha_value": 0.07774018164125365, "duration": 68.47508382797241, "step": 22250}
{"episode_reward": 31.0, "episode": 90.0, "Q1 loss": 3.899374807357788, "Q2 loss": 3.898699849128723, "Mean Target Q": 177.93930841064454, "Mean Q1": 177.93824255371095, "Mean Q2": 177.9377684326172, "critic_loss": 7.798074634552002, "batch_reward": 1.5362421875, "actor_loss": -179.65064782714845, "actor_target_entropy": -2.0, "actor_entropy": 0.9066373047828674, "alpha_loss": 0.0012706453041173517, "alpha_value": 0.07773395064774855, "duration": 68.72041368484497, "step": 22500}
{"episode_reward": 0.0, "episode": 91.0, "Q1 loss": 3.710996587753296, "Q2 loss": 3.687008227348328, "Mean Target Q": 179.48636004638672, "Mean Q1": 179.48293365478514, "Mean Q2": 179.4830078125, "critic_loss": 7.398004825592041, "batch_reward": 1.5378359375, "actor_loss": -180.70952233886717, "actor_target_entropy": -2.0, "actor_entropy": 0.8991076846122742, "alpha_loss": 0.0014892513426020741, "alpha_value": 0.07766829546736763, "duration": 68.70627737045288, "step": 22750}
{"episode_reward": 985.0, "episode": 92.0, "Q1 loss": 3.8366345539093016, "Q2 loss": 3.841866792678833, "Mean Target Q": 180.9949421386719, "Mean Q1": 180.99308514404296, "Mean Q2": 180.993330078125, "critic_loss": 7.678501361846924, "batch_reward": 1.5392578125, "actor_loss": -183.09376721191407, "actor_target_entropy": -2.0, "actor_entropy": 0.8940931057929993, "alpha_loss": -4.55377446487546e-05, "alpha_value": 0.07758550321293668, "duration": 86.28457164764404, "step": 23000}
{"episode_reward": 0.0, "episode": 93.0, "Q1 loss": 3.7606133966445925, "Q2 loss": 3.778137664794922, "Mean Target Q": 183.2756235961914, "Mean Q1": 183.2735883178711, "Mean Q2": 183.2731018676758, "critic_loss": 7.538751058578491, "batch_reward": 1.5485, "actor_loss": -184.8134893798828, "actor_target_entropy": -2.0, "actor_entropy": 0.9602414922714233, "alpha_loss": 0.002398001321591437, "alpha_value": 0.07746499926667044, "duration": 68.75223112106323, "step": 23250}
{"episode_reward": 948.0, "episode": 94.0, "Q1 loss": 3.923301494598389, "Q2 loss": 3.9400350399017334, "Mean Target Q": 185.27724237060548, "Mean Q1": 185.27316943359375, "Mean Q2": 185.273828125, "critic_loss": 7.863336517333984, "batch_reward": 1.5765234375, "actor_loss": -186.73160119628906, "actor_target_entropy": -2.0, "actor_entropy": 0.9563504724502564, "alpha_loss": 0.0003059337204322219, "alpha_value": 0.07732646626143129, "duration": 68.87092876434326, "step": 23500}
{"episode_reward": 968.0, "episode": 95.0, "Q1 loss": 3.9512409477233885, "Q2 loss": 3.9237931880950927, "Mean Target Q": 187.82743371582032, "Mean Q1": 187.8270014038086, "Mean Q2": 187.82775665283202, "critic_loss": 7.875034135818481, "batch_reward": 1.600671875, "actor_loss": -189.56601428222658, "actor_target_entropy": -2.0, "actor_entropy": 0.9769021620750428, "alpha_loss": -0.002119912225753069, "alpha_value": 0.07743532198764394, "duration": 68.91423392295837, "step": 23750}
{"episode_reward": 998.0, "episode": 96.0, "Q1 loss": 4.005916351318359, "Q2 loss": 4.008092599868775, "Mean Target Q": 190.300984375, "Mean Q1": 190.29838537597655, "Mean Q2": 190.29809088134766, "critic_loss": 8.014008945465088, "batch_reward": 1.621609375, "actor_loss": -191.38399230957032, "actor_target_entropy": -2.0, "actor_entropy": 0.9843865151405334, "alpha_loss": -0.003396731016226113, "alpha_value": 0.07775811269765069, "duration": 68.59349727630615, "step": 24000}
{"episode_reward": 939.0, "episode": 97.0, "Q1 loss": 3.9200809097290037, "Q2 loss": 3.9232082929611205, "Mean Target Q": 192.3900048828125, "Mean Q1": 192.38867517089844, "Mean Q2": 192.38755700683595, "critic_loss": 7.843289197921753, "batch_reward": 1.638609375, "actor_loss": -193.75741638183592, "actor_target_entropy": -2.0, "actor_entropy": 0.9898707752227783, "alpha_loss": -0.001627504694275558, "alpha_value": 0.07797051233998413, "duration": 68.61701393127441, "step": 24250}
{"episode_reward": 954.0, "episode": 98.0, "Q1 loss": 3.9990320234298706, "Q2 loss": 3.968759943008423, "Mean Target Q": 195.54188159179688, "Mean Q1": 195.5400224609375, "Mean Q2": 195.5404981689453, "critic_loss": 7.967791988372802, "batch_reward": 1.6726484375, "actor_loss": -197.29920617675782, "actor_target_entropy": -2.0, "actor_entropy": 0.9819762926101685, "alpha_loss": -0.0019094797801226377, "alpha_value": 0.07811209196017604, "duration": 68.74597501754761, "step": 24500}
{"episode_reward": 951.0, "episode": 99.0, "Q1 loss": 4.080447263717652, "Q2 loss": 4.035649107933044, "Mean Target Q": 197.1660892944336, "Mean Q1": 197.16231707763671, "Mean Q2": 197.16233654785157, "critic_loss": 8.116096347808838, "batch_reward": 1.6765234375, "actor_loss": -198.7948575439453, "actor_target_entropy": -2.0, "actor_entropy": 0.9810179514884949, "alpha_loss": -0.004969996077008545, "alpha_value": 0.0785429016916899, "duration": 68.65583944320679, "step": 24750}
{"episode_reward": 254.0, "episode": 100.0, "Q1 loss": 4.03491530418396, "Q2 loss": 4.017670597076416, "Mean Target Q": 198.72342932128907, "Mean Q1": 198.72334936523438, "Mean Q2": 198.72312579345703, "critic_loss": 8.052585893630981, "batch_reward": 1.686578125, "actor_loss": -200.07580126953124, "actor_target_entropy": -2.0, "actor_entropy": 1.002312749862671, "alpha_loss": -0.005208992118947208, "alpha_value": 0.07901783605300813, "step": 25000}
{"duration": 77.51088261604309, "step": 25000}
{"episode_reward": 971.0, "episode": 101.0, "Q1 loss": 3.9800143938064574, "Q2 loss": 3.980661561012268, "Mean Target Q": 200.367568359375, "Mean Q1": 200.36582592773436, "Mean Q2": 200.36671105957032, "critic_loss": 7.960675952911377, "batch_reward": 1.69503125, "actor_loss": -201.74920935058594, "actor_target_entropy": -2.0, "actor_entropy": 1.0036913676261903, "alpha_loss": 0.0017829125025309622, "alpha_value": 0.07915080592944655, "duration": 68.57715916633606, "step": 25250}
{"episode_reward": 359.0, "episode": 102.0, "Q1 loss": 3.8527280168533324, "Q2 loss": 3.8486908750534057, "Mean Target Q": 200.25915203857423, "Mean Q1": 200.25391290283204, "Mean Q2": 200.25462438964843, "critic_loss": 7.701418897628784, "batch_reward": 1.6928828125, "actor_loss": -201.68366845703125, "actor_target_entropy": -2.0, "actor_entropy": 0.9803533067703247, "alpha_loss": 0.005097584538161754, "alpha_value": 0.07884755042837435, "duration": 68.31735897064209, "step": 25500}
{"episode_reward": 290.0, "episode": 103.0, "Q1 loss": 4.054015295982361, "Q2 loss": 4.064465754508972, "Mean Target Q": 201.9537188720703, "Mean Q1": 201.9561516723633, "Mean Q2": 201.95495190429688, "critic_loss": 8.118481061935425, "batch_reward": 1.69353125, "actor_loss": -203.73552172851564, "actor_target_entropy": -2.0, "actor_entropy": 0.9326148343086242, "alpha_loss": 0.003799635854549706, "alpha_value": 0.07843290288156254, "duration": 68.51409602165222, "step": 25750}
{"episode_reward": 876.0, "episode": 104.0, "Q1 loss": 4.184882643699646, "Q2 loss": 4.134493155479431, "Mean Target Q": 202.68914801025392, "Mean Q1": 202.68494036865235, "Mean Q2": 202.68633868408205, "critic_loss": 8.319375816345215, "batch_reward": 1.694203125, "actor_loss": -203.74810180664062, "actor_target_entropy": -2.0, "actor_entropy": 0.9147956109046936, "alpha_loss": 0.0036964902468025686, "alpha_value": 0.07803851489786284, "duration": 68.57897639274597, "step": 26000}
{"episode_reward": 61.0, "episode": 105.0, "Q1 loss": 4.272986570358277, "Q2 loss": 4.275334080696106, "Mean Target Q": 203.78445306396483, "Mean Q1": 203.7834566040039, "Mean Q2": 203.78331005859374, "critic_loss": 8.548320669174194, "batch_reward": 1.6973046875, "actor_loss": -204.57219165039064, "actor_target_entropy": -2.0, "actor_entropy": 0.878502022266388, "alpha_loss": 0.00438022494642064, "alpha_value": 0.07758146251510309, "duration": 68.71391582489014, "step": 26250}
{"episode_reward": 961.0, "episode": 106.0, "Q1 loss": 4.200339440345764, "Q2 loss": 4.2143939609527585, "Mean Target Q": 206.00431799316405, "Mean Q1": 206.00301214599608, "Mean Q2": 206.00282147216797, "critic_loss": 8.414733400344849, "batch_reward": 1.722171875, "actor_loss": -207.40306469726562, "actor_target_entropy": -2.0, "actor_entropy": 0.8867432951927186, "alpha_loss": 0.00779736823309213, "alpha_value": 0.0770455543976312, "duration": 68.7646541595459, "step": 26500}
{"episode_reward": 978.0, "episode": 107.0, "Q1 loss": 4.146142992019653, "Q2 loss": 4.1679449558258055, "Mean Target Q": 208.17951953125, "Mean Q1": 208.17697955322265, "Mean Q2": 208.17787420654298, "critic_loss": 8.314087963104248, "batch_reward": 1.72971875, "actor_loss": -209.59215002441405, "actor_target_entropy": -2.0, "actor_entropy": 0.8836247782707214, "alpha_loss": 0.00384984315931797, "alpha_value": 0.07648739046636974, "duration": 69.10608077049255, "step": 26750}
{"episode_reward": 0.0, "episode": 108.0, "Q1 loss": 4.334245186805725, "Q2 loss": 4.315529536247253, "Mean Target Q": 209.18440283203125, "Mean Q1": 209.18335711669923, "Mean Q2": 209.1830717163086, "critic_loss": 8.64977473449707, "batch_reward": 1.7280234375, "actor_loss": -210.0112145996094, "actor_target_entropy": -2.0, "actor_entropy": 0.8815489101409912, "alpha_loss": 0.004829011922236532, "alpha_value": 0.07611937232218818, "duration": 68.74926614761353, "step": 27000}
{"episode_reward": 905.0, "episode": 109.0, "Q1 loss": 4.2497772769927975, "Q2 loss": 4.262974837303162, "Mean Target Q": 209.76788696289063, "Mean Q1": 209.76709063720702, "Mean Q2": 209.76662384033204, "critic_loss": 8.512752140045166, "batch_reward": 1.7280390625, "actor_loss": -211.47955163574218, "actor_target_entropy": -2.0, "actor_entropy": 0.900768105506897, "alpha_loss": 0.005777226069942117, "alpha_value": 0.07552613791968857, "duration": 68.64172792434692, "step": 27250}
{"episode_reward": 8.0, "episode": 110.0, "Q1 loss": 4.255460936546326, "Q2 loss": 4.247193119049072, "Mean Target Q": 210.8007074584961, "Mean Q1": 210.7962325439453, "Mean Q2": 210.79693731689454, "critic_loss": 8.502654048919677, "batch_reward": 1.7175078125, "actor_loss": -212.30524047851563, "actor_target_entropy": -2.0, "actor_entropy": 0.9305299344062805, "alpha_loss": 0.007588796900585294, "alpha_value": 0.07490620076127061, "duration": 75.74627590179443, "step": 27500}
{"episode_reward": 121.0, "episode": 111.0, "Q1 loss": 4.4516644859313965, "Q2 loss": 4.459401174545288, "Mean Target Q": 210.56578704833984, "Mean Q1": 210.56530230712892, "Mean Q2": 210.56448095703124, "critic_loss": 8.911065670013429, "batch_reward": 1.6913671875, "actor_loss": -211.8020389404297, "actor_target_entropy": -2.0, "actor_entropy": 0.9420883960723877, "alpha_loss": 0.0033135113222524525, "alpha_value": 0.07440261806580659, "duration": 68.75195026397705, "step": 27750}
{"episode_reward": 0.0, "episode": 112.0, "Q1 loss": 4.310334244728089, "Q2 loss": 4.3145296049118045, "Mean Target Q": 210.66435980224608, "Mean Q1": 210.6601499633789, "Mean Q2": 210.66121270751952, "critic_loss": 8.624863859176635, "batch_reward": 1.6776015625, "actor_loss": -211.61272253417968, "actor_target_entropy": -2.0, "actor_entropy": 0.9490927991867065, "alpha_loss": 0.0014648009845986962, "alpha_value": 0.0741588179582597, "duration": 68.91600608825684, "step": 28000}
{"episode_reward": 0.0, "episode": 113.0, "Q1 loss": 4.164561092376709, "Q2 loss": 4.158946806907654, "Mean Target Q": 212.83659100341796, "Mean Q1": 212.83653100585937, "Mean Q2": 212.83574493408204, "critic_loss": 8.323507921218873, "batch_reward": 1.68878125, "actor_loss": -214.64769250488283, "actor_target_entropy": -2.0, "actor_entropy": 0.9043421974182129, "alpha_loss": -0.0001984335659071803, "alpha_value": 0.0740700502031387, "duration": 68.95452928543091, "step": 28250}
{"episode_reward": 984.0, "episode": 114.0, "Q1 loss": 4.174792797088623, "Q2 loss": 4.183992732048035, "Mean Target Q": 214.67536572265624, "Mean Q1": 214.67248767089845, "Mean Q2": 214.6737604370117, "critic_loss": 8.358785505294799, "batch_reward": 1.7005, "actor_loss": -216.048076171875, "actor_target_entropy": -2.0, "actor_entropy": 0.9495580382347107, "alpha_loss": -0.0015337877557612956, "alpha_value": 0.07424105361223997, "duration": 68.90860843658447, "step": 28500}
{"episode_reward": 972.0, "episode": 115.0, "Q1 loss": 4.208327154159546, "Q2 loss": 4.195784927368164, "Mean Target Q": 217.08384124755858, "Mean Q1": 217.08330432128906, "Mean Q2": 217.0820464477539, "critic_loss": 8.40411206817627, "batch_reward": 1.724703125, "actor_loss": -217.90179174804686, "actor_target_entropy": -2.0, "actor_entropy": 0.9737715067863464, "alpha_loss": -0.0009694099659100175, "alpha_value": 0.07428819888250544, "duration": 68.85683155059814, "step": 28750}
{"episode_reward": 980.0, "episode": 116.0, "Q1 loss": 4.181244809150696, "Q2 loss": 4.182453969955445, "Mean Target Q": 219.23668798828126, "Mean Q1": 219.23444287109376, "Mean Q2": 219.2361439819336, "critic_loss": 8.363698780059815, "batch_reward": 1.7420625, "actor_loss": -220.48122497558595, "actor_target_entropy": -2.0, "actor_entropy": 0.9738814964294433, "alpha_loss": -0.001909311801660806, "alpha_value": 0.07449093139619624, "duration": 69.16689538955688, "step": 29000}
{"episode_reward": 966.0, "episode": 117.0, "Q1 loss": 4.192419819831848, "Q2 loss": 4.185226643562317, "Mean Target Q": 220.8074575805664, "Mean Q1": 220.80616888427735, "Mean Q2": 220.80575164794922, "critic_loss": 8.377646434783935, "batch_reward": 1.7586875, "actor_loss": -222.29810192871093, "actor_target_entropy": -2.0, "actor_entropy": 0.9525549678802491, "alpha_loss": 0.0008873574789613485, "alpha_value": 0.074484080231979, "duration": 69.46571588516235, "step": 29250}
{"episode_reward": 1000.0, "episode": 118.0, "Q1 loss": 4.342056736946106, "Q2 loss": 4.341268392562866, "Mean Target Q": 222.67337475585938, "Mean Q1": 222.6710121459961, "Mean Q2": 222.6710546875, "critic_loss": 8.6833251247406, "batch_reward": 1.7731953125, "actor_loss": -224.02921923828126, "actor_target_entropy": -2.0, "actor_entropy": 0.9191789627075195, "alpha_loss": -0.0010180200119502842, "alpha_value": 0.07452051711958281, "duration": 69.11200785636902, "step": 29500}
{"episode_reward": 991.0, "episode": 119.0, "Q1 loss": 4.208316964149475, "Q2 loss": 4.238347274780273, "Mean Target Q": 223.7808209838867, "Mean Q1": 223.7783776245117, "Mean Q2": 223.77837780761718, "critic_loss": 8.446664226531983, "batch_reward": 1.7807890625, "actor_loss": -225.50443420410156, "actor_target_entropy": -2.0, "actor_entropy": 0.9575585370063782, "alpha_loss": -5.3170036990195514e-05, "alpha_value": 0.07457845291668165, "duration": 68.90453433990479, "step": 29750}
{"episode_reward": 60.0, "episode": 120.0, "Q1 loss": 4.393453530311584, "Q2 loss": 4.418804458618164, "Mean Target Q": 224.5605418701172, "Mean Q1": 224.55901641845702, "Mean Q2": 224.5596069946289, "critic_loss": 8.81225797843933, "batch_reward": 1.77484375, "actor_loss": -225.99634912109374, "actor_target_entropy": -2.0, "actor_entropy": 0.9350171365737915, "alpha_loss": -0.003359172790311277, "alpha_value": 0.0747290075828513, "step": 30000}
{"duration": 77.19595980644226, "step": 30000}
{"episode_reward": 947.0, "episode": 121.0, "Q1 loss": 4.39566951084137, "Q2 loss": 4.367025027275085, "Mean Target Q": 226.63336608886718, "Mean Q1": 226.62935455322267, "Mean Q2": 226.63002606201172, "critic_loss": 8.762694522857666, "batch_reward": 1.7846171875, "actor_loss": -227.96849865722658, "actor_target_entropy": -2.0, "actor_entropy": 0.9210993366241456, "alpha_loss": -0.0027950996244326233, "alpha_value": 0.07506358572667726, "duration": 69.64003276824951, "step": 30250}
{"episode_reward": 111.0, "episode": 122.0, "Q1 loss": 4.545777058601379, "Q2 loss": 4.578398394584656, "Mean Target Q": 227.66476580810547, "Mean Q1": 227.66462872314452, "Mean Q2": 227.66420654296874, "critic_loss": 9.124175464630127, "batch_reward": 1.7821875, "actor_loss": -228.91797924804686, "actor_target_entropy": -2.0, "actor_entropy": 0.9546837306022644, "alpha_loss": -0.003231642208993435, "alpha_value": 0.07530006303814492, "duration": 68.6507260799408, "step": 30500}
{"episode_reward": 72.0, "episode": 123.0, "Q1 loss": 4.556178412437439, "Q2 loss": 4.557953058242798, "Mean Target Q": 228.45678759765624, "Mean Q1": 228.45391302490233, "Mean Q2": 228.45396655273439, "critic_loss": 9.114131490707397, "batch_reward": 1.7761796875, "actor_loss": -229.60488012695313, "actor_target_entropy": -2.0, "actor_entropy": 0.9609544472694397, "alpha_loss": -0.0032739432165399193, "alpha_value": 0.07567037032952655, "duration": 68.76441502571106, "step": 30750}
{"episode_reward": 962.0, "episode": 124.0, "Q1 loss": 4.640126691818237, "Q2 loss": 4.622565176010132, "Mean Target Q": 230.1077311401367, "Mean Q1": 230.1056229248047, "Mean Q2": 230.10485528564453, "critic_loss": 9.262691862106323, "batch_reward": 1.79090625, "actor_loss": -231.504388671875, "actor_target_entropy": -2.0, "actor_entropy": 0.9186269655227661, "alpha_loss": -0.0007027858141809702, "alpha_value": 0.07586017328652306, "duration": 68.86168670654297, "step": 31000}
{"episode_reward": 948.0, "episode": 125.0, "Q1 loss": 4.628653649330139, "Q2 loss": 4.692649802207947, "Mean Target Q": 230.6975919189453, "Mean Q1": 230.6949052734375, "Mean Q2": 230.69536297607422, "critic_loss": 9.32130341720581, "batch_reward": 1.799109375, "actor_loss": -231.94356201171874, "actor_target_entropy": -2.0, "actor_entropy": 0.9577100229263306, "alpha_loss": -0.0032860497515648603, "alpha_value": 0.07605759299965895, "duration": 68.90769290924072, "step": 31250}
{"episode_reward": 325.0, "episode": 126.0, "Q1 loss": 4.425949330329895, "Q2 loss": 4.404756670951843, "Mean Target Q": 232.11735284423827, "Mean Q1": 232.11586291503906, "Mean Q2": 232.11579803466796, "critic_loss": 8.830706001281738, "batch_reward": 1.8086953125, "actor_loss": -233.41333227539062, "actor_target_entropy": -2.0, "actor_entropy": 0.9462040205001832, "alpha_loss": 0.0010667113782837987, "alpha_value": 0.0762554853674527, "duration": 68.98977088928223, "step": 31500}
{"episode_reward": 967.0, "episode": 127.0, "Q1 loss": 4.547016947746277, "Q2 loss": 4.579367527961731, "Mean Target Q": 233.96133477783204, "Mean Q1": 233.9588372192383, "Mean Q2": 233.95935235595704, "critic_loss": 9.126384466171265, "batch_reward": 1.832359375, "actor_loss": -235.42228259277343, "actor_target_entropy": -2.0, "actor_entropy": 0.9308964195251465, "alpha_loss": -0.004609375094994902, "alpha_value": 0.07639238842649457, "duration": 68.89753150939941, "step": 31750}
{"episode_reward": 960.0, "episode": 128.0, "Q1 loss": 4.503240683555603, "Q2 loss": 4.494736576080323, "Mean Target Q": 235.15069982910157, "Mean Q1": 235.14653326416015, "Mean Q2": 235.14721008300782, "critic_loss": 8.99797723007202, "batch_reward": 1.842421875, "actor_loss": -236.3589365234375, "actor_target_entropy": -2.0, "actor_entropy": 0.9174957747459411, "alpha_loss": 0.001340019035153091, "alpha_value": 0.0764841495337077, "duration": 68.91267681121826, "step": 32000}
{"episode_reward": 963.0, "episode": 129.0, "Q1 loss": 4.847450936317444, "Q2 loss": 4.8164298038482665, "Mean Target Q": 236.8141381225586, "Mean Q1": 236.8131146850586, "Mean Q2": 236.81217681884766, "critic_loss": 9.663880769729614, "batch_reward": 1.8465625, "actor_loss": -237.8346417236328, "actor_target_entropy": -2.0, "actor_entropy": 0.9505187802314758, "alpha_loss": -0.004533358580432832, "alpha_value": 0.07669436091492382, "duration": 68.92397475242615, "step": 32250}
{"episode_reward": 766.0, "episode": 130.0, "Q1 loss": 4.799544890403747, "Q2 loss": 4.836486082077027, "Mean Target Q": 238.18988165283204, "Mean Q1": 238.18813287353515, "Mean Q2": 238.18854296875, "critic_loss": 9.636031003952027, "batch_reward": 1.8555625, "actor_loss": -239.22285229492186, "actor_target_entropy": -2.0, "actor_entropy": 0.92709011220932, "alpha_loss": -8.239802718162537e-05, "alpha_value": 0.07691790622344447, "duration": 68.97797870635986, "step": 32500}
{"episode_reward": 221.0, "episode": 131.0, "Q1 loss": 4.7103678388595585, "Q2 loss": 4.704262261390686, "Mean Target Q": 239.32207678222656, "Mean Q1": 239.319134765625, "Mean Q2": 239.31968377685547, "critic_loss": 9.414630107879638, "batch_reward": 1.858015625, "actor_loss": -240.60414343261718, "actor_target_entropy": -2.0, "actor_entropy": 0.925771146774292, "alpha_loss": -0.0009816318089142441, "alpha_value": 0.07698215175593967, "duration": 69.04425048828125, "step": 32750}
{"episode_reward": 946.0, "episode": 132.0, "Q1 loss": 4.482005077362061, "Q2 loss": 4.49922435760498, "Mean Target Q": 241.0438983154297, "Mean Q1": 241.04086462402344, "Mean Q2": 241.04085412597655, "critic_loss": 8.981229442596435, "batch_reward": 1.8727421875, "actor_loss": -242.61354040527343, "actor_target_entropy": -2.0, "actor_entropy": 0.9232282419204711, "alpha_loss": -0.0018301710151135922, "alpha_value": 0.07703855749207209, "duration": 68.98153614997864, "step": 33000}
{"episode_reward": 981.0, "episode": 133.0, "Q1 loss": 4.74264603805542, "Q2 loss": 4.697250290870667, "Mean Target Q": 242.50930877685548, "Mean Q1": 242.50813458251952, "Mean Q2": 242.50940521240236, "critic_loss": 9.439896339416505, "batch_reward": 1.884390625, "actor_loss": -243.55612646484374, "actor_target_entropy": -2.0, "actor_entropy": 0.9545748839378357, "alpha_loss": 0.0015139766903594137, "alpha_value": 0.07712208076370546, "duration": 69.62768530845642, "step": 33250}
{"episode_reward": 994.0, "episode": 134.0, "Q1 loss": 4.632238353729248, "Q2 loss": 4.625269043922424, "Mean Target Q": 244.52976336669923, "Mean Q1": 244.52661328125, "Mean Q2": 244.52652307128906, "critic_loss": 9.257507402420044, "batch_reward": 1.8985078125, "actor_loss": -245.498259765625, "actor_target_entropy": -2.0, "actor_entropy": 0.9417369728088378, "alpha_loss": 0.0008852779380977154, "alpha_value": 0.07710339093129873, "duration": 69.13194966316223, "step": 33500}
{"episode_reward": 984.0, "episode": 135.0, "Q1 loss": 4.73408388710022, "Q2 loss": 4.704403534889221, "Mean Target Q": 245.8111651611328, "Mean Q1": 245.81084991455077, "Mean Q2": 245.81103137207032, "critic_loss": 9.438487382888795, "batch_reward": 1.9110390625, "actor_loss": -247.17309252929687, "actor_target_entropy": -2.0, "actor_entropy": 0.9290296978950501, "alpha_loss": 0.0010030262228101492, "alpha_value": 0.07691284400785738, "duration": 68.98020076751709, "step": 33750}
{"episode_reward": 448.0, "episode": 136.0, "Q1 loss": 4.859463968276978, "Q2 loss": 4.888133842468262, "Mean Target Q": 247.017796875, "Mean Q1": 247.01372735595703, "Mean Q2": 247.01438708496093, "critic_loss": 9.747597810745239, "batch_reward": 1.9241328125, "actor_loss": -248.37456335449218, "actor_target_entropy": -2.0, "actor_entropy": 0.8955817284584046, "alpha_loss": 0.0031646439274773002, "alpha_value": 0.07670559113438975, "duration": 69.85288691520691, "step": 34000}
{"episode_reward": 969.0, "episode": 137.0, "Q1 loss": 4.806638802528381, "Q2 loss": 4.7623857450485225, "Mean Target Q": 248.14542535400392, "Mean Q1": 248.1427359008789, "Mean Q2": 248.143078125, "critic_loss": 9.569024534225465, "batch_reward": 1.925546875, "actor_loss": -249.1437646484375, "actor_target_entropy": -2.0, "actor_entropy": 0.9212358441352845, "alpha_loss": 0.0009437224743887782, "alpha_value": 0.07648837606702827, "duration": 69.07096886634827, "step": 34250}
{"episode_reward": 87.0, "episode": 138.0, "Q1 loss": 4.750272758483887, "Q2 loss": 4.755668502807617, "Mean Target Q": 248.13141961669922, "Mean Q1": 248.13053515625, "Mean Q2": 248.1305880126953, "critic_loss": 9.505941267013549, "batch_reward": 1.91996875, "actor_loss": -249.6280411376953, "actor_target_entropy": -2.0, "actor_entropy": 0.905222192287445, "alpha_loss": -0.0035445407507941127, "alpha_value": 0.07663431482303673, "duration": 69.03565573692322, "step": 34500}
{"episode_reward": 969.0, "episode": 139.0, "Q1 loss": 4.665783976554871, "Q2 loss": 4.731242658615113, "Mean Target Q": 249.3196973876953, "Mean Q1": 249.31693060302734, "Mean Q2": 249.3168853149414, "critic_loss": 9.397026634216308, "batch_reward": 1.9178828125, "actor_loss": -250.75208422851563, "actor_target_entropy": -2.0, "actor_entropy": 0.907330228805542, "alpha_loss": -0.003240040857810527, "alpha_value": 0.07701433743734624, "duration": 69.22557258605957, "step": 34750}
{"episode_reward": 258.0, "episode": 140.0, "Q1 loss": 4.875313499450684, "Q2 loss": 4.869125464439392, "Mean Target Q": 250.00045043945312, "Mean Q1": 249.99920544433593, "Mean Q2": 249.99916790771485, "critic_loss": 9.744438932418824, "batch_reward": 1.92365625, "actor_loss": -251.13427673339845, "actor_target_entropy": -2.0, "actor_entropy": 0.8971485733985901, "alpha_loss": -0.0008100276263430714, "alpha_value": 0.07718570009171519, "step": 35000}
{"duration": 77.40223717689514, "step": 35000}
{"episode_reward": 0.0, "episode": 141.0, "Q1 loss": 4.7424873046875, "Q2 loss": 4.7094466619491575, "Mean Target Q": 250.5503814086914, "Mean Q1": 250.54909381103516, "Mean Q2": 250.54909631347655, "critic_loss": 9.451933992385865, "batch_reward": 1.9095, "actor_loss": -251.85325146484374, "actor_target_entropy": -2.0, "actor_entropy": 0.9012528223991394, "alpha_loss": -0.0019734126040712, "alpha_value": 0.07733026845326241, "duration": 68.99565720558167, "step": 35250}
{"episode_reward": 820.0, "episode": 142.0, "Q1 loss": 4.89819456577301, "Q2 loss": 4.8726938467025755, "Mean Target Q": 251.96236309814452, "Mean Q1": 251.96100286865234, "Mean Q2": 251.9600650024414, "critic_loss": 9.770888429641724, "batch_reward": 1.9104140625, "actor_loss": -253.48473767089843, "actor_target_entropy": -2.0, "actor_entropy": 0.9615504741668701, "alpha_loss": -0.003070934010669589, "alpha_value": 0.07757643087764204, "duration": 68.82223296165466, "step": 35500}
{"episode_reward": 976.0, "episode": 143.0, "Q1 loss": 4.89772465133667, "Q2 loss": 4.887812244415283, "Mean Target Q": 253.7826649169922, "Mean Q1": 253.77851110839845, "Mean Q2": 253.77926379394532, "critic_loss": 9.785536922454835, "batch_reward": 1.938140625, "actor_loss": -254.77400756835937, "actor_target_entropy": -2.0, "actor_entropy": 0.9453805532455445, "alpha_loss": 0.004319168172311038, "alpha_value": 0.07761696568009889, "duration": 68.91537642478943, "step": 35750}
{"episode_reward": 979.0, "episode": 144.0, "Q1 loss": 4.905896162033081, "Q2 loss": 4.896040769577026, "Mean Target Q": 255.55605670166017, "Mean Q1": 255.55422186279296, "Mean Q2": 255.55437731933594, "critic_loss": 9.801936939239502, "batch_reward": 1.947609375, "actor_loss": -256.80717407226564, "actor_target_entropy": -2.0, "actor_entropy": 0.9175835766792297, "alpha_loss": -0.0011008204389363527, "alpha_value": 0.07736741382291498, "duration": 69.08132290840149, "step": 36000}
{"episode_reward": 954.0, "episode": 145.0, "Q1 loss": 4.9937220392227175, "Q2 loss": 5.006249320983887, "Mean Target Q": 257.3767340087891, "Mean Q1": 257.3758384399414, "Mean Q2": 257.3756159667969, "critic_loss": 9.999971349716187, "batch_reward": 1.9583671875, "actor_loss": -258.97214331054687, "actor_target_entropy": -2.0, "actor_entropy": 0.9170232758522033, "alpha_loss": 0.000812968117184937, "alpha_value": 0.07741597550183109, "duration": 69.23589420318604, "step": 36250}
{"episode_reward": 5.0, "episode": 146.0, "Q1 loss": 4.846171039581299, "Q2 loss": 4.8656746120452885, "Mean Target Q": 257.87912878417967, "Mean Q1": 257.87732110595704, "Mean Q2": 257.87803314208986, "critic_loss": 9.711845630645753, "batch_reward": 1.9514921875, "actor_loss": -259.57087536621094, "actor_target_entropy": -2.0, "actor_entropy": 0.9041075186729431, "alpha_loss": 0.0020458830194547773, "alpha_value": 0.0772414397526853, "duration": 69.1940770149231, "step": 36500}
{"episode_reward": 0.0, "episode": 147.0, "Q1 loss": 4.859288777351379, "Q2 loss": 4.893755930900574, "Mean Target Q": 258.3728129882812, "Mean Q1": 258.3709532470703, "Mean Q2": 258.37045129394534, "critic_loss": 9.753044706344605, "batch_reward": 1.939390625, "actor_loss": -259.87979052734374, "actor_target_entropy": -2.0, "actor_entropy": 0.870705994606018, "alpha_loss": 0.003611483095213771, "alpha_value": 0.07695479703855503, "duration": 69.06852912902832, "step": 36750}
{"episode_reward": 993.0, "episode": 148.0, "Q1 loss": 4.813574848175048, "Q2 loss": 4.811945211410523, "Mean Target Q": 259.1972376098633, "Mean Q1": 259.19583081054685, "Mean Q2": 259.1957774658203, "critic_loss": 9.625520050048829, "batch_reward": 1.939234375, "actor_loss": -260.4407032470703, "actor_target_entropy": -2.0, "actor_entropy": 0.9048675208091735, "alpha_loss": 0.00189904704131186, "alpha_value": 0.0766109778773644, "duration": 69.8071858882904, "step": 37000}
{"episode_reward": 9.0, "episode": 149.0, "Q1 loss": 4.934054389953613, "Q2 loss": 4.959914705276489, "Mean Target Q": 259.4033784790039, "Mean Q1": 259.40138903808594, "Mean Q2": 259.4019490356445, "critic_loss": 9.893969110488891, "batch_reward": 1.9231953125, "actor_loss": -261.1350865478516, "actor_target_entropy": -2.0, "actor_entropy": 0.8958373379707336, "alpha_loss": 0.00030064287688583136, "alpha_value": 0.07655507899027236, "duration": 69.21533417701721, "step": 37250}
{"episode_reward": 74.0, "episode": 150.0, "Q1 loss": 5.0039819164276125, "Q2 loss": 4.973277136802674, "Mean Target Q": 260.52080834960935, "Mean Q1": 260.51842578125, "Mean Q2": 260.5172712402344, "critic_loss": 9.97725908279419, "batch_reward": 1.9292890625, "actor_loss": -261.67921801757814, "actor_target_entropy": -2.0, "actor_entropy": 0.922677152633667, "alpha_loss": 0.002906876384746283, "alpha_value": 0.07640095362186246, "duration": 69.57276654243469, "step": 37500}
{"episode_reward": 827.0, "episode": 151.0, "Q1 loss": 5.050802205085755, "Q2 loss": 5.0621746654510495, "Mean Target Q": 260.66049768066404, "Mean Q1": 260.65709362792967, "Mean Q2": 260.6583121337891, "critic_loss": 10.112976871490478, "batch_reward": 1.9230234375, "actor_loss": -262.2498405761719, "actor_target_entropy": -2.0, "actor_entropy": 0.9470292973518372, "alpha_loss": 0.004640073019079864, "alpha_value": 0.07600964784002376, "duration": 69.29478597640991, "step": 37750}
{"episode_reward": 288.0, "episode": 152.0, "Q1 loss": 5.012037558555603, "Q2 loss": 5.014280116081237, "Mean Target Q": 260.1657245483398, "Mean Q1": 260.1649302368164, "Mean Q2": 260.1652645263672, "critic_loss": 10.026317686080933, "batch_reward": 1.9224140625, "actor_loss": -261.06411108398436, "actor_target_entropy": -2.0, "actor_entropy": 0.9113782668113708, "alpha_loss": 0.002172348130028695, "alpha_value": 0.07566229944959849, "duration": 69.34097337722778, "step": 38000}
{"episode_reward": 0.0, "episode": 153.0, "Q1 loss": 4.921102297782898, "Q2 loss": 4.90649754524231, "Mean Target Q": 260.4226524658203, "Mean Q1": 260.4234423217773, "Mean Q2": 260.4212463989258, "critic_loss": 9.82759983253479, "batch_reward": 1.9099921875, "actor_loss": -261.34229650878905, "actor_target_entropy": -2.0, "actor_entropy": 0.8764058027267456, "alpha_loss": 0.004080534082837403, "alpha_value": 0.07541331421881452, "duration": 69.42422103881836, "step": 38250}
{"episode_reward": 975.0, "episode": 154.0, "Q1 loss": 5.353287034988403, "Q2 loss": 5.363472631454468, "Mean Target Q": 262.58341497802735, "Mean Q1": 262.57668286132815, "Mean Q2": 262.5785486450195, "critic_loss": 10.716759630203248, "batch_reward": 1.9366171875, "actor_loss": -263.58672399902343, "actor_target_entropy": -2.0, "actor_entropy": 0.8631339845657349, "alpha_loss": 0.004625288657378405, "alpha_value": 0.07493977216546237, "duration": 69.3948163986206, "step": 38500}
{"episode_reward": 982.0, "episode": 155.0, "Q1 loss": 4.852294405937195, "Q2 loss": 4.883294676780701, "Mean Target Q": 263.2977306518555, "Mean Q1": 263.298609375, "Mean Q2": 263.2970260009766, "critic_loss": 9.735589065551757, "batch_reward": 1.9429921875, "actor_loss": -264.5106484375, "actor_target_entropy": -2.0, "actor_entropy": 0.85972562789917, "alpha_loss": 0.004908549866639077, "alpha_value": 0.074454045111232, "duration": 69.40913128852844, "step": 38750}
{"episode_reward": 860.0, "episode": 156.0, "Q1 loss": 4.9757029209136965, "Q2 loss": 4.953813070297241, "Mean Target Q": 264.3183381958008, "Mean Q1": 264.31517169189453, "Mean Q2": 264.3156612548828, "critic_loss": 9.92951598548889, "batch_reward": 1.9506875, "actor_loss": -266.231751953125, "actor_target_entropy": -2.0, "actor_entropy": 0.8411910905838013, "alpha_loss": 0.001264352486934513, "alpha_value": 0.07418201348150896, "duration": 69.51986622810364, "step": 39000}
{"episode_reward": 800.0, "episode": 157.0, "Q1 loss": 5.241319794654846, "Q2 loss": 5.210338380813599, "Mean Target Q": 265.2775989379883, "Mean Q1": 265.27670947265625, "Mean Q2": 265.27631359863284, "critic_loss": 10.45165817642212, "batch_reward": 1.96396875, "actor_loss": -267.0424599609375, "actor_target_entropy": -2.0, "actor_entropy": 0.8904382834434509, "alpha_loss": 0.0005567160155624151, "alpha_value": 0.07410076716502466, "duration": 69.36156463623047, "step": 39250}
{"episode_reward": 964.0, "episode": 158.0, "Q1 loss": 4.961696907043457, "Q2 loss": 4.948528293609619, "Mean Target Q": 266.5340800170898, "Mean Q1": 266.531084777832, "Mean Q2": 266.53221295166014, "critic_loss": 9.910225179672242, "batch_reward": 1.9799765625, "actor_loss": -267.90294189453124, "actor_target_entropy": -2.0, "actor_entropy": 0.8168489513397217, "alpha_loss": 0.0011664108149707316, "alpha_value": 0.07401738312257279, "duration": 69.49085545539856, "step": 39500}
{"episode_reward": 971.0, "episode": 159.0, "Q1 loss": 5.045877700805664, "Q2 loss": 5.03599796295166, "Mean Target Q": 268.2939709472656, "Mean Q1": 268.29286242675784, "Mean Q2": 268.29274926757813, "critic_loss": 10.081875646591186, "batch_reward": 1.990890625, "actor_loss": -269.51660888671876, "actor_target_entropy": -2.0, "actor_entropy": 0.8482700915336608, "alpha_loss": 0.004001414294820279, "alpha_value": 0.07377958923085287, "duration": 69.68301773071289, "step": 39750}
{"episode_reward": 991.0, "episode": 160.0, "Q1 loss": 5.0553536834716795, "Q2 loss": 5.123589156150818, "Mean Target Q": 268.1243242797851, "Mean Q1": 268.12187670898436, "Mean Q2": 268.12170581054687, "critic_loss": 10.178942808151245, "batch_reward": 1.9858359375, "actor_loss": -269.53277783203123, "actor_target_entropy": -2.0, "actor_entropy": 0.8113539924621582, "alpha_loss": 0.005929283774457872, "alpha_value": 0.07326344053484139, "step": 40000}
{"duration": 77.63954520225525, "step": 40000}
{"episode_reward": 958.0, "episode": 161.0, "Q1 loss": 5.047414004325867, "Q2 loss": 5.051585657119751, "Mean Target Q": 270.2943067626953, "Mean Q1": 270.2954762573242, "Mean Q2": 270.29516735839843, "critic_loss": 10.098999658584594, "batch_reward": 2.0031484375, "actor_loss": -271.40331323242185, "actor_target_entropy": -2.0, "actor_entropy": 0.8040936012268066, "alpha_loss": 0.00364167204964906, "alpha_value": 0.07288326921948332, "duration": 69.40302300453186, "step": 40250}
{"episode_reward": 961.0, "episode": 162.0, "Q1 loss": 5.559063606262207, "Q2 loss": 5.5572663593292235, "Mean Target Q": 271.30778576660157, "Mean Q1": 271.30218103027346, "Mean Q2": 271.3021813964844, "critic_loss": 11.116329984664917, "batch_reward": 2.010515625, "actor_loss": -272.80249755859376, "actor_target_entropy": -2.0, "actor_entropy": 0.8327966141700744, "alpha_loss": 0.002496621986851096, "alpha_value": 0.07254459771516437, "duration": 69.07056093215942, "step": 40500}
{"episode_reward": 605.0, "episode": 163.0, "Q1 loss": 5.10690254688263, "Q2 loss": 5.090175287246704, "Mean Target Q": 272.4798957519531, "Mean Q1": 272.4775535888672, "Mean Q2": 272.47836706542967, "critic_loss": 10.19707780456543, "batch_reward": 2.0246015625, "actor_loss": -273.87808178710935, "actor_target_entropy": -2.0, "actor_entropy": 0.7724045157432556, "alpha_loss": 0.0033117438899353145, "alpha_value": 0.07222021422845151, "duration": 69.35518455505371, "step": 40750}
{"episode_reward": 944.0, "episode": 164.0, "Q1 loss": 5.450218695640564, "Q2 loss": 5.411618045806884, "Mean Target Q": 274.08897814941406, "Mean Q1": 274.08937353515626, "Mean Q2": 274.08844885253905, "critic_loss": 10.861836744308471, "batch_reward": 2.027015625, "actor_loss": -275.9413791503906, "actor_target_entropy": -2.0, "actor_entropy": 0.7796083698272706, "alpha_loss": 0.0012507889969274402, "alpha_value": 0.07203344038015899, "duration": 69.86708545684814, "step": 41000}
{"episode_reward": 989.0, "episode": 165.0, "Q1 loss": 5.2369308099746705, "Q2 loss": 5.243494788169861, "Mean Target Q": 274.8859840087891, "Mean Q1": 274.8823874511719, "Mean Q2": 274.8833975830078, "critic_loss": 10.480425582885742, "batch_reward": 2.0359453125, "actor_loss": -276.4904892578125, "actor_target_entropy": -2.0, "actor_entropy": 0.798025809764862, "alpha_loss": 0.0028421733621507884, "alpha_value": 0.07183758056439321, "duration": 69.76158785820007, "step": 41250}
{"episode_reward": 787.0, "episode": 166.0, "Q1 loss": 5.14347178554535, "Q2 loss": 5.121913425445556, "Mean Target Q": 275.5677633056641, "Mean Q1": 275.56803601074216, "Mean Q2": 275.5673375244141, "critic_loss": 10.26538522720337, "batch_reward": 2.0585078125, "actor_loss": -276.6840966796875, "actor_target_entropy": -2.0, "actor_entropy": 0.7889421043395997, "alpha_loss": 0.0006395395090803504, "alpha_value": 0.07159170609519115, "duration": 69.27502083778381, "step": 41500}
{"episode_reward": 980.0, "episode": 167.0, "Q1 loss": 5.251276970863342, "Q2 loss": 5.21247020149231, "Mean Target Q": 275.83469201660154, "Mean Q1": 275.83263427734374, "Mean Q2": 275.8331336669922, "critic_loss": 10.46374716949463, "batch_reward": 2.0585625, "actor_loss": -276.6326955566406, "actor_target_entropy": -2.0, "actor_entropy": 0.760676344871521, "alpha_loss": 0.0002978017865680158, "alpha_value": 0.0715626788431177, "duration": 69.45673274993896, "step": 41750}
{"episode_reward": 76.0, "episode": 168.0, "Q1 loss": 4.877536762237549, "Q2 loss": 4.86983500957489, "Mean Target Q": 276.7990689697266, "Mean Q1": 276.79830029296875, "Mean Q2": 276.7992491455078, "critic_loss": 9.747371795654297, "batch_reward": 2.0581328125, "actor_loss": -277.6908127441406, "actor_target_entropy": -2.0, "actor_entropy": 0.7750957765579224, "alpha_loss": 0.002762358617503196, "alpha_value": 0.07146023048423612, "duration": 69.35512471199036, "step": 42000}
{"episode_reward": 959.0, "episode": 169.0, "Q1 loss": 5.034026233673096, "Q2 loss": 4.986863821029663, "Mean Target Q": 278.01275830078123, "Mean Q1": 278.00989965820315, "Mean Q2": 278.0086232910156, "critic_loss": 10.02089003753662, "batch_reward": 2.063140625, "actor_loss": -279.3223173828125, "actor_target_entropy": -2.0, "actor_entropy": 0.7523538088798523, "alpha_loss": 0.0009169289094861597, "alpha_value": 0.07120295340603572, "duration": 69.36638045310974, "step": 42250}
{"episode_reward": 954.0, "episode": 170.0, "Q1 loss": 5.158736435890198, "Q2 loss": 5.12158270740509, "Mean Target Q": 278.28127185058594, "Mean Q1": 278.27961535644533, "Mean Q2": 278.2794997558594, "critic_loss": 10.280319118499756, "batch_reward": 2.0705546875, "actor_loss": -279.653302734375, "actor_target_entropy": -2.0, "actor_entropy": 0.7648984112739563, "alpha_loss": 0.002174927621148527, "alpha_value": 0.07106405107887677, "duration": 69.5844030380249, "step": 42500}
{"episode_reward": 976.0, "episode": 171.0, "Q1 loss": 5.042981385231018, "Q2 loss": 5.043268743515014, "Mean Target Q": 279.2830275878906, "Mean Q1": 279.2819044189453, "Mean Q2": 279.28252941894533, "critic_loss": 10.086250108718872, "batch_reward": 2.0704140625, "actor_loss": -280.5466140136719, "actor_target_entropy": -2.0, "actor_entropy": 0.7558571200370788, "alpha_loss": 0.005928544121794403, "alpha_value": 0.07065599676752923, "duration": 69.54148578643799, "step": 42750}
{"episode_reward": 985.0, "episode": 172.0, "Q1 loss": 5.261529383659362, "Q2 loss": 5.162251083374024, "Mean Target Q": 281.3553499755859, "Mean Q1": 281.35440148925784, "Mean Q2": 281.35557556152344, "critic_loss": 10.423780450820923, "batch_reward": 2.0975546875, "actor_loss": -282.3661789550781, "actor_target_entropy": -2.0, "actor_entropy": 0.7586558289527893, "alpha_loss": 0.00418780202139169, "alpha_value": 0.07019782464187578, "duration": 69.4769926071167, "step": 43000}
{"episode_reward": 970.0, "episode": 173.0, "Q1 loss": 5.055977999687195, "Q2 loss": 5.057581151008606, "Mean Target Q": 282.1548604736328, "Mean Q1": 282.15495947265623, "Mean Q2": 282.1541455078125, "critic_loss": 10.113559164047242, "batch_reward": 2.104828125, "actor_loss": -283.07382788085937, "actor_target_entropy": -2.0, "actor_entropy": 0.7103587675094605, "alpha_loss": 0.0049188753762282435, "alpha_value": 0.0697242197592181, "duration": 69.45071792602539, "step": 43250}
{"episode_reward": 991.0, "episode": 174.0, "Q1 loss": 5.0610827493667605, "Q2 loss": 5.090220832824707, "Mean Target Q": 282.80542626953127, "Mean Q1": 282.8015809326172, "Mean Q2": 282.8017209472656, "critic_loss": 10.15130357170105, "batch_reward": 2.101140625, "actor_loss": -283.86017578125, "actor_target_entropy": -2.0, "actor_entropy": 0.7223393397331238, "alpha_loss": 0.003590357911773026, "alpha_value": 0.06924511239055736, "duration": 69.84756660461426, "step": 43500}
{"episode_reward": 12.0, "episode": 175.0, "Q1 loss": 5.292227719306946, "Q2 loss": 5.237174794197083, "Mean Target Q": 283.6211177978516, "Mean Q1": 283.62074279785156, "Mean Q2": 283.62124865722654, "critic_loss": 10.529402496337891, "batch_reward": 2.103984375, "actor_loss": -285.3603581542969, "actor_target_entropy": -2.0, "actor_entropy": 0.7268186655044556, "alpha_loss": 0.0028204025207087396, "alpha_value": 0.06899744583983744, "duration": 69.70396971702576, "step": 43750}
{"episode_reward": 988.0, "episode": 176.0, "Q1 loss": 5.015638433456421, "Q2 loss": 5.007013486862182, "Mean Target Q": 284.96516149902345, "Mean Q1": 284.96412060546874, "Mean Q2": 284.96318701171873, "critic_loss": 10.022651908874511, "batch_reward": 2.1157265625, "actor_loss": -286.22782861328125, "actor_target_entropy": -2.0, "actor_entropy": 0.7312908678054809, "alpha_loss": 0.0019359742030501366, "alpha_value": 0.06870969403602033, "duration": 69.65072441101074, "step": 44000}
{"episode_reward": 530.0, "episode": 177.0, "Q1 loss": 5.300515965461731, "Q2 loss": 5.275873274803161, "Mean Target Q": 284.87776220703125, "Mean Q1": 284.87510803222654, "Mean Q2": 284.87707531738283, "critic_loss": 10.57638920021057, "batch_reward": 2.102328125, "actor_loss": -286.31779956054686, "actor_target_entropy": -2.0, "actor_entropy": 0.7386363024711609, "alpha_loss": 0.0004630237268283963, "alpha_value": 0.06867120123604831, "duration": 69.6335518360138, "step": 44250}
{"episode_reward": 156.0, "episode": 178.0, "Q1 loss": 5.216106090545654, "Q2 loss": 5.2592597007751465, "Mean Target Q": 286.45733666992186, "Mean Q1": 286.4548771972656, "Mean Q2": 286.4547071533203, "critic_loss": 10.475365798950195, "batch_reward": 2.1167890625, "actor_loss": -287.52338647460937, "actor_target_entropy": -2.0, "actor_entropy": 0.7273127484321594, "alpha_loss": -0.0008663251297548413, "alpha_value": 0.06867655950998135, "duration": 69.8532280921936, "step": 44500}
{"episode_reward": 993.0, "episode": 179.0, "Q1 loss": 5.085687610626221, "Q2 loss": 4.97978427696228, "Mean Target Q": 287.2280604248047, "Mean Q1": 287.22727197265624, "Mean Q2": 287.2264954833984, "critic_loss": 10.065471883773803, "batch_reward": 2.1152890625, "actor_loss": -288.395015625, "actor_target_entropy": -2.0, "actor_entropy": 0.7063034753799439, "alpha_loss": -0.002492464927956462, "alpha_value": 0.06886938352652376, "duration": 70.27484703063965, "step": 44750}
{"episode_reward": 951.0, "episode": 180.0, "Q1 loss": 5.369855285644531, "Q2 loss": 5.341516645431518, "Mean Target Q": 288.21598315429685, "Mean Q1": 288.2148269042969, "Mean Q2": 288.2147491455078, "critic_loss": 10.7113719291687, "batch_reward": 2.12590625, "actor_loss": -289.21779638671876, "actor_target_entropy": -2.0, "actor_entropy": 0.7393035106658935, "alpha_loss": -0.0015115658664144576, "alpha_value": 0.06900018476774661, "step": 45000}
{"duration": 78.87274241447449, "step": 45000}
{"episode_reward": 837.0, "episode": 181.0, "Q1 loss": 5.248028300285339, "Q2 loss": 5.237169459342956, "Mean Target Q": 289.398240234375, "Mean Q1": 289.3966513671875, "Mean Q2": 289.396666015625, "critic_loss": 10.485197744369508, "batch_reward": 2.124359375, "actor_loss": -290.51960815429686, "actor_target_entropy": -2.0, "actor_entropy": 0.7555754985809326, "alpha_loss": -0.003509981229901314, "alpha_value": 0.06926617692329262, "duration": 69.72138500213623, "step": 45250}
{"episode_reward": 326.0, "episode": 182.0, "Q1 loss": 5.266886192321778, "Q2 loss": 5.197390030860901, "Mean Target Q": 290.4650766601562, "Mean Q1": 290.46277099609375, "Mean Q2": 290.4623342285156, "critic_loss": 10.464276220321656, "batch_reward": 2.1274921875, "actor_loss": -291.9254897460938, "actor_target_entropy": -2.0, "actor_entropy": 0.7420584578514099, "alpha_loss": -0.0038059081602841616, "alpha_value": 0.06964206873088188, "duration": 69.45869398117065, "step": 45500}
{"episode_reward": 905.0, "episode": 183.0, "Q1 loss": 5.485574323654175, "Q2 loss": 5.503047799110413, "Mean Target Q": 290.89509204101563, "Mean Q1": 290.89420581054685, "Mean Q2": 290.89491088867186, "critic_loss": 10.98862212562561, "batch_reward": 2.1323203125, "actor_loss": -292.5339423828125, "actor_target_entropy": -2.0, "actor_entropy": 0.7850092196464539, "alpha_loss": -0.0015663887080736459, "alpha_value": 0.06988357992760752, "duration": 69.6974425315857, "step": 45750}
{"episode_reward": 900.0, "episode": 184.0, "Q1 loss": 5.34312761592865, "Q2 loss": 5.315999893188477, "Mean Target Q": 291.35854064941407, "Mean Q1": 291.358369140625, "Mean Q2": 291.3577814941406, "critic_loss": 10.659127508163452, "batch_reward": 2.1450703125, "actor_loss": -292.37531127929685, "actor_target_entropy": -2.0, "actor_entropy": 0.7409232964515686, "alpha_loss": -0.0035733002028428018, "alpha_value": 0.0702305608086841, "duration": 69.77663898468018, "step": 46000}
{"episode_reward": 833.0, "episode": 185.0, "Q1 loss": 5.531080869674683, "Q2 loss": 5.468977742195129, "Mean Target Q": 292.3029019775391, "Mean Q1": 292.3007878417969, "Mean Q2": 292.3014562988281, "critic_loss": 11.000058601379395, "batch_reward": 2.14175, "actor_loss": -293.5482170410156, "actor_target_entropy": -2.0, "actor_entropy": 0.7679850368499755, "alpha_loss": -0.0023030621660873295, "alpha_value": 0.07052963517045319, "duration": 69.67453384399414, "step": 46250}
{"episode_reward": 987.0, "episode": 186.0, "Q1 loss": 5.467079788208008, "Q2 loss": 5.4458042392730714, "Mean Target Q": 293.0928485107422, "Mean Q1": 293.0902373046875, "Mean Q2": 293.08917700195315, "critic_loss": 10.91288402557373, "batch_reward": 2.1551171875, "actor_loss": -294.67145458984373, "actor_target_entropy": -2.0, "actor_entropy": 0.713759808063507, "alpha_loss": -0.00022090323409065603, "alpha_value": 0.07058380370837823, "duration": 69.7986204624176, "step": 46500}
{"episode_reward": 101.0, "episode": 187.0, "Q1 loss": 5.693657234191894, "Q2 loss": 5.627591558456421, "Mean Target Q": 293.8017092285156, "Mean Q1": 293.79903857421874, "Mean Q2": 293.80033984375, "critic_loss": 11.32124881362915, "batch_reward": 2.1476171875, "actor_loss": -295.4661550292969, "actor_target_entropy": -2.0, "actor_entropy": 0.7141927709579468, "alpha_loss": 0.0003161962479352951, "alpha_value": 0.0706728889158579, "duration": 69.81124567985535, "step": 46750}
{"episode_reward": 991.0, "episode": 188.0, "Q1 loss": 5.497920964241028, "Q2 loss": 5.474725639343261, "Mean Target Q": 294.7407843017578, "Mean Q1": 294.73827185058593, "Mean Q2": 294.73901098632814, "critic_loss": 10.972646612167358, "batch_reward": 2.1556796875, "actor_loss": -296.40865600585937, "actor_target_entropy": -2.0, "actor_entropy": 0.6843777809143067, "alpha_loss": 0.002796891369856894, "alpha_value": 0.07046722490745691, "duration": 69.8178358078003, "step": 47000}
{"episode_reward": 54.0, "episode": 189.0, "Q1 loss": 5.693004039764404, "Q2 loss": 5.694086504936219, "Mean Target Q": 295.8384755859375, "Mean Q1": 295.83533752441406, "Mean Q2": 295.83487658691405, "critic_loss": 11.387090572357177, "batch_reward": 2.144625, "actor_loss": -297.18602319335935, "actor_target_entropy": -2.0, "actor_entropy": 0.7034154229164123, "alpha_loss": 0.0019009980913251639, "alpha_value": 0.07020822538822921, "duration": 69.72569751739502, "step": 47250}
{"episode_reward": 986.0, "episode": 190.0, "Q1 loss": 5.697487810134888, "Q2 loss": 5.6546084470748905, "Mean Target Q": 296.5941834716797, "Mean Q1": 296.59449841308594, "Mean Q2": 296.59432165527346, "critic_loss": 11.352096221923828, "batch_reward": 2.1501328125, "actor_loss": -297.79472265625, "actor_target_entropy": -2.0, "actor_entropy": 0.7101890730857849, "alpha_loss": -0.0018994265142828226, "alpha_value": 0.07021285496404478, "duration": 69.79748606681824, "step": 47500}
{"episode_reward": 449.0, "episode": 191.0, "Q1 loss": 5.62929622554779, "Q2 loss": 5.588533088684082, "Mean Target Q": 297.1157277832031, "Mean Q1": 297.1113864746094, "Mean Q2": 297.11137731933593, "critic_loss": 11.217829299926757, "batch_reward": 2.155671875, "actor_loss": -298.65325366210936, "actor_target_entropy": -2.0, "actor_entropy": 0.7213678917884827, "alpha_loss": -0.0019244981845840813, "alpha_value": 0.07038455709274478, "duration": 69.70388078689575, "step": 47750}
{"episode_reward": 74.0, "episode": 192.0, "Q1 loss": 5.4896266059875485, "Q2 loss": 5.454910124778747, "Mean Target Q": 297.9822244873047, "Mean Q1": 297.9803359375, "Mean Q2": 297.980208984375, "critic_loss": 10.944536777496339, "batch_reward": 2.1501640625, "actor_loss": -299.65630541992186, "actor_target_entropy": -2.0, "actor_entropy": 0.7169044017791748, "alpha_loss": 0.0005414475360885262, "alpha_value": 0.07045098834854173, "duration": 69.83318495750427, "step": 48000}
{"episode_reward": 947.0, "episode": 193.0, "Q1 loss": 5.262556077003479, "Q2 loss": 5.274907831192016, "Mean Target Q": 298.34625170898437, "Mean Q1": 298.34731958007814, "Mean Q2": 298.34824853515624, "critic_loss": 10.537463920593261, "batch_reward": 2.157984375, "actor_loss": -299.4363837890625, "actor_target_entropy": -2.0, "actor_entropy": 0.6905927863121033, "alpha_loss": 0.0009284894354641437, "alpha_value": 0.07040756073843733, "duration": 70.13672924041748, "step": 48250}
{"episode_reward": 996.0, "episode": 194.0, "Q1 loss": 5.204906665802002, "Q2 loss": 5.192281694412231, "Mean Target Q": 299.41494763183596, "Mean Q1": 299.41301489257813, "Mean Q2": 299.4118887939453, "critic_loss": 10.39718837738037, "batch_reward": 2.167875, "actor_loss": -300.1468369140625, "actor_target_entropy": -2.0, "actor_entropy": 0.719615595817566, "alpha_loss": 0.0019325811685994268, "alpha_value": 0.07022533601577151, "duration": 69.92371034622192, "step": 48500}
{"episode_reward": 965.0, "episode": 195.0, "Q1 loss": 5.187315051078796, "Q2 loss": 5.194429085731507, "Mean Target Q": 300.04729528808593, "Mean Q1": 300.0446895751953, "Mean Q2": 300.0449521484375, "critic_loss": 10.381744188308716, "batch_reward": 2.167984375, "actor_loss": -301.34038354492185, "actor_target_entropy": -2.0, "actor_entropy": 0.7019861817359925, "alpha_loss": -0.000988923323340714, "alpha_value": 0.0701430958395884, "duration": 70.33453679084778, "step": 48750}
{"episode_reward": 969.0, "episode": 196.0, "Q1 loss": 5.233318978309631, "Q2 loss": 5.244244733810425, "Mean Target Q": 301.05868884277345, "Mean Q1": 301.058349609375, "Mean Q2": 301.0584739990234, "critic_loss": 10.477563724517822, "batch_reward": 2.1796015625, "actor_loss": -302.1402307128906, "actor_target_entropy": -2.0, "actor_entropy": 0.7167126235961914, "alpha_loss": -0.0012534814868122338, "alpha_value": 0.07024424979441697, "duration": 69.90977144241333, "step": 49000}
{"episode_reward": 756.0, "episode": 197.0, "Q1 loss": 5.457159009933472, "Q2 loss": 5.430420922279358, "Mean Target Q": 301.2097595214844, "Mean Q1": 301.2084278564453, "Mean Q2": 301.20923120117186, "critic_loss": 10.887579963684082, "batch_reward": 2.1773046875, "actor_loss": -302.2077580566406, "actor_target_entropy": -2.0, "actor_entropy": 0.6894938173294067, "alpha_loss": 0.0018229174711741508, "alpha_value": 0.07027696854311875, "duration": 69.84810328483582, "step": 49250}
{"episode_reward": 300.0, "episode": 198.0, "Q1 loss": 5.524897599220276, "Q2 loss": 5.514604918479919, "Mean Target Q": 302.28666955566405, "Mean Q1": 302.2819444580078, "Mean Q2": 302.2810168457031, "critic_loss": 11.039502531051635, "batch_reward": 2.1758671875, "actor_loss": -303.109626953125, "actor_target_entropy": -2.0, "actor_entropy": 0.7384906902313232, "alpha_loss": -0.0019314694530330597, "alpha_value": 0.07030668188286124, "duration": 69.7970724105835, "step": 49500}
{"episode_reward": 0.0, "episode": 199.0, "Q1 loss": 5.384288171768189, "Q2 loss": 5.399095622062683, "Mean Target Q": 301.80802270507814, "Mean Q1": 301.8068889160156, "Mean Q2": 301.8085021972656, "critic_loss": 10.783383769989014, "batch_reward": 2.158671875, "actor_loss": -303.155068359375, "actor_target_entropy": -2.0, "actor_entropy": 0.7383348679542542, "alpha_loss": -0.0014651965936645864, "alpha_value": 0.07045995748237979, "duration": 69.98309016227722, "step": 49750}
{"episode_reward": 0.0, "episode": 200.0, "Q1 loss": 5.230846810340881, "Q2 loss": 5.2148484592437745, "Mean Target Q": 302.30928454589844, "Mean Q1": 302.3094442138672, "Mean Q2": 302.3079929199219, "critic_loss": 10.445695247650146, "batch_reward": 2.162625, "actor_loss": -302.9624755859375, "actor_target_entropy": -2.0, "actor_entropy": 0.7507845959663391, "alpha_loss": -0.0026423777900636196, "alpha_value": 0.07066940039675623, "step": 50000}
{"duration": 78.29768347740173, "step": 50000}
{"episode_reward": 976.0, "episode": 201.0, "Q1 loss": 5.294320690155029, "Q2 loss": 5.29933207988739, "Mean Target Q": 303.2303763427734, "Mean Q1": 303.2290213623047, "Mean Q2": 303.23034045410157, "critic_loss": 10.593652772903443, "batch_reward": 2.1791015625, "actor_loss": -304.4057001953125, "actor_target_entropy": -2.0, "actor_entropy": 0.7144453868865966, "alpha_loss": 0.0005991477193310857, "alpha_value": 0.07080797183446204, "duration": 70.02872276306152, "step": 50250}
{"episode_reward": 994.0, "episode": 202.0, "Q1 loss": 5.214497151374817, "Q2 loss": 5.126761709213257, "Mean Target Q": 303.7747491455078, "Mean Q1": 303.77292907714843, "Mean Q2": 303.77184692382815, "critic_loss": 10.34125883293152, "batch_reward": 2.1779453125, "actor_loss": -305.1045334472656, "actor_target_entropy": -2.0, "actor_entropy": 0.7337441520690918, "alpha_loss": 0.0009717367999255657, "alpha_value": 0.07074056351962835, "duration": 69.6078360080719, "step": 50500}
{"episode_reward": 974.0, "episode": 203.0, "Q1 loss": 5.172474805831909, "Q2 loss": 5.179682234764099, "Mean Target Q": 303.9430059814453, "Mean Q1": 303.9416611328125, "Mean Q2": 303.9413054199219, "critic_loss": 10.352157035827636, "batch_reward": 2.179859375, "actor_loss": -304.709208984375, "actor_target_entropy": -2.0, "actor_entropy": 0.712084135055542, "alpha_loss": 0.0005543178021907806, "alpha_value": 0.07061187596513148, "duration": 69.95023012161255, "step": 50750}
{"episode_reward": 0.0, "episode": 204.0, "Q1 loss": 5.11051203918457, "Q2 loss": 5.118069721221924, "Mean Target Q": 304.73143212890625, "Mean Q1": 304.7300910644531, "Mean Q2": 304.73069567871096, "critic_loss": 10.228581743240357, "batch_reward": 2.182296875, "actor_loss": -305.9249011230469, "actor_target_entropy": -2.0, "actor_entropy": 0.7400354900360108, "alpha_loss": 0.0015020952001214027, "alpha_value": 0.07052838899437021, "duration": 69.89196872711182, "step": 51000}
{"episode_reward": 931.0, "episode": 205.0, "Q1 loss": 5.442959077835083, "Q2 loss": 5.428323225975037, "Mean Target Q": 304.45167236328126, "Mean Q1": 304.45019848632813, "Mean Q2": 304.4518492431641, "critic_loss": 10.871282283782959, "batch_reward": 2.175203125, "actor_loss": -305.52749462890625, "actor_target_entropy": -2.0, "actor_entropy": 0.7539958896636962, "alpha_loss": -3.6318609490990637e-05, "alpha_value": 0.07048441697675085, "duration": 69.9757513999939, "step": 51250}
{"episode_reward": 37.0, "episode": 206.0, "Q1 loss": 5.324001976966858, "Q2 loss": 5.28911671257019, "Mean Target Q": 305.00366552734374, "Mean Q1": 305.0004796142578, "Mean Q2": 304.99943127441406, "critic_loss": 10.613118673324585, "batch_reward": 2.174625, "actor_loss": -305.9075466308594, "actor_target_entropy": -2.0, "actor_entropy": 0.7371763558387756, "alpha_loss": 0.0038807794586755333, "alpha_value": 0.0702375681222802, "duration": 69.79427361488342, "step": 51500}
{"episode_reward": 977.0, "episode": 207.0, "Q1 loss": 5.2567586498260495, "Q2 loss": 5.2483119993209835, "Mean Target Q": 305.53724365234376, "Mean Q1": 305.5375849609375, "Mean Q2": 305.53736975097655, "critic_loss": 10.505070644378662, "batch_reward": 2.18896875, "actor_loss": -306.40161767578127, "actor_target_entropy": -2.0, "actor_entropy": 0.7551429343223571, "alpha_loss": 0.0023445495758205653, "alpha_value": 0.06986681765312629, "duration": 69.8558132648468, "step": 51750}
{"episode_reward": 959.0, "episode": 208.0, "Q1 loss": 5.247871913909912, "Q2 loss": 5.206663096427918, "Mean Target Q": 306.3339974365234, "Mean Q1": 306.33144104003907, "Mean Q2": 306.33196997070314, "critic_loss": 10.454535024642944, "batch_reward": 2.199859375, "actor_loss": -307.67841137695314, "actor_target_entropy": -2.0, "actor_entropy": 0.7507345991134644, "alpha_loss": 0.0028859538147225977, "alpha_value": 0.0696357841003392, "duration": 70.55404090881348, "step": 52000}
{"episode_reward": 967.0, "episode": 209.0, "Q1 loss": 5.328255241394043, "Q2 loss": 5.327835104942322, "Mean Target Q": 306.56994921875, "Mean Q1": 306.5668505859375, "Mean Q2": 306.5678918457031, "critic_loss": 10.656090351104737, "batch_reward": 2.2022265625, "actor_loss": -307.7630988769531, "actor_target_entropy": -2.0, "actor_entropy": 0.7426906509399414, "alpha_loss": 0.0013563716397620737, "alpha_value": 0.06938579981325993, "duration": 70.14031839370728, "step": 52250}
{"episode_reward": 495.0, "episode": 210.0, "Q1 loss": 5.391603904724121, "Q2 loss": 5.399673519134521, "Mean Target Q": 306.56210803222655, "Mean Q1": 306.5639776611328, "Mean Q2": 306.5634669189453, "critic_loss": 10.79127740097046, "batch_reward": 2.1999375, "actor_loss": -307.1480830078125, "actor_target_entropy": -2.0, "actor_entropy": 0.6950405106544495, "alpha_loss": 0.0015854362682439387, "alpha_value": 0.06923376120599489, "duration": 70.47802567481995, "step": 52500}
{"episode_reward": 961.0, "episode": 211.0, "Q1 loss": 5.251313877105713, "Q2 loss": 5.282648384094238, "Mean Target Q": 306.9495324707031, "Mean Q1": 306.94673474121095, "Mean Q2": 306.94727209472654, "critic_loss": 10.533962261199951, "batch_reward": 2.200109375, "actor_loss": -307.5093947753906, "actor_target_entropy": -2.0, "actor_entropy": 0.6800110416412354, "alpha_loss": 0.003541972313541919, "alpha_value": 0.06890503675662935, "duration": 69.95898413658142, "step": 52750}
{"episode_reward": 972.0, "episode": 212.0, "Q1 loss": 5.334625021934509, "Q2 loss": 5.302648189544677, "Mean Target Q": 308.443404296875, "Mean Q1": 308.4435145263672, "Mean Q2": 308.4422578125, "critic_loss": 10.637273180007934, "batch_reward": 2.2203359375, "actor_loss": -308.99490185546875, "actor_target_entropy": -2.0, "actor_entropy": 0.6512452335357666, "alpha_loss": 0.003772677403409034, "alpha_value": 0.06857392782506616, "duration": 70.0398383140564, "step": 53000}
{"episode_reward": 976.0, "episode": 213.0, "Q1 loss": 5.427312448501587, "Q2 loss": 5.3601737356185915, "Mean Target Q": 309.1010690917969, "Mean Q1": 309.1017844238281, "Mean Q2": 309.10263818359374, "critic_loss": 10.787486156463624, "batch_reward": 2.2244140625, "actor_loss": -309.52835571289063, "actor_target_entropy": -2.0, "actor_entropy": 0.6728641419410706, "alpha_loss": 0.004515561987645924, "alpha_value": 0.06809630446313214, "duration": 70.21217012405396, "step": 53250}
{"episode_reward": 985.0, "episode": 214.0, "Q1 loss": 5.328671934127808, "Q2 loss": 5.3256115064620975, "Mean Target Q": 309.51036791992186, "Mean Q1": 309.50534252929685, "Mean Q2": 309.5052694091797, "critic_loss": 10.654283416748047, "batch_reward": 2.22659375, "actor_loss": -310.94297485351564, "actor_target_entropy": -2.0, "actor_entropy": 0.6641732745170593, "alpha_loss": 0.0004013644615188241, "alpha_value": 0.06787844286700719, "duration": 70.16129064559937, "step": 53500}
{"episode_reward": 971.0, "episode": 215.0, "Q1 loss": 5.507359331130981, "Q2 loss": 5.544653421401978, "Mean Target Q": 310.1278721923828, "Mean Q1": 310.1288096923828, "Mean Q2": 310.1284200439453, "critic_loss": 11.05201276397705, "batch_reward": 2.231671875, "actor_loss": -310.9744196777344, "actor_target_entropy": -2.0, "actor_entropy": 0.7095968565940857, "alpha_loss": -0.0007281313431449234, "alpha_value": 0.06785875705212036, "duration": 70.13396525382996, "step": 53750}
{"episode_reward": 953.0, "episode": 216.0, "Q1 loss": 5.5125857992172245, "Q2 loss": 5.480871000289917, "Mean Target Q": 311.14038244628904, "Mean Q1": 311.1366898193359, "Mean Q2": 311.13773815917966, "critic_loss": 10.993456783294677, "batch_reward": 2.2380859375, "actor_loss": -312.3455126953125, "actor_target_entropy": -2.0, "actor_entropy": 0.6886049876213074, "alpha_loss": -0.0028221278740093113, "alpha_value": 0.06807667111465736, "duration": 70.21489214897156, "step": 54000}
{"episode_reward": 0.0, "episode": 217.0, "Q1 loss": 5.703203175544739, "Q2 loss": 5.761813903808593, "Mean Target Q": 311.45879431152343, "Mean Q1": 311.45876037597657, "Mean Q2": 311.45849755859376, "critic_loss": 11.465017091751099, "batch_reward": 2.234578125, "actor_loss": -312.6541123046875, "actor_target_entropy": -2.0, "actor_entropy": 0.7216912589073181, "alpha_loss": 0.00024704797938466074, "alpha_value": 0.06816342988954517, "duration": 70.27664923667908, "step": 54250}
{"episode_reward": 967.0, "episode": 218.0, "Q1 loss": 5.777055637359619, "Q2 loss": 5.733101037979126, "Mean Target Q": 312.2213933105469, "Mean Q1": 312.21888732910156, "Mean Q2": 312.2199053955078, "critic_loss": 11.510156692504882, "batch_reward": 2.24703125, "actor_loss": -313.36603344726564, "actor_target_entropy": -2.0, "actor_entropy": 0.7051176428794861, "alpha_loss": 0.00042005332186818124, "alpha_value": 0.06817552547052189, "duration": 70.2103443145752, "step": 54500}
{"episode_reward": 977.0, "episode": 219.0, "Q1 loss": 5.721328542709351, "Q2 loss": 5.677656867027283, "Mean Target Q": 312.95404248046873, "Mean Q1": 312.9547971191406, "Mean Q2": 312.9545020751953, "critic_loss": 11.398985370635986, "batch_reward": 2.244953125, "actor_loss": -314.2214033203125, "actor_target_entropy": -2.0, "actor_entropy": 0.6945804653167724, "alpha_loss": -0.0009714507414028048, "alpha_value": 0.06821924286293958, "duration": 70.26168131828308, "step": 54750}
{"episode_reward": 0.0, "episode": 220.0, "Q1 loss": 5.487511355400086, "Q2 loss": 5.477522813796997, "Mean Target Q": 313.09251989746093, "Mean Q1": 313.08981469726564, "Mean Q2": 313.08985693359375, "critic_loss": 10.965034166336059, "batch_reward": 2.24128125, "actor_loss": -314.1268674316406, "actor_target_entropy": -2.0, "actor_entropy": 0.722713472366333, "alpha_loss": -0.001981175186112523, "alpha_value": 0.06831963497006978, "step": 55000}
{"duration": 78.53173398971558, "step": 55000}
{"episode_reward": 980.0, "episode": 221.0, "Q1 loss": 5.506398600578308, "Q2 loss": 5.479555377960205, "Mean Target Q": 314.3045953369141, "Mean Q1": 314.3009739990234, "Mean Q2": 314.30087194824216, "critic_loss": 10.985953977584838, "batch_reward": 2.2522265625, "actor_loss": -315.47850341796874, "actor_target_entropy": -2.0, "actor_entropy": 0.7637713332176208, "alpha_loss": -0.0031764521095901726, "alpha_value": 0.06862876506256015, "duration": 70.17129468917847, "step": 55250}
{"episode_reward": 973.0, "episode": 222.0, "Q1 loss": 5.544303030967712, "Q2 loss": 5.5301336040496825, "Mean Target Q": 314.5712386474609, "Mean Q1": 314.571310546875, "Mean Q2": 314.57191259765625, "critic_loss": 11.074436595916747, "batch_reward": 2.2546796875, "actor_loss": -315.6999877929687, "actor_target_entropy": -2.0, "actor_entropy": 0.7753405599594116, "alpha_loss": -0.003928381663747132, "alpha_value": 0.06895998869521514, "duration": 70.46614003181458, "step": 55500}
{"episode_reward": 871.0, "episode": 223.0, "Q1 loss": 5.4164380979537965, "Q2 loss": 5.403398592948913, "Mean Target Q": 314.9469580078125, "Mean Q1": 314.9468809814453, "Mean Q2": 314.946724609375, "critic_loss": 10.819836700439453, "batch_reward": 2.26196875, "actor_loss": -315.59899072265625, "actor_target_entropy": -2.0, "actor_entropy": 0.8025979843139649, "alpha_loss": -0.000999918789602816, "alpha_value": 0.06931232346442648, "duration": 69.94735670089722, "step": 55750}
{"episode_reward": 977.0, "episode": 224.0, "Q1 loss": 5.577898221969605, "Q2 loss": 5.565891829490662, "Mean Target Q": 314.9373641357422, "Mean Q1": 314.9326375732422, "Mean Q2": 314.93346899414064, "critic_loss": 11.14379002571106, "batch_reward": 2.264171875, "actor_loss": -316.1324033203125, "actor_target_entropy": -2.0, "actor_entropy": 0.7370836615562439, "alpha_loss": -0.0012947751861065626, "alpha_value": 0.06933713061806611, "duration": 69.91358637809753, "step": 56000}
{"episode_reward": 917.0, "episode": 225.0, "Q1 loss": 5.43452002620697, "Q2 loss": 5.43118190574646, "Mean Target Q": 315.40480505371096, "Mean Q1": 315.40596704101563, "Mean Q2": 315.404767578125, "critic_loss": 10.865701915740967, "batch_reward": 2.270859375, "actor_loss": -316.37118017578126, "actor_target_entropy": -2.0, "actor_entropy": 0.737585202217102, "alpha_loss": -0.0007305300831794739, "alpha_value": 0.06949914046889034, "duration": 69.90245032310486, "step": 56250}
{"episode_reward": 975.0, "episode": 226.0, "Q1 loss": 5.423304744720459, "Q2 loss": 5.441459075927734, "Mean Target Q": 316.8482591552734, "Mean Q1": 316.84555944824217, "Mean Q2": 316.84558276367187, "critic_loss": 10.864763875961303, "batch_reward": 2.2848359375, "actor_loss": -318.0110395507813, "actor_target_entropy": -2.0, "actor_entropy": 0.7552722158432007, "alpha_loss": -0.002481890610419214, "alpha_value": 0.0697081315642245, "duration": 70.59051609039307, "step": 56500}
{"episode_reward": 971.0, "episode": 227.0, "Q1 loss": 5.429636555671692, "Q2 loss": 5.428250903129578, "Mean Target Q": 317.18870861816407, "Mean Q1": 317.1860916748047, "Mean Q2": 317.18655688476565, "critic_loss": 10.857887460708618, "batch_reward": 2.280359375, "actor_loss": -318.69054248046876, "actor_target_entropy": -2.0, "actor_entropy": 0.7323401565551758, "alpha_loss": -0.0010832515000365674, "alpha_value": 0.06986153981555907, "duration": 70.13420128822327, "step": 56750}
{"episode_reward": 635.0, "episode": 228.0, "Q1 loss": 5.409431309700012, "Q2 loss": 5.414327484130859, "Mean Target Q": 318.26152307128905, "Mean Q1": 318.26144274902344, "Mean Q2": 318.26176989746097, "critic_loss": 10.823758823394776, "batch_reward": 2.2944453125, "actor_loss": -319.34979931640623, "actor_target_entropy": -2.0, "actor_entropy": 0.7281566543579101, "alpha_loss": 0.0010140037462115288, "alpha_value": 0.06983277690172683, "duration": 70.0963385105133, "step": 57000}
{"episode_reward": 969.0, "episode": 229.0, "Q1 loss": 5.636251164436341, "Q2 loss": 5.637462369918823, "Mean Target Q": 318.9307344970703, "Mean Q1": 318.92744104003907, "Mean Q2": 318.9266129150391, "critic_loss": 11.273713567733765, "batch_reward": 2.2934140625, "actor_loss": -319.62602758789063, "actor_target_entropy": -2.0, "actor_entropy": 0.7085145769119263, "alpha_loss": 0.0010401944406330586, "alpha_value": 0.06967719762402057, "duration": 116.02560377120972, "step": 57250}
{"episode_reward": 909.0, "episode": 230.0, "Q1 loss": 5.539418906211853, "Q2 loss": 5.493424304962158, "Mean Target Q": 319.4086584472656, "Mean Q1": 319.4067032470703, "Mean Q2": 319.4073957519531, "critic_loss": 11.032843206405639, "batch_reward": 2.3022265625, "actor_loss": -320.5304697265625, "actor_target_entropy": -2.0, "actor_entropy": 0.6933566884994506, "alpha_loss": 0.0018111339909955859, "alpha_value": 0.06955117554974329, "duration": 73.86147832870483, "step": 57500}
{"episode_reward": 985.0, "episode": 231.0, "Q1 loss": 5.390915201187134, "Q2 loss": 5.456722817420959, "Mean Target Q": 319.8040360107422, "Mean Q1": 319.8032059326172, "Mean Q2": 319.80172204589843, "critic_loss": 10.847638027191163, "batch_reward": 2.303796875, "actor_loss": -321.03302978515626, "actor_target_entropy": -2.0, "actor_entropy": 0.7124461727142334, "alpha_loss": -0.0020968604972586035, "alpha_value": 0.06957481736998476, "duration": 70.27414631843567, "step": 57750}
{"episode_reward": 975.0, "episode": 232.0, "Q1 loss": 5.448289495468139, "Q2 loss": 5.491537364959717, "Mean Target Q": 320.9951949462891, "Mean Q1": 320.99156372070314, "Mean Q2": 320.9915316162109, "critic_loss": 10.939826868057251, "batch_reward": 2.3102734375, "actor_loss": -321.9102302246094, "actor_target_entropy": -2.0, "actor_entropy": 0.7019071316719055, "alpha_loss": 0.0013381916768848896, "alpha_value": 0.0696326782609672, "duration": 70.42326307296753, "step": 58000}
{"episode_reward": 997.0, "episode": 233.0, "Q1 loss": 5.346831621170044, "Q2 loss": 5.311548034667969, "Mean Target Q": 321.4096865234375, "Mean Q1": 321.412560546875, "Mean Q2": 321.41368103027344, "critic_loss": 10.65837965774536, "batch_reward": 2.326453125, "actor_loss": -322.7149013671875, "actor_target_entropy": -2.0, "actor_entropy": 0.6991987557411193, "alpha_loss": 0.0023818792849779127, "alpha_value": 0.06942782107377374, "duration": 71.05933046340942, "step": 58250}
{"episode_reward": 968.0, "episode": 234.0, "Q1 loss": 5.222870573997498, "Q2 loss": 5.243487519264221, "Mean Target Q": 322.56307690429685, "Mean Q1": 322.5620979003906, "Mean Q2": 322.5619807128906, "critic_loss": 10.46635810470581, "batch_reward": 2.338359375, "actor_loss": -323.8516999511719, "actor_target_entropy": -2.0, "actor_entropy": 0.688677001953125, "alpha_loss": 0.0013245256077498197, "alpha_value": 0.0692845507646069, "duration": 70.29942893981934, "step": 58500}
{"episode_reward": 962.0, "episode": 235.0, "Q1 loss": 5.47560488986969, "Q2 loss": 5.472575240135193, "Mean Target Q": 323.3032915039062, "Mean Q1": 323.29858544921876, "Mean Q2": 323.29935791015623, "critic_loss": 10.948180130004882, "batch_reward": 2.337078125, "actor_loss": -324.7849558105469, "actor_target_entropy": -2.0, "actor_entropy": 0.6837690534591675, "alpha_loss": 0.0008648928911425173, "alpha_value": 0.0691345641689919, "duration": 70.3121383190155, "step": 58750}
{"episode_reward": 987.0, "episode": 236.0, "Q1 loss": 5.586502482414246, "Q2 loss": 5.571816836357117, "Mean Target Q": 323.9888801269531, "Mean Q1": 323.9886845703125, "Mean Q2": 323.9875321044922, "critic_loss": 11.158319324493409, "batch_reward": 2.34215625, "actor_loss": -324.93051708984376, "actor_target_entropy": -2.0, "actor_entropy": 0.6492026872634887, "alpha_loss": 0.0008977335803210735, "alpha_value": 0.06903478727269233, "duration": 71.22972655296326, "step": 59000}
{"episode_reward": 991.0, "episode": 237.0, "Q1 loss": 5.384479244232177, "Q2 loss": 5.433278753280639, "Mean Target Q": 324.74331591796874, "Mean Q1": 324.74253967285154, "Mean Q2": 324.7429232177734, "critic_loss": 10.817757978439332, "batch_reward": 2.3572109375, "actor_loss": -326.13303466796873, "actor_target_entropy": -2.0, "actor_entropy": 0.663867199420929, "alpha_loss": 0.002441047721076757, "alpha_value": 0.06884029799573384, "duration": 70.29689288139343, "step": 59250}
{"episode_reward": 963.0, "episode": 238.0, "Q1 loss": 5.656709916114807, "Q2 loss": 5.715806679725647, "Mean Target Q": 324.943537109375, "Mean Q1": 324.9403662109375, "Mean Q2": 324.94010119628905, "critic_loss": 11.372516590118408, "batch_reward": 2.3458359375, "actor_loss": -325.9389660644531, "actor_target_entropy": -2.0, "actor_entropy": 0.7195813417434692, "alpha_loss": -0.00035913600074127317, "alpha_value": 0.06872249660774048, "duration": 70.48616194725037, "step": 59500}
{"episode_reward": 294.0, "episode": 239.0, "Q1 loss": 5.669595255851745, "Q2 loss": 5.640226528167725, "Mean Target Q": 325.2507785644531, "Mean Q1": 325.2491707763672, "Mean Q2": 325.2489079589844, "critic_loss": 11.309821758270264, "batch_reward": 2.3455234375, "actor_loss": -326.304134765625, "actor_target_entropy": -2.0, "actor_entropy": 0.7035521159172058, "alpha_loss": -0.0003534616231918335, "alpha_value": 0.06878170092920975, "duration": 70.46992444992065, "step": 59750}
{"episode_reward": 980.0, "episode": 240.0, "Q1 loss": 5.482496036529541, "Q2 loss": 5.482924563407898, "Mean Target Q": 325.4935975341797, "Mean Q1": 325.49135815429685, "Mean Q2": 325.4922227783203, "critic_loss": 10.965420589447021, "batch_reward": 2.3520625, "actor_loss": -326.9082404785156, "actor_target_entropy": -2.0, "actor_entropy": 0.6715653519630432, "alpha_loss": -0.000995191558264196, "alpha_value": 0.06883568812772684, "step": 60000}
{"duration": 84.5274658203125, "step": 60000}
{"episode_reward": 948.0, "episode": 241.0, "Q1 loss": 5.5419179697036745, "Q2 loss": 5.61509665298462, "Mean Target Q": 326.53470495605467, "Mean Q1": 326.53464208984377, "Mean Q2": 326.53456652832034, "critic_loss": 11.157014608383179, "batch_reward": 2.356359375, "actor_loss": -327.6924206542969, "actor_target_entropy": -2.0, "actor_entropy": 0.7076410393714905, "alpha_loss": -0.0025403858204372225, "alpha_value": 0.06900978538154545, "duration": 70.36113905906677, "step": 60250}
{"episode_reward": 940.0, "episode": 242.0, "Q1 loss": 5.557007005691529, "Q2 loss": 5.505382824897766, "Mean Target Q": 327.1102307128906, "Mean Q1": 327.10897229003905, "Mean Q2": 327.10869030761717, "critic_loss": 11.06238983154297, "batch_reward": 2.3710546875, "actor_loss": -328.40192236328124, "actor_target_entropy": -2.0, "actor_entropy": 0.7456392955780029, "alpha_loss": -0.00198584992159158, "alpha_value": 0.0692813919231094, "duration": 70.33357334136963, "step": 60500}
{"episode_reward": 947.0, "episode": 243.0, "Q1 loss": 5.478955204963684, "Q2 loss": 5.484580359458923, "Mean Target Q": 328.48831506347653, "Mean Q1": 328.486123046875, "Mean Q2": 328.4862872314453, "critic_loss": 10.963535593032837, "batch_reward": 2.377890625, "actor_loss": -329.72811938476565, "actor_target_entropy": -2.0, "actor_entropy": 0.7237966523170472, "alpha_loss": 0.0008925170889124274, "alpha_value": 0.0694090487391082, "duration": 70.3151638507843, "step": 60750}
{"episode_reward": 927.0, "episode": 244.0, "Q1 loss": 5.44384020614624, "Q2 loss": 5.419347825050354, "Mean Target Q": 329.0909265136719, "Mean Q1": 329.08826440429686, "Mean Q2": 329.08885278320315, "critic_loss": 10.863188041687012, "batch_reward": 2.3807421875, "actor_loss": -330.10165380859377, "actor_target_entropy": -2.0, "actor_entropy": 0.7480520720481872, "alpha_loss": 0.001715316460467875, "alpha_value": 0.06912533052094734, "duration": 95.6428325176239, "step": 61000}
{"episode_reward": 921.0, "episode": 245.0, "Q1 loss": 5.626286624908447, "Q2 loss": 5.606868753433227, "Mean Target Q": 328.8953395996094, "Mean Q1": 328.8971358642578, "Mean Q2": 328.89589233398436, "critic_loss": 11.233155376434325, "batch_reward": 2.3923046875, "actor_loss": -329.61845971679685, "actor_target_entropy": -2.0, "actor_entropy": 0.7184469656944275, "alpha_loss": 0.0003157213730737567, "alpha_value": 0.06915115116597031, "duration": 70.62875819206238, "step": 61250}
{"episode_reward": 984.0, "episode": 246.0, "Q1 loss": 5.392046080589294, "Q2 loss": 5.446973300933838, "Mean Target Q": 329.47268969726565, "Mean Q1": 329.4676030273437, "Mean Q2": 329.4673123779297, "critic_loss": 10.839019355773926, "batch_reward": 2.3821640625, "actor_loss": -330.2982109375, "actor_target_entropy": -2.0, "actor_entropy": 0.7079026055335998, "alpha_loss": 0.0012794124712236226, "alpha_value": 0.06900865588848716, "duration": 70.55173563957214, "step": 61500}
{"episode_reward": 977.0, "episode": 247.0, "Q1 loss": 5.619135071754456, "Q2 loss": 5.66541402053833, "Mean Target Q": 330.0780704345703, "Mean Q1": 330.07671374511716, "Mean Q2": 330.0770035400391, "critic_loss": 11.284549077987672, "batch_reward": 2.3851171875, "actor_loss": -331.4224750976563, "actor_target_entropy": -2.0, "actor_entropy": 0.7129451327323914, "alpha_loss": 0.0006405555689707399, "alpha_value": 0.06890420171081874, "duration": 70.47570323944092, "step": 61750}
{"episode_reward": 375.0, "episode": 248.0, "Q1 loss": 5.366423539161682, "Q2 loss": 5.325423674583435, "Mean Target Q": 331.04886669921876, "Mean Q1": 331.04795788574216, "Mean Q2": 331.0476977539063, "critic_loss": 10.691847194671631, "batch_reward": 2.4038515625, "actor_loss": -332.11873583984374, "actor_target_entropy": -2.0, "actor_entropy": 0.7321884799003601, "alpha_loss": 0.0002669597575441003, "alpha_value": 0.06890188271349437, "duration": 70.53149366378784, "step": 62000}
{"episode_reward": 983.0, "episode": 249.0, "Q1 loss": 5.39441976928711, "Q2 loss": 5.421395809173584, "Mean Target Q": 331.1635885009766, "Mean Q1": 331.1621268310547, "Mean Q2": 331.1624560546875, "critic_loss": 10.815815547943116, "batch_reward": 2.3949375, "actor_loss": -332.0004982910156, "actor_target_entropy": -2.0, "actor_entropy": 0.7106999258995056, "alpha_loss": 0.00042331129219383, "alpha_value": 0.0687844095425997, "duration": 71.29223370552063, "step": 62250}
{"episode_reward": 818.0, "episode": 250.0, "Q1 loss": 5.483982422828674, "Q2 loss": 5.4885962867736815, "Mean Target Q": 332.2238759765625, "Mean Q1": 332.22188671875, "Mean Q2": 332.2217713623047, "critic_loss": 10.97257871246338, "batch_reward": 2.4107578125, "actor_loss": -332.83298901367186, "actor_target_entropy": -2.0, "actor_entropy": 0.7427033801078796, "alpha_loss": 0.0017711962563917041, "alpha_value": 0.06870597887406205, "duration": 70.50637292861938, "step": 62500}
{"episode_reward": 994.0, "episode": 251.0, "Q1 loss": 5.217096573829651, "Q2 loss": 5.199019580841065, "Mean Target Q": 332.6640349121094, "Mean Q1": 332.6620699462891, "Mean Q2": 332.6621469726563, "critic_loss": 10.416116149902344, "batch_reward": 2.414140625, "actor_loss": -333.81919067382813, "actor_target_entropy": -2.0, "actor_entropy": 0.7735410718917847, "alpha_loss": 6.172619946300983e-05, "alpha_value": 0.06864857812759166, "duration": 70.61368060112, "step": 62750}
{"episode_reward": 967.0, "episode": 252.0, "Q1 loss": 5.214506974220276, "Q2 loss": 5.231062193870544, "Mean Target Q": 332.8367738037109, "Mean Q1": 332.8341024169922, "Mean Q2": 332.8341029052734, "critic_loss": 10.445569171905518, "batch_reward": 2.4118359375, "actor_loss": -333.3770847167969, "actor_target_entropy": -2.0, "actor_entropy": 0.7596640658378601, "alpha_loss": 0.0008937871796078979, "alpha_value": 0.0685499939946589, "duration": 70.44840502738953, "step": 63000}
{"episode_reward": 941.0, "episode": 253.0, "Q1 loss": 5.434137666702271, "Q2 loss": 5.50001470375061, "Mean Target Q": 333.97003576660154, "Mean Q1": 333.967912109375, "Mean Q2": 333.96711328125, "critic_loss": 10.934152376174927, "batch_reward": 2.42453125, "actor_loss": -334.955505859375, "actor_target_entropy": -2.0, "actor_entropy": 0.7290773429870605, "alpha_loss": 0.0015306944344192744, "alpha_value": 0.06846666593951743, "duration": 70.58950090408325, "step": 63250}
{"episode_reward": 982.0, "episode": 254.0, "Q1 loss": 5.246958414077759, "Q2 loss": 5.241479495048523, "Mean Target Q": 334.68807165527346, "Mean Q1": 334.6866959228516, "Mean Q2": 334.6875245361328, "critic_loss": 10.488437870025635, "batch_reward": 2.4287578125, "actor_loss": -335.73113671875, "actor_target_entropy": -2.0, "actor_entropy": 0.7730613703727722, "alpha_loss": 0.0012340751425363122, "alpha_value": 0.06825774078183672, "duration": 70.41230702400208, "step": 63500}
{"episode_reward": 991.0, "episode": 255.0, "Q1 loss": 5.115907007217407, "Q2 loss": 5.174871600151062, "Mean Target Q": 335.16315844726563, "Mean Q1": 335.1638825683594, "Mean Q2": 335.1631795654297, "critic_loss": 10.290778619766236, "batch_reward": 2.43534375, "actor_loss": -336.40148046875, "actor_target_entropy": -2.0, "actor_entropy": 0.7399953384399414, "alpha_loss": 0.00038404518226161597, "alpha_value": 0.06821269606821645, "duration": 70.43065905570984, "step": 63750}
{"episode_reward": 788.0, "episode": 256.0, "Q1 loss": 5.39161623287201, "Q2 loss": 5.3421745777130125, "Mean Target Q": 334.52239660644534, "Mean Q1": 334.51976477050783, "Mean Q2": 334.5211104736328, "critic_loss": 10.733790817260742, "batch_reward": 2.430375, "actor_loss": -334.778671875, "actor_target_entropy": -2.0, "actor_entropy": 0.778503625869751, "alpha_loss": -0.0011116184149868786, "alpha_value": 0.06824212709986119, "duration": 71.3079445362091, "step": 64000}
{"episode_reward": 997.0, "episode": 257.0, "Q1 loss": 5.401513269424439, "Q2 loss": 5.411244687080384, "Mean Target Q": 335.55662060546877, "Mean Q1": 335.55722473144533, "Mean Q2": 335.5565515136719, "critic_loss": 10.812757974624633, "batch_reward": 2.44703125, "actor_loss": -336.45117993164064, "actor_target_entropy": -2.0, "actor_entropy": 0.7507525963783264, "alpha_loss": 0.002359671010170132, "alpha_value": 0.06819801185654772, "duration": 70.6422803401947, "step": 64250}
{"episode_reward": 975.0, "episode": 258.0, "Q1 loss": 5.2840870628356935, "Q2 loss": 5.26872522354126, "Mean Target Q": 336.52668542480467, "Mean Q1": 336.52376696777344, "Mean Q2": 336.52395959472653, "critic_loss": 10.552812274932862, "batch_reward": 2.450875, "actor_loss": -337.5571298828125, "actor_target_entropy": -2.0, "actor_entropy": 0.7056646981239318, "alpha_loss": 0.0021502256579697134, "alpha_value": 0.0678964550897522, "duration": 70.7889256477356, "step": 64500}
{"episode_reward": 1000.0, "episode": 259.0, "Q1 loss": 5.28085055065155, "Q2 loss": 5.2819297428131105, "Mean Target Q": 336.5329935302734, "Mean Q1": 336.53153857421876, "Mean Q2": 336.5319926757812, "critic_loss": 10.562780282974243, "batch_reward": 2.4514609375, "actor_loss": -337.682169921875, "actor_target_entropy": -2.0, "actor_entropy": 0.6734265446662903, "alpha_loss": 0.0009978818548843265, "alpha_value": 0.06767963229929516, "duration": 70.89083218574524, "step": 64750}
{"episode_reward": 990.0, "episode": 260.0, "Q1 loss": 5.264564839363098, "Q2 loss": 5.2651935386657716, "Mean Target Q": 337.7095771484375, "Mean Q1": 337.70510388183595, "Mean Q2": 337.70570422363284, "critic_loss": 10.529758375167846, "batch_reward": 2.4663984375, "actor_loss": -338.65191088867186, "actor_target_entropy": -2.0, "actor_entropy": 0.7337182803153992, "alpha_loss": 0.002372287714853883, "alpha_value": 0.06753393339857194, "step": 65000}
{"duration": 79.04723882675171, "step": 65000}
{"episode_reward": 967.0, "episode": 261.0, "Q1 loss": 5.290797229766846, "Q2 loss": 5.303205994606018, "Mean Target Q": 337.8887325439453, "Mean Q1": 337.8890812988281, "Mean Q2": 337.88835925292966, "critic_loss": 10.594003200531006, "batch_reward": 2.4698671875, "actor_loss": -338.53440869140627, "actor_target_entropy": -2.0, "actor_entropy": 0.7509217920303345, "alpha_loss": 0.004468423537909985, "alpha_value": 0.0672064860530121, "duration": 70.52633833885193, "step": 65250}
{"episode_reward": 974.0, "episode": 262.0, "Q1 loss": 5.422184843063355, "Q2 loss": 5.440457763671875, "Mean Target Q": 338.6277709960938, "Mean Q1": 338.62505541992186, "Mean Q2": 338.62575842285156, "critic_loss": 10.862642589569091, "batch_reward": 2.4714921875, "actor_loss": -339.9326337890625, "actor_target_entropy": -2.0, "actor_entropy": 0.7162585515975952, "alpha_loss": 0.0038771375506184997, "alpha_value": 0.06679384959850285, "duration": 70.58968925476074, "step": 65500}
{"episode_reward": 975.0, "episode": 263.0, "Q1 loss": 5.395138284683227, "Q2 loss": 5.486466520309448, "Mean Target Q": 338.7559323730469, "Mean Q1": 338.7561385498047, "Mean Q2": 338.75599230957033, "critic_loss": 10.881604803085327, "batch_reward": 2.4720703125, "actor_loss": -339.8272436523437, "actor_target_entropy": -2.0, "actor_entropy": 0.670300708770752, "alpha_loss": 0.004921986589208245, "alpha_value": 0.06627898279176607, "duration": 70.42781400680542, "step": 65750}
{"episode_reward": 302.0, "episode": 264.0, "Q1 loss": 5.570388458251953, "Q2 loss": 5.597993850708008, "Mean Target Q": 339.10714685058593, "Mean Q1": 339.10541870117186, "Mean Q2": 339.10380517578125, "critic_loss": 11.168382276535034, "batch_reward": 2.4759296875, "actor_loss": -340.168744140625, "actor_target_entropy": -2.0, "actor_entropy": 0.6502065548896789, "alpha_loss": 0.008248586689122022, "alpha_value": 0.0656658046164703, "duration": 70.99911570549011, "step": 66000}
{"episode_reward": 977.0, "episode": 265.0, "Q1 loss": 5.535553615570068, "Q2 loss": 5.55631236076355, "Mean Target Q": 339.1144573974609, "Mean Q1": 339.1122790527344, "Mean Q2": 339.11458679199217, "critic_loss": 11.091865978240968, "batch_reward": 2.4730625, "actor_loss": -340.07239624023435, "actor_target_entropy": -2.0, "actor_entropy": 0.6805654358863831, "alpha_loss": 0.004728279013186693, "alpha_value": 0.065007097519255, "duration": 70.38864636421204, "step": 66250}
{"episode_reward": 625.0, "episode": 266.0, "Q1 loss": 5.433038437843323, "Q2 loss": 5.355851285934448, "Mean Target Q": 339.7969949951172, "Mean Q1": 339.79559350585936, "Mean Q2": 339.79476049804686, "critic_loss": 10.788889726638795, "batch_reward": 2.4863203125, "actor_loss": -340.599806640625, "actor_target_entropy": -2.0, "actor_entropy": 0.6917270994186402, "alpha_loss": 0.0012221938506700098, "alpha_value": 0.06475895870065457, "duration": 70.55943632125854, "step": 66500}
{"episode_reward": 920.0, "episode": 267.0, "Q1 loss": 5.6540386085510255, "Q2 loss": 5.7054035177230835, "Mean Target Q": 339.97701586914064, "Mean Q1": 339.97591821289063, "Mean Q2": 339.97637609863284, "critic_loss": 11.359442165374755, "batch_reward": 2.476359375, "actor_loss": -340.51707202148435, "actor_target_entropy": -2.0, "actor_entropy": 0.6901666440963745, "alpha_loss": 0.0026464003408327697, "alpha_value": 0.06458906284441895, "duration": 70.58522939682007, "step": 66750}
{"episode_reward": 924.0, "episode": 268.0, "Q1 loss": 5.9460399665832515, "Q2 loss": 5.9292868156433105, "Mean Target Q": 340.238697265625, "Mean Q1": 340.2349661865234, "Mean Q2": 340.2363826904297, "critic_loss": 11.875326791763305, "batch_reward": 2.477359375, "actor_loss": -340.7891735839844, "actor_target_entropy": -2.0, "actor_entropy": 0.6638284749984741, "alpha_loss": 0.0022031651721335947, "alpha_value": 0.0643531995930156, "duration": 70.7165834903717, "step": 67000}
{"episode_reward": 0.0, "episode": 269.0, "Q1 loss": 5.784712610244751, "Q2 loss": 5.866473533630371, "Mean Target Q": 341.0592529296875, "Mean Q1": 341.0609278564453, "Mean Q2": 341.0589443359375, "critic_loss": 11.651186130523682, "batch_reward": 2.476046875, "actor_loss": -342.4194438476562, "actor_target_entropy": -2.0, "actor_entropy": 0.6728503122329712, "alpha_loss": 0.001231876316247508, "alpha_value": 0.0641246448590475, "duration": 70.5808675289154, "step": 67250}
{"episode_reward": 968.0, "episode": 270.0, "Q1 loss": 5.692127326965332, "Q2 loss": 5.646892066001892, "Mean Target Q": 341.9406270751953, "Mean Q1": 341.94001245117187, "Mean Q2": 341.94084436035155, "critic_loss": 11.33901940536499, "batch_reward": 2.48865625, "actor_loss": -343.3331140136719, "actor_target_entropy": -2.0, "actor_entropy": 0.7375068202018737, "alpha_loss": -0.0018026921739801765, "alpha_value": 0.06420038486951145, "duration": 70.60557055473328, "step": 67500}
{"episode_reward": 979.0, "episode": 271.0, "Q1 loss": 5.783129622459412, "Q2 loss": 5.8021198101043705, "Mean Target Q": 341.72266918945314, "Mean Q1": 341.71888757324217, "Mean Q2": 341.71953247070314, "critic_loss": 11.585249460220338, "batch_reward": 2.4857890625, "actor_loss": -343.05409106445313, "actor_target_entropy": -2.0, "actor_entropy": 0.6707208530902863, "alpha_loss": 0.0019705935609526932, "alpha_value": 0.06419509124379137, "duration": 71.18179655075073, "step": 67750}
{"episode_reward": 425.0, "episode": 272.0, "Q1 loss": 5.5495599803924565, "Q2 loss": 5.556630065917969, "Mean Target Q": 342.06981970214844, "Mean Q1": 342.0696309814453, "Mean Q2": 342.0684853515625, "critic_loss": 11.106190090179444, "batch_reward": 2.4780703125, "actor_loss": -343.0118798828125, "actor_target_entropy": -2.0, "actor_entropy": 0.6743335695266723, "alpha_loss": -0.002468782348558307, "alpha_value": 0.0642675688557831, "duration": 70.82473969459534, "step": 68000}
{"episode_reward": 23.0, "episode": 273.0, "Q1 loss": 5.897908236503601, "Q2 loss": 5.876247833251953, "Mean Target Q": 342.5893046875, "Mean Q1": 342.58564611816405, "Mean Q2": 342.58636779785155, "critic_loss": 11.774156059265136, "batch_reward": 2.4812421875, "actor_loss": -343.4615869140625, "actor_target_entropy": -2.0, "actor_entropy": 0.6636850929260254, "alpha_loss": 0.0007925134138204158, "alpha_value": 0.0643299347245614, "duration": 70.70827674865723, "step": 68250}
{"episode_reward": 960.0, "episode": 274.0, "Q1 loss": 5.7046405181884765, "Q2 loss": 5.698186250686645, "Mean Target Q": 342.8534797363281, "Mean Q1": 342.8544090576172, "Mean Q2": 342.8542384033203, "critic_loss": 11.402826808929444, "batch_reward": 2.4770859375, "actor_loss": -343.68332373046877, "actor_target_entropy": -2.0, "actor_entropy": 0.6800881485939025, "alpha_loss": 0.00094442845787853, "alpha_value": 0.06423633155207938, "duration": 70.85233569145203, "step": 68500}
{"episode_reward": 954.0, "episode": 275.0, "Q1 loss": 5.509267763137817, "Q2 loss": 5.604246456146241, "Mean Target Q": 343.0421483154297, "Mean Q1": 343.04100805664064, "Mean Q2": 343.0425910644531, "critic_loss": 11.113514226913452, "batch_reward": 2.4814375, "actor_loss": -343.8679680175781, "actor_target_entropy": -2.0, "actor_entropy": 0.6610141663551331, "alpha_loss": 0.0002565554641187191, "alpha_value": 0.06424134951970673, "duration": 70.92049908638, "step": 68750}
{"episode_reward": 756.0, "episode": 276.0, "Q1 loss": 5.63578722858429, "Q2 loss": 5.669130947113037, "Mean Target Q": 343.6340280761719, "Mean Q1": 343.63202795410155, "Mean Q2": 343.62959094238283, "critic_loss": 11.304918186187745, "batch_reward": 2.490484375, "actor_loss": -344.6267407226562, "actor_target_entropy": -2.0, "actor_entropy": 0.6213056182861328, "alpha_loss": 0.0016380806425586342, "alpha_value": 0.06409115822287369, "duration": 70.83804225921631, "step": 69000}
{"episode_reward": 977.0, "episode": 277.0, "Q1 loss": 5.7526950902938845, "Q2 loss": 5.679144947052002, "Mean Target Q": 343.6506744384766, "Mean Q1": 343.64724865722656, "Mean Q2": 343.6482778320312, "critic_loss": 11.431840007781982, "batch_reward": 2.491140625, "actor_loss": -344.90345629882813, "actor_target_entropy": -2.0, "actor_entropy": 0.6566344494819641, "alpha_loss": -0.000989403121173382, "alpha_value": 0.064022049694148, "duration": 70.75038743019104, "step": 69250}
{"episode_reward": 945.0, "episode": 278.0, "Q1 loss": 5.595285789489746, "Q2 loss": 5.65404185295105, "Mean Target Q": 343.64697326660155, "Mean Q1": 343.6457264404297, "Mean Q2": 343.6464755859375, "critic_loss": 11.249327644348144, "batch_reward": 2.4939453125, "actor_loss": -344.9430947265625, "actor_target_entropy": -2.0, "actor_entropy": 0.6559582748413086, "alpha_loss": -9.322931338101626e-05, "alpha_value": 0.06409139951379753, "duration": 71.32340669631958, "step": 69500}
{"episode_reward": 993.0, "episode": 279.0, "Q1 loss": 5.5812247905731205, "Q2 loss": 5.575881722450256, "Mean Target Q": 344.53106127929686, "Mean Q1": 344.5290906982422, "Mean Q2": 344.528408203125, "critic_loss": 11.15710648727417, "batch_reward": 2.5104921875, "actor_loss": -345.91892016601565, "actor_target_entropy": -2.0, "actor_entropy": 0.6560823078155518, "alpha_loss": 0.0008038525437004864, "alpha_value": 0.06406296314169485, "duration": 70.89049220085144, "step": 69750}
{"episode_reward": 994.0, "episode": 280.0, "Q1 loss": 5.461141537666321, "Q2 loss": 5.4865226678848265, "Mean Target Q": 344.63487963867186, "Mean Q1": 344.63527685546876, "Mean Q2": 344.6357846679688, "critic_loss": 10.947664184570312, "batch_reward": 2.50540625, "actor_loss": -346.26046948242185, "actor_target_entropy": -2.0, "actor_entropy": 0.665089695930481, "alpha_loss": -0.0016672921939752997, "alpha_value": 0.06418477114306183, "step": 70000}
{"duration": 81.63421654701233, "step": 70000}
{"episode_reward": 10.0, "episode": 281.0, "Q1 loss": 5.680258302688599, "Q2 loss": 5.668958882331848, "Mean Target Q": 344.8499344482422, "Mean Q1": 344.8457802734375, "Mean Q2": 344.8463953857422, "critic_loss": 11.34921720123291, "batch_reward": 2.4933046875, "actor_loss": -346.0017248535156, "actor_target_entropy": -2.0, "actor_entropy": 0.663391185760498, "alpha_loss": -0.0021571621713228524, "alpha_value": 0.06429302181693766, "duration": 74.54732251167297, "step": 70250}
{"episode_reward": 880.0, "episode": 282.0, "Q1 loss": 5.814493448257446, "Q2 loss": 5.835304270744324, "Mean Target Q": 345.50840710449216, "Mean Q1": 345.5071638183594, "Mean Q2": 345.50731591796875, "critic_loss": 11.649797708511352, "batch_reward": 2.5022734375, "actor_loss": -346.51069189453125, "actor_target_entropy": -2.0, "actor_entropy": 0.669971444606781, "alpha_loss": 0.00022973442124202848, "alpha_value": 0.06444193013804328, "duration": 74.2452700138092, "step": 70500}
{"episode_reward": 970.0, "episode": 283.0, "Q1 loss": 5.37655196094513, "Q2 loss": 5.409963478088379, "Mean Target Q": 345.8337694091797, "Mean Q1": 345.83365197753903, "Mean Q2": 345.83325415039064, "critic_loss": 10.786515436172486, "batch_reward": 2.496515625, "actor_loss": -346.9651279296875, "actor_target_entropy": -2.0, "actor_entropy": 0.6643684840202332, "alpha_loss": -0.003012845020275563, "alpha_value": 0.06455911966856905, "duration": 74.40845942497253, "step": 70750}
{"episode_reward": 404.0, "episode": 284.0, "Q1 loss": 5.772930736541748, "Q2 loss": 5.739611303329467, "Mean Target Q": 346.42021728515624, "Mean Q1": 346.41649682617185, "Mean Q2": 346.41753588867186, "critic_loss": 11.512542016983032, "batch_reward": 2.492421875, "actor_loss": -347.60315112304687, "actor_target_entropy": -2.0, "actor_entropy": 0.6691413340568543, "alpha_loss": -0.0035631313100457193, "alpha_value": 0.06486993066211907, "duration": 74.40608859062195, "step": 71000}
{"episode_reward": 427.0, "episode": 285.0, "Q1 loss": 5.795079174041748, "Q2 loss": 5.817065665245056, "Mean Target Q": 347.2714353027344, "Mean Q1": 347.26850842285154, "Mean Q2": 347.2682250976562, "critic_loss": 11.612144830703736, "batch_reward": 2.504609375, "actor_loss": -348.36111328125, "actor_target_entropy": -2.0, "actor_entropy": 0.6650718593597412, "alpha_loss": -0.0013527738079428674, "alpha_value": 0.06511867323135846, "duration": 74.4035894870758, "step": 71250}
{"episode_reward": 972.0, "episode": 286.0, "Q1 loss": 5.838539617538452, "Q2 loss": 5.860272389411926, "Mean Target Q": 347.5439046630859, "Mean Q1": 347.54446057128905, "Mean Q2": 347.5435303955078, "critic_loss": 11.698812023162843, "batch_reward": 2.5044296875, "actor_loss": -348.3594384765625, "actor_target_entropy": -2.0, "actor_entropy": 0.6784946279525756, "alpha_loss": -0.0038846586965955794, "alpha_value": 0.06546107827853341, "duration": 75.18246579170227, "step": 71500}
{"episode_reward": 959.0, "episode": 287.0, "Q1 loss": 5.409243185043335, "Q2 loss": 5.412945129394531, "Mean Target Q": 347.96243188476564, "Mean Q1": 347.96192468261717, "Mean Q2": 347.9627009277344, "critic_loss": 10.822188301086426, "batch_reward": 2.5115546875, "actor_loss": -348.8056875, "actor_target_entropy": -2.0, "actor_entropy": 0.6744602637290955, "alpha_loss": -0.0008409244865179062, "alpha_value": 0.06563901433570701, "duration": 72.29860258102417, "step": 71750}
{"episode_reward": 911.0, "episode": 288.0, "Q1 loss": 5.540021533966065, "Q2 loss": 5.540194110870361, "Mean Target Q": 349.05346240234377, "Mean Q1": 349.05033068847655, "Mean Q2": 349.0503981933594, "critic_loss": 11.080215618133545, "batch_reward": 2.511828125, "actor_loss": -349.99143701171874, "actor_target_entropy": -2.0, "actor_entropy": 0.7036324653625489, "alpha_loss": -0.00020068513322621585, "alpha_value": 0.06561487097062528, "duration": 70.97936415672302, "step": 72000}
{"episode_reward": 964.0, "episode": 289.0, "Q1 loss": 5.711836933135986, "Q2 loss": 5.703488305091858, "Mean Target Q": 348.94220166015623, "Mean Q1": 348.9428078613281, "Mean Q2": 348.9425994873047, "critic_loss": 11.415325239181518, "batch_reward": 2.5187109375, "actor_loss": -349.6403701171875, "actor_target_entropy": -2.0, "actor_entropy": 0.7137903056144714, "alpha_loss": -0.0012208409341983497, "alpha_value": 0.06576520263791784, "duration": 70.85845422744751, "step": 72250}
{"episode_reward": 928.0, "episode": 290.0, "Q1 loss": 5.479276330947876, "Q2 loss": 5.5458956689834595, "Mean Target Q": 350.01845727539063, "Mean Q1": 350.0161499023437, "Mean Q2": 350.0162652587891, "critic_loss": 11.025172008514405, "batch_reward": 2.5251953125, "actor_loss": -351.2146628417969, "actor_target_entropy": -2.0, "actor_entropy": 0.7069981851577759, "alpha_loss": -0.0020411631944589315, "alpha_value": 0.06601908031126988, "duration": 70.70660829544067, "step": 72500}
{"episode_reward": 864.0, "episode": 291.0, "Q1 loss": 5.691795245170593, "Q2 loss": 5.720421718597412, "Mean Target Q": 350.16434033203126, "Mean Q1": 350.16355871582033, "Mean Q2": 350.1636895751953, "critic_loss": 11.412216995239257, "batch_reward": 2.526625, "actor_loss": -351.57451171875, "actor_target_entropy": -2.0, "actor_entropy": 0.6458520493507385, "alpha_loss": 0.001033513587899506, "alpha_value": 0.0660577308650972, "duration": 71.31204533576965, "step": 72750}
{"episode_reward": 969.0, "episode": 292.0, "Q1 loss": 5.481592269897461, "Q2 loss": 5.423380061149597, "Mean Target Q": 350.88409399414064, "Mean Q1": 350.88128149414064, "Mean Q2": 350.88239453125, "critic_loss": 10.904972316741944, "batch_reward": 2.535546875, "actor_loss": -351.9820400390625, "actor_target_entropy": -2.0, "actor_entropy": 0.6686477980613709, "alpha_loss": -0.0006003846880048514, "alpha_value": 0.06598994383146406, "duration": 74.48604726791382, "step": 73000}
{"episode_reward": 818.0, "episode": 293.0, "Q1 loss": 5.465046966552735, "Q2 loss": 5.493293528556824, "Mean Target Q": 350.92497705078125, "Mean Q1": 350.92482629394533, "Mean Q2": 350.92371850585937, "critic_loss": 10.958340517044068, "batch_reward": 2.5310625, "actor_loss": -351.9277868652344, "actor_target_entropy": -2.0, "actor_entropy": 0.661311589717865, "alpha_loss": -0.0008604380367323756, "alpha_value": 0.06604656740489795, "duration": 73.65654301643372, "step": 73250}
{"episode_reward": 689.0, "episode": 294.0, "Q1 loss": 5.555063367843628, "Q2 loss": 5.50073166847229, "Mean Target Q": 350.9543494873047, "Mean Q1": 350.9538522949219, "Mean Q2": 350.9543620605469, "critic_loss": 11.055795045852662, "batch_reward": 2.538859375, "actor_loss": -352.4263596191406, "actor_target_entropy": -2.0, "actor_entropy": 0.6749987664222717, "alpha_loss": 0.0005792406960390508, "alpha_value": 0.06607653286879649, "duration": 73.9552800655365, "step": 73500}
{"episode_reward": 939.0, "episode": 295.0, "Q1 loss": 5.546895588874817, "Q2 loss": 5.58588440322876, "Mean Target Q": 351.16204809570314, "Mean Q1": 351.1575538330078, "Mean Q2": 351.1591574707031, "critic_loss": 11.132780000686646, "batch_reward": 2.534265625, "actor_loss": -351.9997924804687, "actor_target_entropy": -2.0, "actor_entropy": 0.6570609536170959, "alpha_loss": -7.67428851686418e-05, "alpha_value": 0.06612250108456993, "duration": 73.94348692893982, "step": 73750}
{"episode_reward": 924.0, "episode": 296.0, "Q1 loss": 5.4001776247024535, "Q2 loss": 5.354657279014587, "Mean Target Q": 351.9865944824219, "Mean Q1": 351.98501611328123, "Mean Q2": 351.9836418457031, "critic_loss": 10.754834896087646, "batch_reward": 2.539703125, "actor_loss": -353.11133544921876, "actor_target_entropy": -2.0, "actor_entropy": 0.6639365837574005, "alpha_loss": 0.0035303273098543286, "alpha_value": 0.0658498391653566, "duration": 74.05733275413513, "step": 74000}
{"episode_reward": 515.0, "episode": 297.0, "Q1 loss": 5.49765154838562, "Q2 loss": 5.478506004333496, "Mean Target Q": 352.14325207519533, "Mean Q1": 352.1427856445313, "Mean Q2": 352.1434813232422, "critic_loss": 10.976157569885254, "batch_reward": 2.5319765625, "actor_loss": -352.88245849609376, "actor_target_entropy": -2.0, "actor_entropy": 0.685714255809784, "alpha_loss": 0.0007376618674024939, "alpha_value": 0.06563640489099959, "duration": 74.20911860466003, "step": 74250}
{"episode_reward": 921.0, "episode": 298.0, "Q1 loss": 5.461068069458007, "Q2 loss": 5.480316876411438, "Mean Target Q": 352.7727186279297, "Mean Q1": 352.7709738769531, "Mean Q2": 352.77053967285156, "critic_loss": 10.941384981155396, "batch_reward": 2.550734375, "actor_loss": -354.0076257324219, "actor_target_entropy": -2.0, "actor_entropy": 0.7159151244163513, "alpha_loss": 0.000101698761805892, "alpha_value": 0.06558086181740769, "duration": 85.90263080596924, "step": 74500}
{"episode_reward": 999.0, "episode": 299.0, "Q1 loss": 5.289902255058289, "Q2 loss": 5.251538834571838, "Mean Target Q": 353.5243977050781, "Mean Q1": 353.52245654296877, "Mean Q2": 353.52243139648436, "critic_loss": 10.541441082000732, "batch_reward": 2.5526796875, "actor_loss": -354.81319921875, "actor_target_entropy": -2.0, "actor_entropy": 0.6745277881622315, "alpha_loss": -0.001458718872629106, "alpha_value": 0.06561537842634689, "duration": 73.93460488319397, "step": 74750}
{"episode_reward": 977.0, "episode": 300.0, "Q1 loss": 5.277492998123169, "Q2 loss": 5.308127143859863, "Mean Target Q": 353.7921997070313, "Mean Q1": 353.7917844238281, "Mean Q2": 353.79207006835935, "critic_loss": 10.58562010383606, "batch_reward": 2.5599921875, "actor_loss": -354.92126611328126, "actor_target_entropy": -2.0, "actor_entropy": 0.6937969608306884, "alpha_loss": -0.0019132910030893982, "alpha_value": 0.0658370268732814, "step": 75000}
{"duration": 83.41215586662292, "step": 75000}
{"episode_reward": 964.0, "episode": 301.0, "Q1 loss": 5.297658739089965, "Q2 loss": 5.321352276802063, "Mean Target Q": 353.97614526367187, "Mean Q1": 353.9732299804688, "Mean Q2": 353.9741791992187, "critic_loss": 10.6190110206604, "batch_reward": 2.5507265625, "actor_loss": -355.14963916015626, "actor_target_entropy": -2.0, "actor_entropy": 0.6596180915832519, "alpha_loss": 5.256808595731854e-05, "alpha_value": 0.06594119693630164, "duration": 74.13254451751709, "step": 75250}
{"episode_reward": 6.0, "episode": 302.0, "Q1 loss": 5.36240664100647, "Q2 loss": 5.353106119155884, "Mean Target Q": 354.2548303222656, "Mean Q1": 354.25366760253905, "Mean Q2": 354.2531611328125, "critic_loss": 10.715512735366822, "batch_reward": 2.54678125, "actor_loss": -355.052595703125, "actor_target_entropy": -2.0, "actor_entropy": 0.6803008079528808, "alpha_loss": -0.00023060240037739278, "alpha_value": 0.06595302904143728, "duration": 74.07148671150208, "step": 75500}
{"episode_reward": 0.0, "episode": 303.0, "Q1 loss": 5.4163810138702395, "Q2 loss": 5.351988349914551, "Mean Target Q": 354.68988806152345, "Mean Q1": 354.689728515625, "Mean Q2": 354.68983557128905, "critic_loss": 10.768369342803956, "batch_reward": 2.5473359375, "actor_loss": -355.5867395019531, "actor_target_entropy": -2.0, "actor_entropy": 0.6870913701057434, "alpha_loss": 0.0011624619932845235, "alpha_value": 0.06598269321227573, "duration": 74.19844579696655, "step": 75750}
{"episode_reward": 994.0, "episode": 304.0, "Q1 loss": 5.519622379302978, "Q2 loss": 5.5433244800567625, "Mean Target Q": 355.03963049316405, "Mean Q1": 355.0367025146484, "Mean Q2": 355.0377115478516, "critic_loss": 11.062946868896484, "batch_reward": 2.5452734375, "actor_loss": -356.0904279785156, "actor_target_entropy": -2.0, "actor_entropy": 0.6816238503456116, "alpha_loss": 0.001027327895630151, "alpha_value": 0.06579957432679485, "duration": 74.09016060829163, "step": 76000}
{"episode_reward": 930.0, "episode": 305.0, "Q1 loss": 5.212688051223755, "Q2 loss": 5.228566912651062, "Mean Target Q": 355.6271749267578, "Mean Q1": 355.6258347167969, "Mean Q2": 355.62504626464846, "critic_loss": 10.441254928588867, "batch_reward": 2.5526875, "actor_loss": -356.9324047851562, "actor_target_entropy": -2.0, "actor_entropy": 0.6794035487174988, "alpha_loss": 0.0024835701817646624, "alpha_value": 0.06565477560697633, "duration": 74.59014105796814, "step": 76250}
{"episode_reward": 994.0, "episode": 306.0, "Q1 loss": 5.692058514595032, "Q2 loss": 5.684105569839478, "Mean Target Q": 355.97211840820313, "Mean Q1": 355.97213415527347, "Mean Q2": 355.97131616210936, "critic_loss": 11.3761640625, "batch_reward": 2.549765625, "actor_loss": -357.33159790039065, "actor_target_entropy": -2.0, "actor_entropy": 0.6683568592071534, "alpha_loss": 0.0014073596154339611, "alpha_value": 0.06536739916301672, "duration": 74.31023788452148, "step": 76500}
{"episode_reward": 550.0, "episode": 307.0, "Q1 loss": 5.69292274093628, "Q2 loss": 5.671015130996704, "Mean Target Q": 355.78128369140626, "Mean Q1": 355.7782393798828, "Mean Q2": 355.7794813232422, "critic_loss": 11.363937866210938, "batch_reward": 2.550875, "actor_loss": -356.86209326171877, "actor_target_entropy": -2.0, "actor_entropy": 0.6836207809448243, "alpha_loss": 0.0004085237551480532, "alpha_value": 0.06530736360290589, "duration": 74.35544490814209, "step": 76750}
{"episode_reward": 979.0, "episode": 308.0, "Q1 loss": 5.6386086502075194, "Q2 loss": 5.639860018730164, "Mean Target Q": 355.7520919189453, "Mean Q1": 355.75173217773437, "Mean Q2": 355.7512978515625, "critic_loss": 11.278468685150147, "batch_reward": 2.556640625, "actor_loss": -356.83169482421874, "actor_target_entropy": -2.0, "actor_entropy": 0.6758585233688355, "alpha_loss": -0.00024677623761817816, "alpha_value": 0.06530337753093993, "duration": 74.2277159690857, "step": 77000}
{"episode_reward": 0.0, "episode": 309.0, "Q1 loss": 5.315534767150879, "Q2 loss": 5.373438661575317, "Mean Target Q": 355.5093602294922, "Mean Q1": 355.50515649414064, "Mean Q2": 355.50441076660155, "critic_loss": 10.688973455429077, "batch_reward": 2.550078125, "actor_loss": -356.3872751464844, "actor_target_entropy": -2.0, "actor_entropy": 0.688127977848053, "alpha_loss": -0.0003645077971741557, "alpha_value": 0.06535416013035594, "duration": 74.72524333000183, "step": 77250}
{"episode_reward": 1000.0, "episode": 310.0, "Q1 loss": 5.404238140106201, "Q2 loss": 5.413278617858887, "Mean Target Q": 356.18582495117187, "Mean Q1": 356.18759301757814, "Mean Q2": 356.18944921875, "critic_loss": 10.817516733169557, "batch_reward": 2.561015625, "actor_loss": -356.9473876953125, "actor_target_entropy": -2.0, "actor_entropy": 0.7016638798713684, "alpha_loss": 0.001035929919220507, "alpha_value": 0.06526323534605076, "duration": 92.93436884880066, "step": 77500}
{"episode_reward": 957.0, "episode": 311.0, "Q1 loss": 5.436603609085083, "Q2 loss": 5.412680327415466, "Mean Target Q": 356.22594152832033, "Mean Q1": 356.22268395996093, "Mean Q2": 356.2213839111328, "critic_loss": 10.849283910751343, "batch_reward": 2.56396875, "actor_loss": -357.1968798828125, "actor_target_entropy": -2.0, "actor_entropy": 0.6710698218345642, "alpha_loss": -0.00019170246459543705, "alpha_value": 0.06533455515825606, "duration": 74.36176657676697, "step": 77750}
{"episode_reward": 962.0, "episode": 312.0, "Q1 loss": 5.53790085697174, "Q2 loss": 5.565453576087951, "Mean Target Q": 356.9414688720703, "Mean Q1": 356.9420941162109, "Mean Q2": 356.94229650878907, "critic_loss": 11.103354448318482, "batch_reward": 2.570828125, "actor_loss": -358.078666015625, "actor_target_entropy": -2.0, "actor_entropy": 0.7006850781440734, "alpha_loss": 0.0001313583077862859, "alpha_value": 0.06525064009986391, "duration": 78.88131737709045, "step": 78000}
{"episode_reward": 997.0, "episode": 313.0, "Q1 loss": 5.554366740226746, "Q2 loss": 5.541213214874268, "Mean Target Q": 356.60843103027344, "Mean Q1": 356.60625231933597, "Mean Q2": 356.6076951904297, "critic_loss": 11.09557999610901, "batch_reward": 2.561546875, "actor_loss": -357.7374912109375, "actor_target_entropy": -2.0, "actor_entropy": 0.7045729250907898, "alpha_loss": 0.00014582465961575507, "alpha_value": 0.06521164908534029, "duration": 74.19444155693054, "step": 78250}
{"episode_reward": 983.0, "episode": 314.0, "Q1 loss": 5.42808931350708, "Q2 loss": 5.442857602119446, "Mean Target Q": 357.41028051757814, "Mean Q1": 357.41302087402346, "Mean Q2": 357.4116502685547, "critic_loss": 10.870946941375733, "batch_reward": 2.5697421875, "actor_loss": -358.5801123046875, "actor_target_entropy": -2.0, "actor_entropy": 0.7347245016098023, "alpha_loss": -0.0014718399834819137, "alpha_value": 0.06530887825142094, "duration": 74.58193731307983, "step": 78500}
{"episode_reward": 1000.0, "episode": 315.0, "Q1 loss": 5.668942698478698, "Q2 loss": 5.677038143157959, "Mean Target Q": 357.6745383300781, "Mean Q1": 357.66855822753905, "Mean Q2": 357.6699736328125, "critic_loss": 11.345980800628663, "batch_reward": 2.580765625, "actor_loss": -358.73884106445314, "actor_target_entropy": -2.0, "actor_entropy": 0.7410993576049805, "alpha_loss": 0.0010485308161005378, "alpha_value": 0.06534619341455873, "duration": 75.37189078330994, "step": 78750}
{"episode_reward": 0.0, "episode": 316.0, "Q1 loss": 5.4017288885116574, "Q2 loss": 5.350878598213196, "Mean Target Q": 357.62985498046874, "Mean Q1": 357.6280489501953, "Mean Q2": 357.6276180419922, "critic_loss": 10.752607471466064, "batch_reward": 2.573515625, "actor_loss": -358.74487646484374, "actor_target_entropy": -2.0, "actor_entropy": 0.6962756948471069, "alpha_loss": 0.0001140145855024457, "alpha_value": 0.06528207585105263, "duration": 74.43107986450195, "step": 79000}
{"episode_reward": 968.0, "episode": 317.0, "Q1 loss": 5.378938625335693, "Q2 loss": 5.41925898361206, "Mean Target Q": 357.6843161621094, "Mean Q1": 357.6849764404297, "Mean Q2": 357.6850115966797, "critic_loss": 10.79819758605957, "batch_reward": 2.5717734375, "actor_loss": -358.82007153320313, "actor_target_entropy": -2.0, "actor_entropy": 0.748281512260437, "alpha_loss": -0.0017098801229149102, "alpha_value": 0.06535940727339744, "duration": 74.29739379882812, "step": 79250}
{"episode_reward": 939.0, "episode": 318.0, "Q1 loss": 5.169172250747681, "Q2 loss": 5.168262010574341, "Mean Target Q": 357.9592604980469, "Mean Q1": 357.9581009521484, "Mean Q2": 357.95586975097655, "critic_loss": 10.337434228897095, "batch_reward": 2.5771875, "actor_loss": -359.02288671875, "actor_target_entropy": -2.0, "actor_entropy": 0.7055590834617614, "alpha_loss": 0.0016142339273355901, "alpha_value": 0.06537872229706766, "duration": 84.63915586471558, "step": 79500}
{"episode_reward": 984.0, "episode": 319.0, "Q1 loss": 5.3384196434021, "Q2 loss": 5.308719964981079, "Mean Target Q": 358.0081697998047, "Mean Q1": 358.0092243652344, "Mean Q2": 358.01017053222654, "critic_loss": 10.647139574050904, "batch_reward": 2.583265625, "actor_loss": -359.22407763671873, "actor_target_entropy": -2.0, "actor_entropy": 0.7844432005882264, "alpha_loss": -0.0015831884522922337, "alpha_value": 0.06536535229630519, "duration": 74.54410314559937, "step": 79750}
{"episode_reward": 921.0, "episode": 320.0, "Q1 loss": 5.4804543008804325, "Q2 loss": 5.473303430557251, "Mean Target Q": 358.3642509765625, "Mean Q1": 358.3622736816406, "Mean Q2": 358.36332006835937, "critic_loss": 10.953757726669311, "batch_reward": 2.5824765625, "actor_loss": -359.4821040039063, "actor_target_entropy": -2.0, "actor_entropy": 0.7668265957832336, "alpha_loss": 0.00081674710707739, "alpha_value": 0.06539785797874487, "step": 80000}
{"duration": 83.39112710952759, "step": 80000}
{"episode_reward": 937.0, "episode": 321.0, "Q1 loss": 5.4676028909683225, "Q2 loss": 5.474409065246582, "Mean Target Q": 358.6619649658203, "Mean Q1": 358.65835290527343, "Mean Q2": 358.657990234375, "critic_loss": 10.942011968612672, "batch_reward": 2.5921953125, "actor_loss": -359.51214086914064, "actor_target_entropy": -2.0, "actor_entropy": 0.7376226148605347, "alpha_loss": 0.0018105721101164818, "alpha_value": 0.06520022224224717, "duration": 74.60678434371948, "step": 80250}
{"episode_reward": 964.0, "episode": 322.0, "Q1 loss": 5.258174830436706, "Q2 loss": 5.224389741897583, "Mean Target Q": 358.40909997558595, "Mean Q1": 358.4078684082031, "Mean Q2": 358.40829382324216, "critic_loss": 10.482564567565918, "batch_reward": 2.5974140625, "actor_loss": -359.2826145019531, "actor_target_entropy": -2.0, "actor_entropy": 0.6918575367927551, "alpha_loss": 0.0019353329916484655, "alpha_value": 0.06506672990116956, "duration": 74.50013589859009, "step": 80500}
{"episode_reward": 958.0, "episode": 323.0, "Q1 loss": 5.320790654182434, "Q2 loss": 5.271361834526062, "Mean Target Q": 358.79387634277344, "Mean Q1": 358.7937364501953, "Mean Q2": 358.793873046875, "critic_loss": 10.592152502059937, "batch_reward": 2.6010703125, "actor_loss": -359.7484338378906, "actor_target_entropy": -2.0, "actor_entropy": 0.6768993601799012, "alpha_loss": 0.001747316351160407, "alpha_value": 0.06483679622027237, "duration": 74.52684187889099, "step": 80750}
{"episode_reward": 972.0, "episode": 324.0, "Q1 loss": 5.200921381950378, "Q2 loss": 5.232571042060852, "Mean Target Q": 359.1134553222656, "Mean Q1": 359.1100045166016, "Mean Q2": 359.1099493408203, "critic_loss": 10.43349237060547, "batch_reward": 2.59759375, "actor_loss": -359.9904323730469, "actor_target_entropy": -2.0, "actor_entropy": 0.6902586908340455, "alpha_loss": 0.0007091021528467536, "alpha_value": 0.06476006583080483, "duration": 74.47812676429749, "step": 81000}
{"episode_reward": 975.0, "episode": 325.0, "Q1 loss": 5.213182881355285, "Q2 loss": 5.245369239807129, "Mean Target Q": 359.1512084960938, "Mean Q1": 359.1515611572266, "Mean Q2": 359.15122399902344, "critic_loss": 10.458552095413207, "batch_reward": 2.5958125, "actor_loss": -360.3152185058594, "actor_target_entropy": -2.0, "actor_entropy": 0.7316631002426147, "alpha_loss": 0.0023984567117877305, "alpha_value": 0.06457981625808264, "duration": 74.51134371757507, "step": 81250}
{"episode_reward": 984.0, "episode": 326.0, "Q1 loss": 5.132839365005493, "Q2 loss": 5.035485369682312, "Mean Target Q": 359.4221611328125, "Mean Q1": 359.42033581542967, "Mean Q2": 359.4211359863281, "critic_loss": 10.16832473564148, "batch_reward": 2.606796875, "actor_loss": -360.3161669921875, "actor_target_entropy": -2.0, "actor_entropy": 0.7006644978523254, "alpha_loss": 0.0025542451683431863, "alpha_value": 0.0643153551938859, "duration": 74.50076007843018, "step": 81500}
{"episode_reward": 964.0, "episode": 327.0, "Q1 loss": 4.971207045555115, "Q2 loss": 5.0030690813064576, "Mean Target Q": 359.39224353027345, "Mean Q1": 359.39159020996095, "Mean Q2": 359.39196142578123, "critic_loss": 9.974276138305664, "batch_reward": 2.6078125, "actor_loss": -360.31645190429685, "actor_target_entropy": -2.0, "actor_entropy": 0.7334305715560913, "alpha_loss": 0.003260397857055068, "alpha_value": 0.06398146719540358, "duration": 75.15529274940491, "step": 81750}
{"episode_reward": 50.0, "episode": 328.0, "Q1 loss": 5.127089755058289, "Q2 loss": 5.091791152000427, "Mean Target Q": 359.0430418701172, "Mean Q1": 359.04364575195314, "Mean Q2": 359.04146667480467, "critic_loss": 10.218880928039551, "batch_reward": 2.5978515625, "actor_loss": -360.27540869140626, "actor_target_entropy": -2.0, "actor_entropy": 0.7348610482215882, "alpha_loss": 0.003198405534029007, "alpha_value": 0.06374517949960301, "duration": 74.56101393699646, "step": 82000}
{"episode_reward": 966.0, "episode": 329.0, "Q1 loss": 5.015208541870117, "Q2 loss": 4.976023316383362, "Mean Target Q": 359.54141577148437, "Mean Q1": 359.539884765625, "Mean Q2": 359.54160900878907, "critic_loss": 9.991231826782226, "batch_reward": 2.6133828125, "actor_loss": -360.78332861328124, "actor_target_entropy": -2.0, "actor_entropy": 0.7164943656921386, "alpha_loss": 0.003518069668672979, "alpha_value": 0.0633272526827314, "duration": 75.96540427207947, "step": 82250}
{"episode_reward": 961.0, "episode": 330.0, "Q1 loss": 4.891186700820922, "Q2 loss": 4.92783910369873, "Mean Target Q": 359.9547572021484, "Mean Q1": 359.95123217773437, "Mean Q2": 359.95137109375, "critic_loss": 9.819025815963744, "batch_reward": 2.6160234375, "actor_loss": -360.46231372070315, "actor_target_entropy": -2.0, "actor_entropy": 0.7180015306472778, "alpha_loss": 0.0033524446710944174, "alpha_value": 0.062985379730566, "duration": 74.65623116493225, "step": 82500}
{"episode_reward": 969.0, "episode": 331.0, "Q1 loss": 5.11276096534729, "Q2 loss": 5.081873147010803, "Mean Target Q": 359.4978048095703, "Mean Q1": 359.49669323730467, "Mean Q2": 359.4969241943359, "critic_loss": 10.19463410949707, "batch_reward": 2.611890625, "actor_loss": -360.03646875, "actor_target_entropy": -2.0, "actor_entropy": 0.6940996146202087, "alpha_loss": 0.0008910233131609858, "alpha_value": 0.06282685048490565, "duration": 75.6124529838562, "step": 82750}
{"episode_reward": 114.0, "episode": 332.0, "Q1 loss": 5.087310343742371, "Q2 loss": 5.095939574241638, "Mean Target Q": 359.1510252685547, "Mean Q1": 359.1509014892578, "Mean Q2": 359.1511416015625, "critic_loss": 10.183249923706054, "batch_reward": 2.612859375, "actor_loss": -360.1143703613281, "actor_target_entropy": -2.0, "actor_entropy": 0.7077716498374939, "alpha_loss": 0.0015039016003720463, "alpha_value": 0.06264196990982786, "duration": 74.63574981689453, "step": 83000}
{"episode_reward": 925.0, "episode": 333.0, "Q1 loss": 5.171996332168579, "Q2 loss": 5.087163241386413, "Mean Target Q": 358.8077883300781, "Mean Q1": 358.805982421875, "Mean Q2": 358.8062137451172, "critic_loss": 10.259159606933594, "batch_reward": 2.61171875, "actor_loss": -359.83414599609375, "actor_target_entropy": -2.0, "actor_entropy": 0.6891684455871582, "alpha_loss": 0.0015201834943145514, "alpha_value": 0.06247174828362677, "duration": 81.98372006416321, "step": 83250}
{"episode_reward": 972.0, "episode": 334.0, "Q1 loss": 4.797186116218567, "Q2 loss": 4.745628914833069, "Mean Target Q": 359.47081201171875, "Mean Q1": 359.47111291503904, "Mean Q2": 359.47069885253904, "critic_loss": 9.542815053939819, "batch_reward": 2.61203125, "actor_loss": -360.4782653808594, "actor_target_entropy": -2.0, "actor_entropy": 0.6677114777565002, "alpha_loss": 0.00721919879456982, "alpha_value": 0.062065796525049836, "duration": 74.5608901977539, "step": 83500}
{"episode_reward": 969.0, "episode": 335.0, "Q1 loss": 4.936826928138733, "Q2 loss": 4.9297167558670045, "Mean Target Q": 359.9743944091797, "Mean Q1": 359.9725484619141, "Mean Q2": 359.9722606201172, "critic_loss": 9.866543684005737, "batch_reward": 2.6226328125, "actor_loss": -360.91589013671876, "actor_target_entropy": -2.0, "actor_entropy": 0.683790813446045, "alpha_loss": 0.005252075030468404, "alpha_value": 0.06145887945790892, "duration": 74.56587934494019, "step": 83750}
{"episode_reward": 998.0, "episode": 336.0, "Q1 loss": 5.005263973236084, "Q2 loss": 4.976778804779053, "Mean Target Q": 359.77453393554686, "Mean Q1": 359.77615979003906, "Mean Q2": 359.77527429199216, "critic_loss": 9.982042770385743, "batch_reward": 2.61365625, "actor_loss": -360.6648327636719, "actor_target_entropy": -2.0, "actor_entropy": 0.710262885093689, "alpha_loss": 0.0029497550511732697, "alpha_value": 0.06102592167848788, "duration": 74.616619348526, "step": 84000}
{"episode_reward": 951.0, "episode": 337.0, "Q1 loss": 4.906194877624512, "Q2 loss": 4.910566341400147, "Mean Target Q": 360.62238647460936, "Mean Q1": 360.6186424560547, "Mean Q2": 360.61891271972655, "critic_loss": 9.816761241912841, "batch_reward": 2.625890625, "actor_loss": -361.7818132324219, "actor_target_entropy": -2.0, "actor_entropy": 0.6710625514984131, "alpha_loss": 0.002154031065758318, "alpha_value": 0.0607815643896718, "duration": 74.69091892242432, "step": 84250}
{"episode_reward": 962.0, "episode": 338.0, "Q1 loss": 4.96631600856781, "Q2 loss": 4.93511837387085, "Mean Target Q": 360.88082373046876, "Mean Q1": 360.88023559570314, "Mean Q2": 360.88151184082034, "critic_loss": 9.901434406280517, "batch_reward": 2.631109375, "actor_loss": -361.87619067382815, "actor_target_entropy": -2.0, "actor_entropy": 0.7159757905006409, "alpha_loss": 0.0018071000995114445, "alpha_value": 0.06055940305974924, "duration": 74.5989282131195, "step": 84500}
{"episode_reward": 991.0, "episode": 339.0, "Q1 loss": 4.967128024101258, "Q2 loss": 4.97775502204895, "Mean Target Q": 360.67224548339846, "Mean Q1": 360.66995532226565, "Mean Q2": 360.66912609863283, "critic_loss": 9.944883045196534, "batch_reward": 2.6212109375, "actor_loss": -361.4115568847656, "actor_target_entropy": -2.0, "actor_entropy": 0.7066091456413269, "alpha_loss": 0.001451237892266363, "alpha_value": 0.060486401908502206, "duration": 74.60912275314331, "step": 84750}
{"episode_reward": 419.0, "episode": 340.0, "Q1 loss": 4.693213232994079, "Q2 loss": 4.678674219131469, "Mean Target Q": 361.01915869140623, "Mean Q1": 361.0195614013672, "Mean Q2": 361.0203232421875, "critic_loss": 9.371887454986572, "batch_reward": 2.642125, "actor_loss": -362.18941259765626, "actor_target_entropy": -2.0, "actor_entropy": 0.7295518860816955, "alpha_loss": -0.0005285506909713149, "alpha_value": 0.06041826377972847, "step": 85000}
{"duration": 83.33113884925842, "step": 85000}
{"episode_reward": 979.0, "episode": 341.0, "Q1 loss": 4.8068901662826535, "Q2 loss": 4.753224516868591, "Mean Target Q": 361.1149108886719, "Mean Q1": 361.1150185546875, "Mean Q2": 361.1135725097656, "critic_loss": 9.560114631652832, "batch_reward": 2.63540625, "actor_loss": -361.89791357421876, "actor_target_entropy": -2.0, "actor_entropy": 0.6860789523124695, "alpha_loss": 2.4716004263609646e-05, "alpha_value": 0.06039233435812562, "duration": 74.9297194480896, "step": 85250}
{"episode_reward": 718.0, "episode": 342.0, "Q1 loss": 4.791748187065124, "Q2 loss": 4.739796339035034, "Mean Target Q": 361.4800108642578, "Mean Q1": 361.4782763671875, "Mean Q2": 361.47901953125, "critic_loss": 9.531544532775879, "batch_reward": 2.643, "actor_loss": -362.4649030761719, "actor_target_entropy": -2.0, "actor_entropy": 0.7126042742729187, "alpha_loss": -0.0014117260975763202, "alpha_value": 0.06049487347833486, "duration": 74.58420991897583, "step": 85500}
{"episode_reward": 957.0, "episode": 343.0, "Q1 loss": 4.884110144615173, "Q2 loss": 4.849366816520691, "Mean Target Q": 361.21721728515627, "Mean Q1": 361.21490454101564, "Mean Q2": 361.21478198242187, "critic_loss": 9.733476936340333, "batch_reward": 2.6398203125, "actor_loss": -362.25675268554687, "actor_target_entropy": -2.0, "actor_entropy": 0.698021231174469, "alpha_loss": -0.0015842803372070192, "alpha_value": 0.06070896524086838, "duration": 76.38978362083435, "step": 85750}
{"episode_reward": 966.0, "episode": 344.0, "Q1 loss": 4.743141936302185, "Q2 loss": 4.680461353302002, "Mean Target Q": 361.66256213378904, "Mean Q1": 361.66199377441404, "Mean Q2": 361.66170666503905, "critic_loss": 9.42360329246521, "batch_reward": 2.6354375, "actor_loss": -362.75050268554685, "actor_target_entropy": -2.0, "actor_entropy": 0.6853646984100342, "alpha_loss": -0.0014809068613685667, "alpha_value": 0.06081646857318683, "duration": 74.78975677490234, "step": 86000}
{"episode_reward": 999.0, "episode": 345.0, "Q1 loss": 4.776255310058594, "Q2 loss": 4.784308877944946, "Mean Target Q": 361.79956225585937, "Mean Q1": 361.7997576904297, "Mean Q2": 361.7996022949219, "critic_loss": 9.560564165115357, "batch_reward": 2.65275, "actor_loss": -363.148115234375, "actor_target_entropy": -2.0, "actor_entropy": 0.7043808598518372, "alpha_loss": 0.0008594882837496698, "alpha_value": 0.060864263249696425, "duration": 100.89156794548035, "step": 86250}
{"episode_reward": 955.0, "episode": 346.0, "Q1 loss": 4.697139992713928, "Q2 loss": 4.69601187133789, "Mean Target Q": 361.63439794921874, "Mean Q1": 361.63194970703125, "Mean Q2": 361.6317299804688, "critic_loss": 9.393151861190796, "batch_reward": 2.6513125, "actor_loss": -362.4455988769531, "actor_target_entropy": -2.0, "actor_entropy": 0.6753727807998657, "alpha_loss": 0.0020281912535429, "alpha_value": 0.06065913035067383, "duration": 114.83472871780396, "step": 86500}
{"episode_reward": 983.0, "episode": 347.0, "Q1 loss": 4.8245416383743285, "Q2 loss": 4.7634782209396365, "Mean Target Q": 361.10569775390627, "Mean Q1": 361.1046875, "Mean Q2": 361.10442553710936, "critic_loss": 9.588019882202149, "batch_reward": 2.643796875, "actor_loss": -362.6033356933594, "actor_target_entropy": -2.0, "actor_entropy": 0.738420608997345, "alpha_loss": -0.0010124472989700734, "alpha_value": 0.06062476062715075, "duration": 110.68419790267944, "step": 86750}
{"episode_reward": 29.0, "episode": 348.0, "Q1 loss": 4.508728472709656, "Q2 loss": 4.496839024543762, "Mean Target Q": 361.4569808349609, "Mean Q1": 361.4568818359375, "Mean Q2": 361.45757470703126, "critic_loss": 9.005567472457885, "batch_reward": 2.65553125, "actor_loss": -362.42860717773436, "actor_target_entropy": -2.0, "actor_entropy": 0.7279441237449646, "alpha_loss": -0.0008303337153047323, "alpha_value": 0.06070733645903349, "duration": 110.36049842834473, "step": 87000}
{"episode_reward": 962.0, "episode": 349.0, "Q1 loss": 4.734018475532531, "Q2 loss": 4.697274497032166, "Mean Target Q": 361.72441162109374, "Mean Q1": 361.7234012451172, "Mean Q2": 361.72281713867187, "critic_loss": 9.431292966842651, "batch_reward": 2.65734375, "actor_loss": -362.724876953125, "actor_target_entropy": -2.0, "actor_entropy": 0.6825359134674073, "alpha_loss": 0.001404194668866694, "alpha_value": 0.06070114864201483, "duration": 112.71406412124634, "step": 87250}
{"episode_reward": 1000.0, "episode": 350.0, "Q1 loss": 4.883242463111878, "Q2 loss": 4.815146951675415, "Mean Target Q": 362.0696004638672, "Mean Q1": 362.06778295898437, "Mean Q2": 362.06841088867185, "critic_loss": 9.698389408111572, "batch_reward": 2.65971875, "actor_loss": -363.353435546875, "actor_target_entropy": -2.0, "actor_entropy": 0.6685484523773193, "alpha_loss": 0.0012436131653375923, "alpha_value": 0.06051394649474304, "duration": 75.96949291229248, "step": 87500}
{"episode_reward": 968.0, "episode": 351.0, "Q1 loss": 17.633367274284364, "Q2 loss": 17.72385033226013, "Mean Target Q": 362.18338671875, "Mean Q1": 362.1708767089844, "Mean Q2": 362.1706179199219, "critic_loss": 35.357217796325685, "batch_reward": 2.6529921875, "actor_loss": -363.0102966308594, "actor_target_entropy": -2.0, "actor_entropy": 0.7821689009666443, "alpha_loss": -0.0018190411431714892, "alpha_value": 0.060452101519594764, "duration": 73.04027318954468, "step": 87750}
{"episode_reward": 996.0, "episode": 352.0, "Q1 loss": 5.774763489723205, "Q2 loss": 5.806175058364868, "Mean Target Q": 362.48776721191405, "Mean Q1": 362.48860583496094, "Mean Q2": 362.4889251708984, "critic_loss": 11.580938564300537, "batch_reward": 2.6700703125, "actor_loss": -363.63869384765627, "actor_target_entropy": -2.0, "actor_entropy": 0.9526198387145997, "alpha_loss": 0.003817159521393478, "alpha_value": 0.06050669682769604, "duration": 73.07450675964355, "step": 88000}
{"episode_reward": 728.0, "episode": 353.0, "Q1 loss": 5.05998123550415, "Q2 loss": 5.045423431396484, "Mean Target Q": 362.4646356201172, "Mean Q1": 362.46314453125, "Mean Q2": 362.46287353515623, "critic_loss": 10.105404680252075, "batch_reward": 2.6563984375, "actor_loss": -363.28237377929685, "actor_target_entropy": -2.0, "actor_entropy": 0.809841468334198, "alpha_loss": 0.002825194594450295, "alpha_value": 0.0600859232075542, "duration": 73.16384220123291, "step": 88250}
{"episode_reward": 972.0, "episode": 354.0, "Q1 loss": 4.844647346496582, "Q2 loss": 4.835090754508972, "Mean Target Q": 362.4472917480469, "Mean Q1": 362.4473204345703, "Mean Q2": 362.44743127441404, "critic_loss": 9.67973810005188, "batch_reward": 2.6657734375, "actor_loss": -363.50214501953127, "actor_target_entropy": -2.0, "actor_entropy": 0.726334846496582, "alpha_loss": -0.0004210646012797952, "alpha_value": 0.06000484449740026, "duration": 73.39235734939575, "step": 88500}
{"episode_reward": 966.0, "episode": 355.0, "Q1 loss": 4.7254152593612675, "Q2 loss": 4.709778738021851, "Mean Target Q": 362.4934326171875, "Mean Q1": 362.4934697265625, "Mean Q2": 362.4932951660156, "critic_loss": 9.435194017410279, "batch_reward": 2.665875, "actor_loss": -363.25344677734375, "actor_target_entropy": -2.0, "actor_entropy": 0.7036879873275756, "alpha_loss": -0.001149518036749214, "alpha_value": 0.06014151711761366, "duration": 73.5685715675354, "step": 88750}
{"episode_reward": 977.0, "episode": 356.0, "Q1 loss": 4.627018044471741, "Q2 loss": 4.653505841255188, "Mean Target Q": 362.24883825683594, "Mean Q1": 362.24747790527346, "Mean Q2": 362.2477059326172, "critic_loss": 9.280523866653443, "batch_reward": 2.6734453125, "actor_loss": -363.18528759765627, "actor_target_entropy": -2.0, "actor_entropy": 0.6762493762969971, "alpha_loss": 7.262315694242715e-05, "alpha_value": 0.06016623685694732, "duration": 75.14674162864685, "step": 89000}
{"episode_reward": 931.0, "episode": 357.0, "Q1 loss": 4.689476010322571, "Q2 loss": 4.629073513984681, "Mean Target Q": 362.07708251953125, "Mean Q1": 362.0754774169922, "Mean Q2": 362.07590576171873, "critic_loss": 9.318549520492553, "batch_reward": 2.671109375, "actor_loss": -362.95079711914065, "actor_target_entropy": -2.0, "actor_entropy": 0.7001391677856446, "alpha_loss": -0.0005107166562229395, "alpha_value": 0.060200904926676464, "duration": 73.64508318901062, "step": 89250}
{"episode_reward": 989.0, "episode": 358.0, "Q1 loss": 4.742114772796631, "Q2 loss": 4.706343249320984, "Mean Target Q": 362.53532885742186, "Mean Q1": 362.5364523925781, "Mean Q2": 362.5352757568359, "critic_loss": 9.448458034515381, "batch_reward": 2.6813984375, "actor_loss": -363.35096435546876, "actor_target_entropy": -2.0, "actor_entropy": 0.7126044688224793, "alpha_loss": -0.001947185818105936, "alpha_value": 0.060343220912762105, "duration": 73.60562586784363, "step": 89500}
{"episode_reward": 970.0, "episode": 359.0, "Q1 loss": 4.772296420097351, "Q2 loss": 4.7266642971038815, "Mean Target Q": 362.69030187988284, "Mean Q1": 362.68693591308596, "Mean Q2": 362.688443359375, "critic_loss": 9.49896074104309, "batch_reward": 2.6838984375, "actor_loss": -363.283380859375, "actor_target_entropy": -2.0, "actor_entropy": 0.7409920229911804, "alpha_loss": -0.0009253078447654843, "alpha_value": 0.06042074598595447, "duration": 73.77225041389465, "step": 89750}
{"episode_reward": 972.0, "episode": 360.0, "Q1 loss": 4.747518030166626, "Q2 loss": 4.726877514839172, "Mean Target Q": 362.96620349121093, "Mean Q1": 362.9663660888672, "Mean Q2": 362.965685546875, "critic_loss": 9.474395561218262, "batch_reward": 2.682484375, "actor_loss": -363.85919921875, "actor_target_entropy": -2.0, "actor_entropy": 0.7412532711029053, "alpha_loss": -0.0013605705806985498, "alpha_value": 0.060594842988406235, "step": 90000}
{"duration": 82.19490361213684, "step": 90000}
{"episode_reward": 996.0, "episode": 361.0, "Q1 loss": 4.795711717605591, "Q2 loss": 4.824737383842468, "Mean Target Q": 363.2863244628906, "Mean Q1": 363.2836328125, "Mean Q2": 363.28342639160155, "critic_loss": 9.620449108123779, "batch_reward": 2.686328125, "actor_loss": -364.104291015625, "actor_target_entropy": -2.0, "actor_entropy": 0.7761596293449402, "alpha_loss": 0.00017105763917788864, "alpha_value": 0.06068653512227414, "duration": 73.81919741630554, "step": 90250}
{"episode_reward": 0.0, "episode": 362.0, "Q1 loss": 4.769102639198303, "Q2 loss": 4.760628432273864, "Mean Target Q": 363.2029348144531, "Mean Q1": 363.2023787841797, "Mean Q2": 363.20279577636717, "critic_loss": 9.529731100082397, "batch_reward": 2.691125, "actor_loss": -363.8154611816406, "actor_target_entropy": -2.0, "actor_entropy": 0.7487428212165832, "alpha_loss": 0.002115381633862853, "alpha_value": 0.06057755776106674, "duration": 73.75232601165771, "step": 90500}
{"episode_reward": 973.0, "episode": 363.0, "Q1 loss": 4.868563801765442, "Q2 loss": 4.855669457435608, "Mean Target Q": 362.85423254394533, "Mean Q1": 362.8548575439453, "Mean Q2": 362.8552705078125, "critic_loss": 9.724233264923095, "batch_reward": 2.673359375, "actor_loss": -363.7595158691406, "actor_target_entropy": -2.0, "actor_entropy": 0.7380210056304931, "alpha_loss": 0.000293601855635643, "alpha_value": 0.060377446061827586, "duration": 73.6345739364624, "step": 90750}
{"episode_reward": 971.0, "episode": 364.0, "Q1 loss": 4.629962079048156, "Q2 loss": 4.619243772506714, "Mean Target Q": 363.7050200195313, "Mean Q1": 363.70309899902344, "Mean Q2": 363.7033345947266, "critic_loss": 9.24920583152771, "batch_reward": 2.6975, "actor_loss": -364.6944990234375, "actor_target_entropy": -2.0, "actor_entropy": 0.7295734453201294, "alpha_loss": 0.0003648333307355642, "alpha_value": 0.060338744525270126, "duration": 73.71788549423218, "step": 91000}
{"episode_reward": 973.0, "episode": 365.0, "Q1 loss": 4.806031882286072, "Q2 loss": 4.7561168203353885, "Mean Target Q": 363.3739307861328, "Mean Q1": 363.3720427246094, "Mean Q2": 363.37191479492185, "critic_loss": 9.56214866065979, "batch_reward": 2.680640625, "actor_loss": -364.5097028808594, "actor_target_entropy": -2.0, "actor_entropy": 0.7560463929176331, "alpha_loss": 0.0024608441377058626, "alpha_value": 0.06024676180802571, "duration": 73.69241523742676, "step": 91250}
{"episode_reward": 773.0, "episode": 366.0, "Q1 loss": 4.62937835407257, "Q2 loss": 4.618591300964355, "Mean Target Q": 363.62743640136716, "Mean Q1": 363.628298828125, "Mean Q2": 363.6284327392578, "critic_loss": 9.247969640731812, "batch_reward": 2.6969765625, "actor_loss": -364.4444584960938, "actor_target_entropy": -2.0, "actor_entropy": 0.6859293546676636, "alpha_loss": 0.0025432961150072516, "alpha_value": 0.05993230212961441, "duration": 73.80482029914856, "step": 91500}
{"episode_reward": 963.0, "episode": 367.0, "Q1 loss": 4.828341106414795, "Q2 loss": 4.804817217826844, "Mean Target Q": 363.9444183349609, "Mean Q1": 363.94259411621096, "Mean Q2": 363.94253857421876, "critic_loss": 9.633158317565918, "batch_reward": 2.6978515625, "actor_loss": -364.3287941894531, "actor_target_entropy": -2.0, "actor_entropy": 0.7099480242729187, "alpha_loss": -5.988604389131069e-05, "alpha_value": 0.059846964914669766, "duration": 73.71948742866516, "step": 91750}
{"episode_reward": 966.0, "episode": 368.0, "Q1 loss": 4.672913140296936, "Q2 loss": 4.657037390708924, "Mean Target Q": 364.11544299316404, "Mean Q1": 364.1140802001953, "Mean Q2": 364.11351342773435, "critic_loss": 9.32995051574707, "batch_reward": 2.69378125, "actor_loss": -365.0743303222656, "actor_target_entropy": -2.0, "actor_entropy": 0.7459761176109314, "alpha_loss": 0.0003899951498024166, "alpha_value": 0.05979740453052785, "duration": 73.88429427146912, "step": 92000}
{"episode_reward": 969.0, "episode": 369.0, "Q1 loss": 4.645916120529175, "Q2 loss": 4.638118140220642, "Mean Target Q": 364.5073531494141, "Mean Q1": 364.5062244873047, "Mean Q2": 364.50707250976564, "critic_loss": 9.28403427886963, "batch_reward": 2.698390625, "actor_loss": -365.3821240234375, "actor_target_entropy": -2.0, "actor_entropy": 0.7320093741416931, "alpha_loss": -0.0004694475596770644, "alpha_value": 0.05982135308887807, "duration": 74.91886067390442, "step": 92250}
{"episode_reward": 954.0, "episode": 370.0, "Q1 loss": 4.824378735542298, "Q2 loss": 4.788720746040344, "Mean Target Q": 364.9271774902344, "Mean Q1": 364.9261057128906, "Mean Q2": 364.9253731689453, "critic_loss": 9.613099473953246, "batch_reward": 2.703109375, "actor_loss": -365.85223413085936, "actor_target_entropy": -2.0, "actor_entropy": 0.7335766491889953, "alpha_loss": 0.0016371946446597577, "alpha_value": 0.059750590593115015, "duration": 74.77289867401123, "step": 92500}
{"episode_reward": 981.0, "episode": 371.0, "Q1 loss": 4.502239951133728, "Q2 loss": 4.453721183776856, "Mean Target Q": 364.7375830078125, "Mean Q1": 364.73614758300783, "Mean Q2": 364.7347897949219, "critic_loss": 8.955961179733276, "batch_reward": 2.7008125, "actor_loss": -365.73913134765627, "actor_target_entropy": -2.0, "actor_entropy": 0.7809304361343383, "alpha_loss": 0.00034514094423502683, "alpha_value": 0.05965664215513989, "duration": 73.87373542785645, "step": 92750}
{"episode_reward": 936.0, "episode": 372.0, "Q1 loss": 4.604278849601745, "Q2 loss": 4.642279075622558, "Mean Target Q": 365.1163504638672, "Mean Q1": 365.11419604492187, "Mean Q2": 365.1159979248047, "critic_loss": 9.246557924270629, "batch_reward": 2.7113984375, "actor_loss": -366.4018129882812, "actor_target_entropy": -2.0, "actor_entropy": 0.7753248729705811, "alpha_loss": 0.0016175507362931967, "alpha_value": 0.05952907864347321, "duration": 73.87661910057068, "step": 93000}
{"episode_reward": 964.0, "episode": 373.0, "Q1 loss": 4.551874318122864, "Q2 loss": 4.569941104888916, "Mean Target Q": 365.4563663330078, "Mean Q1": 365.4548885498047, "Mean Q2": 365.4545338134766, "critic_loss": 9.121815420150757, "batch_reward": 2.71321875, "actor_loss": -366.479234375, "actor_target_entropy": -2.0, "actor_entropy": 0.7489208784103394, "alpha_loss": -0.0005949816918000578, "alpha_value": 0.059486759382650495, "duration": 74.09282517433167, "step": 93250}
{"episode_reward": 962.0, "episode": 374.0, "Q1 loss": 4.531399181365967, "Q2 loss": 4.558120979309082, "Mean Target Q": 365.5823395996094, "Mean Q1": 365.5828731689453, "Mean Q2": 365.5827257080078, "critic_loss": 9.089520147323608, "batch_reward": 2.7228125, "actor_loss": -366.4990224609375, "actor_target_entropy": -2.0, "actor_entropy": 0.7096145467758179, "alpha_loss": 0.0016687538577243686, "alpha_value": 0.05944139967493251, "duration": 74.0103588104248, "step": 93500}
{"episode_reward": 670.0, "episode": 375.0, "Q1 loss": 4.598157287597656, "Q2 loss": 4.642928478240967, "Mean Target Q": 365.36658361816404, "Mean Q1": 365.3654951171875, "Mean Q2": 365.36513879394533, "critic_loss": 9.241085777282715, "batch_reward": 2.7152578125, "actor_loss": -366.22979809570313, "actor_target_entropy": -2.0, "actor_entropy": 0.6967402529716492, "alpha_loss": 0.0019179036715067923, "alpha_value": 0.05926482547500323, "duration": 73.85648012161255, "step": 93750}
{"episode_reward": 958.0, "episode": 376.0, "Q1 loss": 4.70020883846283, "Q2 loss": 4.598008684158325, "Mean Target Q": 365.8867303466797, "Mean Q1": 365.8850859375, "Mean Q2": 365.8858332519531, "critic_loss": 9.298217531204223, "batch_reward": 2.733984375, "actor_loss": -366.83142309570314, "actor_target_entropy": -2.0, "actor_entropy": 0.7018597135543824, "alpha_loss": 0.0035483871903270485, "alpha_value": 0.05894622205076142, "duration": 74.0046820640564, "step": 94000}
{"episode_reward": 957.0, "episode": 377.0, "Q1 loss": 4.7760694189071655, "Q2 loss": 4.795690546035766, "Mean Target Q": 366.1181072998047, "Mean Q1": 366.1177495117187, "Mean Q2": 366.1182158203125, "critic_loss": 9.57175997543335, "batch_reward": 2.7259453125, "actor_loss": -367.30949829101564, "actor_target_entropy": -2.0, "actor_entropy": 0.695290934085846, "alpha_loss": 0.0008480824171565473, "alpha_value": 0.05868959027387958, "duration": 74.08706188201904, "step": 94250}
{"episode_reward": 962.0, "episode": 378.0, "Q1 loss": 4.573343457221985, "Q2 loss": 4.5487278881073, "Mean Target Q": 366.14413879394533, "Mean Q1": 366.1448806152344, "Mean Q2": 366.14318017578125, "critic_loss": 9.122071361541748, "batch_reward": 2.727453125, "actor_loss": -367.26731469726565, "actor_target_entropy": -2.0, "actor_entropy": 0.6493250541687011, "alpha_loss": 0.003145771812647581, "alpha_value": 0.05854935674290909, "duration": 74.07661509513855, "step": 94500}
{"episode_reward": 760.0, "episode": 379.0, "Q1 loss": 4.580257153511047, "Q2 loss": 4.531824273109436, "Mean Target Q": 365.9598748779297, "Mean Q1": 365.95484326171874, "Mean Q2": 365.9562952880859, "critic_loss": 9.112081420898438, "batch_reward": 2.7265625, "actor_loss": -366.9224750976563, "actor_target_entropy": -2.0, "actor_entropy": 0.6552745876312256, "alpha_loss": 0.0029035383714362977, "alpha_value": 0.05818463455133558, "duration": 74.18663144111633, "step": 94750}
{"episode_reward": 970.0, "episode": 380.0, "Q1 loss": 4.578296360969543, "Q2 loss": 4.551458207130432, "Mean Target Q": 366.11610876464846, "Mean Q1": 366.11490844726563, "Mean Q2": 366.1133498535156, "critic_loss": 9.129754550933837, "batch_reward": 2.7313203125, "actor_loss": -367.21383740234376, "actor_target_entropy": -2.0, "actor_entropy": 0.7291936702728271, "alpha_loss": -0.0008056949889287353, "alpha_value": 0.058107927282373184, "step": 95000}
{"duration": 86.46092557907104, "step": 95000}
{"episode_reward": 983.0, "episode": 381.0, "Q1 loss": 4.488552563667297, "Q2 loss": 4.449271053314209, "Mean Target Q": 366.71247644042967, "Mean Q1": 366.71235205078125, "Mean Q2": 366.7137322998047, "critic_loss": 8.937823631286621, "batch_reward": 2.734984375, "actor_loss": -367.6934482421875, "actor_target_entropy": -2.0, "actor_entropy": 0.697169020652771, "alpha_loss": -0.0005640344684943557, "alpha_value": 0.058196990699152455, "duration": 123.76147985458374, "step": 95250}
{"episode_reward": 986.0, "episode": 382.0, "Q1 loss": 4.618418137550354, "Q2 loss": 4.64394209766388, "Mean Target Q": 366.89184350585936, "Mean Q1": 366.8912813720703, "Mean Q2": 366.890955078125, "critic_loss": 9.262360231399537, "batch_reward": 2.7441171875, "actor_loss": -367.78440014648436, "actor_target_entropy": -2.0, "actor_entropy": 0.6933198637962341, "alpha_loss": -0.0014758612681180238, "alpha_value": 0.058208249759995646, "duration": 106.67878651618958, "step": 95500}
{"episode_reward": 992.0, "episode": 383.0, "Q1 loss": 4.490417964935303, "Q2 loss": 4.4544570331573485, "Mean Target Q": 367.036828125, "Mean Q1": 367.03550317382815, "Mean Q2": 367.03558837890625, "critic_loss": 8.944875007629394, "batch_reward": 2.7383203125, "actor_loss": -367.75154638671876, "actor_target_entropy": -2.0, "actor_entropy": 0.6864149899482727, "alpha_loss": -0.0012101287953555583, "alpha_value": 0.058444876547484315, "duration": 112.80628943443298, "step": 95750}
{"episode_reward": 971.0, "episode": 384.0, "Q1 loss": 4.569251185417175, "Q2 loss": 4.60982981300354, "Mean Target Q": 367.1461928710938, "Mean Q1": 367.1454375, "Mean Q2": 367.1437922363281, "critic_loss": 9.179081007003784, "batch_reward": 2.735625, "actor_loss": -368.3673256835938, "actor_target_entropy": -2.0, "actor_entropy": 0.6853260011672974, "alpha_loss": 0.0012596513023599982, "alpha_value": 0.05841279794146857, "duration": 119.67279434204102, "step": 96000}
{"episode_reward": 973.0, "episode": 385.0, "Q1 loss": 4.648893786430359, "Q2 loss": 4.631975597381592, "Mean Target Q": 367.70915087890626, "Mean Q1": 367.70538732910154, "Mean Q2": 367.70812951660156, "critic_loss": 9.280869388580323, "batch_reward": 2.74825, "actor_loss": -368.6924406738281, "actor_target_entropy": -2.0, "actor_entropy": 0.6736821956634521, "alpha_loss": 0.001015764449723065, "alpha_value": 0.05831289065645548, "duration": 118.06473803520203, "step": 96250}
{"episode_reward": 991.0, "episode": 386.0, "Q1 loss": 4.597842450141907, "Q2 loss": 4.561105331420898, "Mean Target Q": 367.673982421875, "Mean Q1": 367.6740153808594, "Mean Q2": 367.671900390625, "critic_loss": 9.158947816848755, "batch_reward": 2.745109375, "actor_loss": -368.6432145996094, "actor_target_entropy": -2.0, "actor_entropy": 0.7174600658416748, "alpha_loss": 0.0010049621486105025, "alpha_value": 0.05817049255654535, "duration": 118.78152275085449, "step": 96500}
{"episode_reward": 973.0, "episode": 387.0, "Q1 loss": 4.462522116661072, "Q2 loss": 4.5245472469329835, "Mean Target Q": 368.31141967773436, "Mean Q1": 368.31084948730467, "Mean Q2": 368.310486328125, "critic_loss": 8.987069370269776, "batch_reward": 2.760765625, "actor_loss": -369.4285998535156, "actor_target_entropy": -2.0, "actor_entropy": 0.6878898959159852, "alpha_loss": 0.0006473643062636257, "alpha_value": 0.05808763647649171, "duration": 108.68833684921265, "step": 96750}
{"episode_reward": 971.0, "episode": 388.0, "Q1 loss": 4.530301796913147, "Q2 loss": 4.470390469551086, "Mean Target Q": 368.3681103515625, "Mean Q1": 368.3681676025391, "Mean Q2": 368.36799426269533, "critic_loss": 9.000692276000976, "batch_reward": 2.7536640625, "actor_loss": -369.31133544921875, "actor_target_entropy": -2.0, "actor_entropy": 0.6757252998352051, "alpha_loss": 0.0015343367159366607, "alpha_value": 0.05797737790835697, "duration": 108.2759838104248, "step": 97000}
{"episode_reward": 425.0, "episode": 389.0, "Q1 loss": 4.683976211547852, "Q2 loss": 4.665722476959228, "Mean Target Q": 368.39184252929687, "Mean Q1": 368.38751342773435, "Mean Q2": 368.38850024414063, "critic_loss": 9.349698692321777, "batch_reward": 2.746203125, "actor_loss": -369.20539013671873, "actor_target_entropy": -2.0, "actor_entropy": 0.6657112050056457, "alpha_loss": 0.0015062646404840052, "alpha_value": 0.05788227440700556, "duration": 108.8057496547699, "step": 97250}
{"episode_reward": 975.0, "episode": 390.0, "Q1 loss": 4.517247346878052, "Q2 loss": 4.554156757354736, "Mean Target Q": 368.7155137939453, "Mean Q1": 368.7156572265625, "Mean Q2": 368.7156076660156, "critic_loss": 9.071404119491577, "batch_reward": 2.753765625, "actor_loss": -369.4585815429688, "actor_target_entropy": -2.0, "actor_entropy": 0.6686042976379395, "alpha_loss": 0.001797870310023427, "alpha_value": 0.05763485275282962, "duration": 103.32772636413574, "step": 97500}
{"episode_reward": 783.0, "episode": 391.0, "Q1 loss": 4.604815532684326, "Q2 loss": 4.601080516815186, "Mean Target Q": 368.8292919921875, "Mean Q1": 368.82968823242186, "Mean Q2": 368.82908923339846, "critic_loss": 9.205896055221558, "batch_reward": 2.7462109375, "actor_loss": -369.5995417480469, "actor_target_entropy": -2.0, "actor_entropy": 0.688115605354309, "alpha_loss": -0.0003762286063283682, "alpha_value": 0.05755206542378865, "duration": 74.68620777130127, "step": 97750}
{"episode_reward": 974.0, "episode": 392.0, "Q1 loss": 4.524319046974182, "Q2 loss": 4.47135364151001, "Mean Target Q": 368.88707739257814, "Mean Q1": 368.88145739746096, "Mean Q2": 368.8835736083984, "critic_loss": 8.995672704696656, "batch_reward": 2.7505703125, "actor_loss": -369.65695947265624, "actor_target_entropy": -2.0, "actor_entropy": 0.6758692855834961, "alpha_loss": -3.233633702620864e-05, "alpha_value": 0.05759040200575784, "duration": 74.18914651870728, "step": 98000}
{"episode_reward": 973.0, "episode": 393.0, "Q1 loss": 4.482881171226501, "Q2 loss": 4.504719634056091, "Mean Target Q": 369.32916772460936, "Mean Q1": 369.3303018798828, "Mean Q2": 369.32974475097654, "critic_loss": 8.987600786209107, "batch_reward": 2.7611328125, "actor_loss": -370.4831823730469, "actor_target_entropy": -2.0, "actor_entropy": 0.6879005465507507, "alpha_loss": 0.001579927921295166, "alpha_value": 0.057546689632896046, "duration": 74.37010645866394, "step": 98250}
{"episode_reward": 933.0, "episode": 394.0, "Q1 loss": 4.548724376678467, "Q2 loss": 4.541565563201904, "Mean Target Q": 369.41328051757813, "Mean Q1": 369.4133403320312, "Mean Q2": 369.413310546875, "critic_loss": 9.090289955139161, "batch_reward": 2.7644296875, "actor_loss": -370.1735092773437, "actor_target_entropy": -2.0, "actor_entropy": 0.6175399174690247, "alpha_loss": 0.0015805580378510059, "alpha_value": 0.057354626965583154, "duration": 91.94122385978699, "step": 98500}
{"episode_reward": 932.0, "episode": 395.0, "Q1 loss": 4.438874183654785, "Q2 loss": 4.4343045816421505, "Mean Target Q": 369.8286973876953, "Mean Q1": 369.826138671875, "Mean Q2": 369.82703955078125, "critic_loss": 8.873178743362427, "batch_reward": 2.7669296875, "actor_loss": -370.81245581054685, "actor_target_entropy": -2.0, "actor_entropy": 0.6461532185077667, "alpha_loss": 0.0052110508782789115, "alpha_value": 0.05700001752265813, "duration": 114.04092025756836, "step": 98750}
{"episode_reward": 954.0, "episode": 396.0, "Q1 loss": 4.546134130477905, "Q2 loss": 4.569415898323059, "Mean Target Q": 370.158978515625, "Mean Q1": 370.15560290527344, "Mean Q2": 370.1556013183594, "critic_loss": 9.115550004959106, "batch_reward": 2.770265625, "actor_loss": -371.24393530273437, "actor_target_entropy": -2.0, "actor_entropy": 0.6550143094062805, "alpha_loss": 0.0015730819888412953, "alpha_value": 0.05663665404802218, "duration": 152.18943405151367, "step": 99000}
{"episode_reward": 988.0, "episode": 397.0, "Q1 loss": 4.440322215080261, "Q2 loss": 4.439459692001343, "Mean Target Q": 370.05908361816404, "Mean Q1": 370.0588757324219, "Mean Q2": 370.05694812011717, "critic_loss": 8.8797818775177, "batch_reward": 2.7723828125, "actor_loss": -371.2734609375, "actor_target_entropy": -2.0, "actor_entropy": 0.6549691669940948, "alpha_loss": 0.002063783656805754, "alpha_value": 0.056510501611457035, "duration": 168.79921078681946, "step": 99250}
{"episode_reward": 8.0, "episode": 398.0, "Q1 loss": 4.510316124916077, "Q2 loss": 4.48214453125, "Mean Target Q": 370.65079846191406, "Mean Q1": 370.6511962890625, "Mean Q2": 370.6519033203125, "critic_loss": 8.992460622787476, "batch_reward": 2.770953125, "actor_loss": -371.6174384765625, "actor_target_entropy": -2.0, "actor_entropy": 0.6776509590148926, "alpha_loss": 0.0017428604909218847, "alpha_value": 0.05626910061687589, "duration": 156.21569800376892, "step": 99500}
{"episode_reward": 978.0, "episode": 399.0, "Q1 loss": 4.364585803985595, "Q2 loss": 4.327867866516113, "Mean Target Q": 370.4816160888672, "Mean Q1": 370.47713671875, "Mean Q2": 370.4787272949219, "critic_loss": 8.69245369529724, "batch_reward": 2.7606796875, "actor_loss": -371.2936364746094, "actor_target_entropy": -2.0, "actor_entropy": 0.6660399899482727, "alpha_loss": 0.0015671937260776758, "alpha_value": 0.056117599746822296, "duration": 145.24013090133667, "step": 99750}
{"episode_reward": 25.0, "episode": 400.0, "Q1 loss": 4.548961643234314, "Q2 loss": 4.535704528471552, "Mean Target Q": 370.37785075850275, "Mean Q1": 370.37744103856835, "Mean Q2": 370.37686402347674, "critic_loss": 9.084666148725763, "batch_reward": 2.7583615712851404, "actor_loss": -371.09215942382815, "actor_target_entropy": -2.0, "actor_entropy": 0.6327020442485809, "alpha_loss": 0.0010304636103101075, "alpha_value": 0.05589346803554918, "step": 99999}
