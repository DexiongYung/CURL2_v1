{"episode_reward": 0.0, "episode": 1.0, "duration": 31.4008150100708, "step": 250}
{"episode_reward": 0.0, "episode": 2.0, "duration": 1.180046558380127, "step": 500}
{"episode_reward": 0.0, "episode": 3.0, "duration": 1.1791918277740479, "step": 750}
{"episode_reward": 20.0, "episode": 4.0, "duration": 1.1780624389648438, "step": 1000}
{"episode_reward": 0.0, "episode": 5.0, "Q1 loss": 0.08580837855301797, "Q2 loss": 0.08798580819903873, "Mean Target Q": -0.015115422417409718, "Mean Q1": -0.0158141004772624, "Mean Q2": -0.014581566980923525, "critic_loss": 0.17379418567032553, "batch_reward": 0.0170546875, "actor_loss": -0.09391253903508186, "actor_target_entropy": -2.0, "actor_entropy": 2.4622389278411867, "alpha_loss": 0.3140238980054855, "alpha_value": 0.09939569401688689, "duration": 322.53419971466064, "step": 1250}
{"episode_reward": 0.0, "episode": 6.0, "Q1 loss": 0.04962787526485044, "Q2 loss": 0.049631201762706044, "Mean Target Q": 0.23406386345624924, "Mean Q1": 0.23343028622865677, "Mean Q2": 0.23339996534585952, "critic_loss": 0.09925907688634471, "batch_reward": 0.0135390625, "actor_loss": -0.3662290599346161, "actor_target_entropy": -2.0, "actor_entropy": 2.531588788986206, "alpha_loss": 0.3292914350032806, "alpha_value": 0.09812722670021723, "duration": 282.8020477294922, "step": 1500}
{"episode_reward": 0.0, "episode": 7.0, "Q1 loss": 0.04608321275631897, "Q2 loss": 0.04609304507321212, "Mean Target Q": 0.4042277669906616, "Mean Q1": 0.40388649451732633, "Mean Q2": 0.4038911113739014, "critic_loss": 0.09217625783244147, "batch_reward": 0.012375, "actor_loss": -0.5345841262340546, "actor_target_entropy": -2.0, "actor_entropy": 2.5433173027038576, "alpha_loss": 0.32537802624702455, "alpha_value": 0.09690811160811025, "duration": 328.75774216651917, "step": 1750}
{"episode_reward": 0.0, "episode": 8.0, "Q1 loss": 0.12693615203350783, "Q2 loss": 0.12692757006979083, "Mean Target Q": 0.5980975410938263, "Mean Q1": 0.597186863064766, "Mean Q2": 0.597218765616417, "critic_loss": 0.2538637219076045, "batch_reward": 0.03496875, "actor_loss": -0.7189944820404053, "actor_target_entropy": -2.0, "actor_entropy": 2.534355827331543, "alpha_loss": 0.3201811547279358, "alpha_value": 0.09571715075537462, "duration": 322.1989834308624, "step": 2000}
{"episode_reward": 58.0, "episode": 9.0, "Q1 loss": 0.06976808217912912, "Q2 loss": 0.07018621000275016, "Mean Target Q": 0.5647737181186676, "Mean Q1": 0.5627744835615158, "Mean Q2": 0.562858842253685, "critic_loss": 0.13995429188013075, "batch_reward": 0.0375546875, "actor_loss": -0.6787333860397339, "actor_target_entropy": -2.0, "actor_entropy": 2.5760390491485596, "alpha_loss": 0.31559149408340453, "alpha_value": 0.09455158286813731, "duration": 303.0581784248352, "step": 2250}
{"episode_reward": 4.0, "episode": 10.0, "Q1 loss": 0.055195696741342545, "Q2 loss": 0.05591508368588984, "Mean Target Q": 0.6465351135730744, "Mean Q1": 0.6455602989196777, "Mean Q2": 0.6454571503400802, "critic_loss": 0.11111078015714884, "batch_reward": 0.0345546875, "actor_loss": -0.778399498462677, "actor_target_entropy": -2.0, "actor_entropy": 2.5511035232543944, "alpha_loss": 0.3064305741786957, "alpha_value": 0.09341676099350076, "duration": 304.45553517341614, "step": 2500}
{"episode_reward": 0.0, "episode": 11.0, "Q1 loss": 0.10105841712653638, "Q2 loss": 0.1004012715369463, "Mean Target Q": 0.8088975253105164, "Mean Q1": 0.8074193706512451, "Mean Q2": 0.8073896837234497, "critic_loss": 0.20145968846976758, "batch_reward": 0.0342578125, "actor_loss": -0.9390955786705018, "actor_target_entropy": -2.0, "actor_entropy": 2.5593018417358397, "alpha_loss": 0.30082548546791077, "alpha_value": 0.09231308217788495, "duration": 317.1458361148834, "step": 2750}
{"episode_reward": 0.0, "episode": 12.0, "Q1 loss": 0.10381592194736004, "Q2 loss": 0.10470147885382175, "Mean Target Q": 0.9572094571590424, "Mean Q1": 0.9559910638332367, "Mean Q2": 0.9559554381370544, "critic_loss": 0.20851740028709173, "batch_reward": 0.0288125, "actor_loss": -1.0996377744674684, "actor_target_entropy": -2.0, "actor_entropy": 2.563011823654175, "alpha_loss": 0.296706102848053, "alpha_value": 0.09122498550968627, "duration": 305.91406059265137, "step": 3000}
{"episode_reward": 0.0, "episode": 13.0, "Q1 loss": 0.1317901705354452, "Q2 loss": 0.13219424371048807, "Mean Target Q": 1.0869556498527526, "Mean Q1": 1.0860751025676727, "Mean Q2": 1.086038073539734, "critic_loss": 0.26398441594094035, "batch_reward": 0.02575, "actor_loss": -1.2425382986068725, "actor_target_entropy": -2.0, "actor_entropy": 2.539623067855835, "alpha_loss": 0.29257255387306214, "alpha_value": 0.09015598122217501, "duration": 492.7101855278015, "step": 3250}
{"episode_reward": 0.0, "episode": 14.0, "Q1 loss": 0.19993217816203832, "Q2 loss": 0.20090217278152706, "Mean Target Q": 1.1912907717227936, "Mean Q1": 1.1894956216812134, "Mean Q2": 1.1895966529846191, "critic_loss": 0.40083435100317, "batch_reward": 0.025, "actor_loss": -1.348119951248169, "actor_target_entropy": -2.0, "actor_entropy": 2.567835985183716, "alpha_loss": 0.287509806394577, "alpha_value": 0.08910138157640227, "duration": 447.4892842769623, "step": 3500}
{"episode_reward": 0.0, "episode": 15.0, "Q1 loss": 1.1257000944912434, "Q2 loss": 1.1361116897165775, "Mean Target Q": 1.3852515664100646, "Mean Q1": 1.3810072345733642, "Mean Q2": 1.3799316167831421, "critic_loss": 2.26181177932024, "batch_reward": 0.054, "actor_loss": -1.5140226669311523, "actor_target_entropy": -2.0, "actor_entropy": 2.5526903610229494, "alpha_loss": 0.27148019361495973, "alpha_value": 0.08808039822219836, "duration": 321.494179725647, "step": 3750}
{"episode_reward": 121.0, "episode": 16.0, "Q1 loss": 0.7030411455631256, "Q2 loss": 0.7147895478606224, "Mean Target Q": 1.4696337728500366, "Mean Q1": 1.4662567133903504, "Mean Q2": 1.466203317642212, "critic_loss": 1.4178307001590729, "batch_reward": 0.0516015625, "actor_loss": -1.6157492389678956, "actor_target_entropy": -2.0, "actor_entropy": 2.5326404876708986, "alpha_loss": 0.2622663838863373, "alpha_value": 0.08711366330627482, "duration": 178.6693696975708, "step": 4000}
{"episode_reward": 0.0, "episode": 17.0, "Q1 loss": 0.6686362991929055, "Q2 loss": 0.6765201106667519, "Mean Target Q": 1.5677743463516236, "Mean Q1": 1.562618956565857, "Mean Q2": 1.562219181060791, "critic_loss": 1.3451564120054245, "batch_reward": 0.0485546875, "actor_loss": -1.740916368484497, "actor_target_entropy": -2.0, "actor_entropy": 2.4939580764770506, "alpha_loss": 0.2549249895811081, "alpha_value": 0.08616071588043867, "duration": 83.99603700637817, "step": 4250}
{"episode_reward": 0.0, "episode": 18.0, "Q1 loss": 1.0400611163377762, "Q2 loss": 1.0429634556174279, "Mean Target Q": 1.7949067754745482, "Mean Q1": 1.7939415574073792, "Mean Q2": 1.793650944709778, "critic_loss": 2.0830245739221573, "batch_reward": 0.070609375, "actor_loss": -1.96595014667511, "actor_target_entropy": -2.0, "actor_entropy": 2.4865339870452883, "alpha_loss": 0.2329324905872345, "alpha_value": 0.08525150117167117, "duration": 83.22570490837097, "step": 4500}
{"episode_reward": 111.0, "episode": 19.0, "Q1 loss": 2.387402675032616, "Q2 loss": 2.402706674337387, "Mean Target Q": 2.384645124435425, "Mean Q1": 2.378347055912018, "Mean Q2": 2.3778389863967897, "critic_loss": 4.7901093544960025, "batch_reward": 0.1417421875, "actor_loss": -2.630271572113037, "actor_target_entropy": -2.0, "actor_entropy": 2.381670419692993, "alpha_loss": 0.17830095410346986, "alpha_value": 0.0844585124888945, "duration": 83.59540152549744, "step": 4750}
{"episode_reward": 521.0, "episode": 20.0, "Q1 loss": 2.375950930595398, "Q2 loss": 2.3679092602729797, "Mean Target Q": 3.1180459880828857, "Mean Q1": 3.1105838232040406, "Mean Q2": 3.1111420454978944, "critic_loss": 4.7438601922988894, "batch_reward": 0.1728046875, "actor_loss": -3.5132968769073485, "actor_target_entropy": -2.0, "actor_entropy": 2.1950988445281983, "alpha_loss": 0.12469637705385685, "alpha_value": 0.08387797908930501, "step": 5000}
{"duration": 114.52238607406616, "step": 5000}
{"episode_reward": 0.0, "episode": 21.0, "Q1 loss": 2.292818211078644, "Q2 loss": 2.2973550300598147, "Mean Target Q": 3.7007949495315553, "Mean Q1": 3.695947413444519, "Mean Q2": 3.694800756454468, "critic_loss": 4.590173248291015, "batch_reward": 0.1633828125, "actor_loss": -4.166230657577515, "actor_target_entropy": -2.0, "actor_entropy": 1.949453748703003, "alpha_loss": 0.09872826650738716, "alpha_value": 0.08343768861871924, "duration": 83.1151807308197, "step": 5250}
{"episode_reward": 0.0, "episode": 22.0, "Q1 loss": 3.1156831860542296, "Q2 loss": 3.107177299499512, "Mean Target Q": 4.296768398284912, "Mean Q1": 4.295288291931152, "Mean Q2": 4.294960854530334, "critic_loss": 6.22286048412323, "batch_reward": 0.154390625, "actor_loss": -4.836220823287964, "actor_target_entropy": -2.0, "actor_entropy": 1.7545317678451537, "alpha_loss": 0.08565313583612442, "alpha_value": 0.0830255392444549, "duration": 82.67043662071228, "step": 5500}
{"episode_reward": 142.0, "episode": 23.0, "Q1 loss": 3.6104108233451844, "Q2 loss": 3.6211027574539183, "Mean Target Q": 5.240857049942017, "Mean Q1": 5.230708448410034, "Mean Q2": 5.231286759376526, "critic_loss": 7.231513605117798, "batch_reward": 0.1729765625, "actor_loss": -5.732097568511963, "actor_target_entropy": -2.0, "actor_entropy": 1.5825131006240845, "alpha_loss": 0.07266900810599328, "alpha_value": 0.08268209912743138, "duration": 82.44112396240234, "step": 5750}
{"episode_reward": 0.0, "episode": 24.0, "Q1 loss": 3.5670952773094178, "Q2 loss": 3.572855185508728, "Mean Target Q": 5.758425127029419, "Mean Q1": 5.752871547698975, "Mean Q2": 5.75427289390564, "critic_loss": 7.139950461387635, "batch_reward": 0.1639375, "actor_loss": -6.207720474243164, "actor_target_entropy": -2.0, "actor_entropy": 1.660059142112732, "alpha_loss": 0.08321801744401455, "alpha_value": 0.08231127704700164, "duration": 126.87458920478821, "step": 6000}
{"episode_reward": 0.0, "episode": 25.0, "Q1 loss": 5.605269203186035, "Q2 loss": 5.595510869503022, "Mean Target Q": 6.7879409465789795, "Mean Q1": 6.781839250564575, "Mean Q2": 6.7807338943481446, "critic_loss": 11.200780072212218, "batch_reward": 0.188671875, "actor_loss": -7.157978691101074, "actor_target_entropy": -2.0, "actor_entropy": 1.6914135627746583, "alpha_loss": 0.10850966125726699, "alpha_value": 0.08182479573819854, "duration": 191.98744773864746, "step": 6250}
{"episode_reward": 503.0, "episode": 26.0, "Q1 loss": 5.726104388236999, "Q2 loss": 5.7127126121520995, "Mean Target Q": 8.02577568435669, "Mean Q1": 8.013007305145264, "Mean Q2": 8.013200267791747, "critic_loss": 11.438817015647889, "batch_reward": 0.232734375, "actor_loss": -8.386743343353272, "actor_target_entropy": -2.0, "actor_entropy": 1.704237949371338, "alpha_loss": 0.12767971026897432, "alpha_value": 0.08118421356924457, "duration": 199.9629602432251, "step": 6500}
{"episode_reward": 0.0, "episode": 27.0, "Q1 loss": 6.4710183420181275, "Q2 loss": 6.526720627784729, "Mean Target Q": 9.338046548843383, "Mean Q1": 9.332560779571534, "Mean Q2": 9.332298084259033, "critic_loss": 12.99773892211914, "batch_reward": 0.273421875, "actor_loss": -9.669081981658936, "actor_target_entropy": -2.0, "actor_entropy": 1.6179236669540404, "alpha_loss": 0.11510289478302002, "alpha_value": 0.08051297894809357, "duration": 207.97478699684143, "step": 6750}
{"episode_reward": 837.0, "episode": 28.0, "Q1 loss": 7.122821327209473, "Q2 loss": 7.223069605827331, "Mean Target Q": 11.715133323669434, "Mean Q1": 11.705148818969727, "Mean Q2": 11.705085063934327, "critic_loss": 14.345890951156616, "batch_reward": 0.363234375, "actor_loss": -12.174846282958985, "actor_target_entropy": -2.0, "actor_entropy": 1.5502150526046754, "alpha_loss": 0.09579810670018196, "alpha_value": 0.07991756730922825, "duration": 211.60385584831238, "step": 7000}
{"episode_reward": 216.0, "episode": 29.0, "Q1 loss": 8.597888189315796, "Q2 loss": 8.680102418899535, "Mean Target Q": 14.030696132659912, "Mean Q1": 14.014791816711426, "Mean Q2": 14.014077980041504, "critic_loss": 17.277990615844725, "batch_reward": 0.4273046875, "actor_loss": -14.55898063659668, "actor_target_entropy": -2.0, "actor_entropy": 1.427400053501129, "alpha_loss": 0.07377103731036186, "alpha_value": 0.07942891368323854, "duration": 205.64035558700562, "step": 7250}
{"episode_reward": 982.0, "episode": 30.0, "Q1 loss": 12.223933977127075, "Q2 loss": 12.43871173477173, "Mean Target Q": 16.522943607330323, "Mean Q1": 16.510677574157715, "Mean Q2": 16.51178314590454, "critic_loss": 24.66264572906494, "batch_reward": 0.502625, "actor_loss": -17.077821990966797, "actor_target_entropy": -2.0, "actor_entropy": 1.212480714559555, "alpha_loss": 0.05714118917286396, "alpha_value": 0.07903620096225335, "duration": 218.49008083343506, "step": 7500}
{"episode_reward": 629.0, "episode": 31.0, "Q1 loss": 10.755875888824463, "Q2 loss": 10.864232875823975, "Mean Target Q": 20.10621435546875, "Mean Q1": 20.10097579574585, "Mean Q2": 20.101190078735353, "critic_loss": 21.62010873031616, "batch_reward": 0.6044140625, "actor_loss": -20.930593826293947, "actor_target_entropy": -2.0, "actor_entropy": 1.1146000928878783, "alpha_loss": 0.036922573942691085, "alpha_value": 0.0787209539544834, "duration": 207.74147486686707, "step": 7750}
{"episode_reward": 972.0, "episode": 32.0, "Q1 loss": 12.4854714012146, "Q2 loss": 12.57829829788208, "Mean Target Q": 23.25173471069336, "Mean Q1": 23.239798385620116, "Mean Q2": 23.240076499938965, "critic_loss": 25.063769687652588, "batch_reward": 0.6708984375, "actor_loss": -24.21732942199707, "actor_target_entropy": -2.0, "actor_entropy": 1.0303558368682861, "alpha_loss": 0.006993750001303852, "alpha_value": 0.07857932170779473, "duration": 85.68148374557495, "step": 8000}
{"episode_reward": 627.0, "episode": 33.0, "Q1 loss": 14.239827772140503, "Q2 loss": 14.32444979095459, "Mean Target Q": 27.106029235839845, "Mean Q1": 27.095360778808594, "Mean Q2": 27.094710487365724, "critic_loss": 28.564277534484862, "batch_reward": 0.75209375, "actor_loss": -28.49589566040039, "actor_target_entropy": -2.0, "actor_entropy": 0.8934088277816773, "alpha_loss": -0.019285515354946256, "alpha_value": 0.07860184326161812, "duration": 81.85155177116394, "step": 8250}
{"episode_reward": 889.0, "episode": 34.0, "Q1 loss": 18.51158811569214, "Q2 loss": 18.573845779418946, "Mean Target Q": 31.224359786987304, "Mean Q1": 31.21563246154785, "Mean Q2": 31.215171890258787, "critic_loss": 37.0854338684082, "batch_reward": 0.825515625, "actor_loss": -32.703012771606446, "actor_target_entropy": -2.0, "actor_entropy": 0.8974212937355042, "alpha_loss": -0.06414268030971289, "alpha_value": 0.07891744892417527, "duration": 80.72853875160217, "step": 8500}
{"episode_reward": 758.0, "episode": 35.0, "Q1 loss": 21.398868209838867, "Q2 loss": 21.584707656860353, "Mean Target Q": 35.4844206237793, "Mean Q1": 35.461603118896484, "Mean Q2": 35.4619400100708, "critic_loss": 42.983575729370116, "batch_reward": 0.8841953125, "actor_loss": -37.20506838989258, "actor_target_entropy": -2.0, "actor_entropy": 0.8521173160076141, "alpha_loss": -0.10522490006685258, "alpha_value": 0.07961627115607671, "duration": 80.90941309928894, "step": 8750}
{"episode_reward": 721.0, "episode": 36.0, "Q1 loss": 24.52203036117554, "Q2 loss": 24.723425258636475, "Mean Target Q": 40.62886010742187, "Mean Q1": 40.61916030883789, "Mean Q2": 40.61956680297852, "critic_loss": 49.24545550537109, "batch_reward": 0.9568671875, "actor_loss": -42.9581955871582, "actor_target_entropy": -2.0, "actor_entropy": 0.8234747617244721, "alpha_loss": -0.13493052977323533, "alpha_value": 0.08063702383599522, "duration": 81.3227608203888, "step": 9000}
{"episode_reward": 865.0, "episode": 37.0, "Q1 loss": 27.903577396392823, "Q2 loss": 28.104056537628175, "Mean Target Q": 45.38510346984863, "Mean Q1": 45.35763316345215, "Mean Q2": 45.360146026611325, "critic_loss": 56.00763385772705, "batch_reward": 0.986953125, "actor_loss": -48.01372970581055, "actor_target_entropy": -2.0, "actor_entropy": 0.6879698112010956, "alpha_loss": -0.1381117381453514, "alpha_value": 0.08177415087516214, "duration": 80.68498945236206, "step": 9250}
{"episode_reward": 135.0, "episode": 38.0, "Q1 loss": 33.17175239562988, "Q2 loss": 33.17484248352051, "Mean Target Q": 48.91724313354492, "Mean Q1": 48.89516087341308, "Mean Q2": 48.892317123413086, "critic_loss": 66.34659495544433, "batch_reward": 0.9615703125, "actor_loss": -51.404146697998044, "actor_target_entropy": -2.0, "actor_entropy": 0.4688933690190315, "alpha_loss": -0.11092115026712418, "alpha_value": 0.0828409011482936, "duration": 80.80913472175598, "step": 9500}
{"episode_reward": 0.0, "episode": 39.0, "Q1 loss": 30.245428527832033, "Q2 loss": 30.419489486694335, "Mean Target Q": 52.94176145935059, "Mean Q1": 52.92444267272949, "Mean Q2": 52.92732914733887, "critic_loss": 60.66491795349121, "batch_reward": 0.972515625, "actor_loss": -55.34744287109375, "actor_target_entropy": -2.0, "actor_entropy": 0.3459045765325427, "alpha_loss": -0.10003486794233322, "alpha_value": 0.08369100145237633, "duration": 80.88506507873535, "step": 9750}
{"episode_reward": 687.0, "episode": 40.0, "Q1 loss": 26.670443618774414, "Q2 loss": 26.850381454467772, "Mean Target Q": 58.045072052001956, "Mean Q1": 58.02547637939453, "Mean Q2": 58.0226845703125, "critic_loss": 53.52082521057129, "batch_reward": 1.0237890625, "actor_loss": -60.57260910034179, "actor_target_entropy": -2.0, "actor_entropy": 0.35448981779813765, "alpha_loss": -0.10346156805753708, "alpha_value": 0.08458200680343489, "step": 10000}
{"duration": 105.47517371177673, "step": 10000}
{"episode_reward": 795.0, "episode": 41.0, "Q1 loss": 24.414001094818115, "Q2 loss": 24.54501333618164, "Mean Target Q": 62.31771905517578, "Mean Q1": 62.29801376342773, "Mean Q2": 62.30098681640625, "critic_loss": 48.95901449584961, "batch_reward": 1.050296875, "actor_loss": -64.73075918579102, "actor_target_entropy": -2.0, "actor_entropy": 0.36929917365312576, "alpha_loss": -0.10461952149868012, "alpha_value": 0.0855273074096802, "duration": 80.49575185775757, "step": 10250}
{"episode_reward": 298.0, "episode": 42.0, "Q1 loss": 22.08949949645996, "Q2 loss": 22.14438846588135, "Mean Target Q": 66.93321574401855, "Mean Q1": 66.91351612854004, "Mean Q2": 66.91403973388672, "critic_loss": 44.233888046264646, "batch_reward": 1.0943984375, "actor_loss": -69.05325567626953, "actor_target_entropy": -2.0, "actor_entropy": 0.37029757285118103, "alpha_loss": -0.09217165926098823, "alpha_value": 0.08642786552288341, "duration": 80.14541220664978, "step": 10500}
{"episode_reward": 933.0, "episode": 43.0, "Q1 loss": 20.476848484039305, "Q2 loss": 20.538872875213624, "Mean Target Q": 71.88289920043945, "Mean Q1": 71.86215768432618, "Mean Q2": 71.86373147583008, "critic_loss": 41.01572124481201, "batch_reward": 1.1607421875, "actor_loss": -73.93313452148438, "actor_target_entropy": -2.0, "actor_entropy": 0.4233919014930725, "alpha_loss": -0.08938743272423744, "alpha_value": 0.08728200178694873, "duration": 81.55307531356812, "step": 10750}
{"episode_reward": 929.0, "episode": 44.0, "Q1 loss": 20.727894603729247, "Q2 loss": 20.88373099899292, "Mean Target Q": 75.59766427612304, "Mean Q1": 75.58411303710938, "Mean Q2": 75.58176959228516, "critic_loss": 41.61162562561035, "batch_reward": 1.1779140625, "actor_loss": -77.36404510498046, "actor_target_entropy": -2.0, "actor_entropy": 0.43244982850551605, "alpha_loss": -0.08441259306669235, "alpha_value": 0.08812789906988375, "duration": 80.22710633277893, "step": 11000}
{"episode_reward": 467.0, "episode": 45.0, "Q1 loss": 20.754870971679686, "Q2 loss": 20.804218494415284, "Mean Target Q": 79.49476873779297, "Mean Q1": 79.47403866577149, "Mean Q2": 79.47375820922852, "critic_loss": 41.55908946228028, "batch_reward": 1.2161328125, "actor_loss": -81.14610009765624, "actor_target_entropy": -2.0, "actor_entropy": 0.44943211579322817, "alpha_loss": -0.07878166091442108, "alpha_value": 0.08896709395615977, "duration": 87.73922681808472, "step": 11250}
{"episode_reward": 846.0, "episode": 46.0, "Q1 loss": 19.80363822937012, "Q2 loss": 19.771045417785643, "Mean Target Q": 83.43262387084961, "Mean Q1": 83.42250436401368, "Mean Q2": 83.42393127441406, "critic_loss": 39.57468364715576, "batch_reward": 1.260515625, "actor_loss": -85.20283856201172, "actor_target_entropy": -2.0, "actor_entropy": 0.5017598142623901, "alpha_loss": -0.07925816687941552, "alpha_value": 0.08979038260086852, "duration": 81.06272053718567, "step": 11500}
{"episode_reward": 650.0, "episode": 47.0, "Q1 loss": 21.005273151397706, "Q2 loss": 21.128331062316896, "Mean Target Q": 86.43759506225587, "Mean Q1": 86.42461386108398, "Mean Q2": 86.42533380126953, "critic_loss": 42.13360412597656, "batch_reward": 1.2722578125, "actor_loss": -88.24441070556641, "actor_target_entropy": -2.0, "actor_entropy": 0.5406632779836654, "alpha_loss": -0.07846716395020485, "alpha_value": 0.09065148686061551, "duration": 81.08665537834167, "step": 11750}
{"episode_reward": 450.0, "episode": 48.0, "Q1 loss": 21.39238238143921, "Q2 loss": 21.48950269317627, "Mean Target Q": 89.97400534057617, "Mean Q1": 89.9641291809082, "Mean Q2": 89.9646877746582, "critic_loss": 42.88188498687744, "batch_reward": 1.30421875, "actor_loss": -91.57384930419921, "actor_target_entropy": -2.0, "actor_entropy": 0.48018684244155885, "alpha_loss": -0.0783873301744461, "alpha_value": 0.09151619959739246, "duration": 80.37863087654114, "step": 12000}
{"episode_reward": 893.0, "episode": 49.0, "Q1 loss": 19.439082836151123, "Q2 loss": 19.627132511138917, "Mean Target Q": 94.13756521606446, "Mean Q1": 94.12309274291992, "Mean Q2": 94.1229443359375, "critic_loss": 39.066215438842775, "batch_reward": 1.3592578125, "actor_loss": -95.99942028808594, "actor_target_entropy": -2.0, "actor_entropy": 0.472765615940094, "alpha_loss": -0.07970785722136497, "alpha_value": 0.09244452422508662, "duration": 80.52631282806396, "step": 12250}
{"episode_reward": 914.0, "episode": 50.0, "Q1 loss": 18.403916439056395, "Q2 loss": 18.55962352371216, "Mean Target Q": 98.09786016845703, "Mean Q1": 98.08957452392578, "Mean Q2": 98.08947018432617, "critic_loss": 36.9635400466919, "batch_reward": 1.401203125, "actor_loss": -99.86978009033203, "actor_target_entropy": -2.0, "actor_entropy": 0.4783955216407776, "alpha_loss": -0.08217900490760803, "alpha_value": 0.09342297724635043, "duration": 80.1433653831482, "step": 12500}
{"episode_reward": 946.0, "episode": 51.0, "Q1 loss": 19.00608917617798, "Q2 loss": 19.153405780792237, "Mean Target Q": 102.01345877075195, "Mean Q1": 102.00113302612304, "Mean Q2": 102.00200186157227, "critic_loss": 38.15949504089355, "batch_reward": 1.43896875, "actor_loss": -103.73519018554687, "actor_target_entropy": -2.0, "actor_entropy": 0.49839921402931214, "alpha_loss": -0.07368702217936515, "alpha_value": 0.09436802062143099, "duration": 79.59684419631958, "step": 12750}
{"episode_reward": 757.0, "episode": 52.0, "Q1 loss": 21.17134577178955, "Q2 loss": 21.205184089660644, "Mean Target Q": 105.73106622314454, "Mean Q1": 105.7184730834961, "Mean Q2": 105.71879473876953, "critic_loss": 42.37652983093262, "batch_reward": 1.4642265625, "actor_loss": -107.74926831054688, "actor_target_entropy": -2.0, "actor_entropy": 0.5122872536182403, "alpha_loss": -0.08192858132719993, "alpha_value": 0.09537367948175665, "duration": 79.68419122695923, "step": 13000}
{"episode_reward": 613.0, "episode": 53.0, "Q1 loss": 22.245484104156493, "Q2 loss": 22.43802613449097, "Mean Target Q": 109.30474575805664, "Mean Q1": 109.29283810424805, "Mean Q2": 109.29273364257813, "critic_loss": 44.68351030731201, "batch_reward": 1.48490625, "actor_loss": -111.32995581054688, "actor_target_entropy": -2.0, "actor_entropy": 0.4310949915647507, "alpha_loss": -0.08087519124150276, "alpha_value": 0.09641004082716603, "duration": 96.25137305259705, "step": 13250}
{"episode_reward": 514.0, "episode": 54.0, "Q1 loss": 20.830565284729005, "Q2 loss": 20.84592548751831, "Mean Target Q": 112.98825790405273, "Mean Q1": 112.9767451171875, "Mean Q2": 112.97907864379883, "critic_loss": 41.676490829467774, "batch_reward": 1.5128671875, "actor_loss": -114.86640692138671, "actor_target_entropy": -2.0, "actor_entropy": 0.45941917526721954, "alpha_loss": -0.07767048172652721, "alpha_value": 0.09748785744522413, "duration": 125.97356057167053, "step": 13500}
{"episode_reward": 832.0, "episode": 55.0, "Q1 loss": 21.672593627929686, "Q2 loss": 21.838397918701173, "Mean Target Q": 116.94556921386719, "Mean Q1": 116.93616360473632, "Mean Q2": 116.93807913208008, "critic_loss": 43.51099154663086, "batch_reward": 1.54365625, "actor_loss": -118.92606805419922, "actor_target_entropy": -2.0, "actor_entropy": 0.3611708035618067, "alpha_loss": -0.07380243283510209, "alpha_value": 0.09852412388850013, "duration": 113.74290800094604, "step": 13750}
{"episode_reward": 776.0, "episode": 56.0, "Q1 loss": 22.86305330657959, "Q2 loss": 22.894285610198974, "Mean Target Q": 120.55011975097656, "Mean Q1": 120.53923541259766, "Mean Q2": 120.5388447265625, "critic_loss": 45.75733891296387, "batch_reward": 1.557140625, "actor_loss": -122.26910095214843, "actor_target_entropy": -2.0, "actor_entropy": 0.368742082118988, "alpha_loss": -0.07420418256521225, "alpha_value": 0.09955343764204588, "duration": 82.81260228157043, "step": 14000}
{"episode_reward": 493.0, "episode": 57.0, "Q1 loss": 21.734447448730467, "Q2 loss": 21.67727834701538, "Mean Target Q": 124.18057125854492, "Mean Q1": 124.16914309692383, "Mean Q2": 124.16981396484375, "critic_loss": 43.41172575378418, "batch_reward": 1.573984375, "actor_loss": -126.39790637207031, "actor_target_entropy": -2.0, "actor_entropy": 0.33355525723844764, "alpha_loss": -0.07048469573259354, "alpha_value": 0.10059788841924362, "duration": 82.25164151191711, "step": 14250}
{"episode_reward": 805.0, "episode": 58.0, "Q1 loss": 22.626957958221436, "Q2 loss": 22.6376015625, "Mean Target Q": 127.36214660644531, "Mean Q1": 127.35017596435547, "Mean Q2": 127.35097027587891, "critic_loss": 45.2645595703125, "batch_reward": 1.5809375, "actor_loss": -129.3012622680664, "actor_target_entropy": -2.0, "actor_entropy": 0.3736752701997757, "alpha_loss": -0.06678446394205094, "alpha_value": 0.10164074549918882, "duration": 83.35262799263, "step": 14500}
{"episode_reward": 510.0, "episode": 59.0, "Q1 loss": 21.28075001144409, "Q2 loss": 21.36533893585205, "Mean Target Q": 131.00996084594726, "Mean Q1": 131.0008596496582, "Mean Q2": 131.00236047363282, "critic_loss": 42.64608891296387, "batch_reward": 1.6093203125, "actor_loss": -132.56861932373047, "actor_target_entropy": -2.0, "actor_entropy": 0.3843811318874359, "alpha_loss": -0.06235548111796379, "alpha_value": 0.1025982931764589, "duration": 82.44196963310242, "step": 14750}
{"episode_reward": 922.0, "episode": 60.0, "Q1 loss": 20.3016270904541, "Q2 loss": 20.435823360443116, "Mean Target Q": 134.81297570800783, "Mean Q1": 134.80565002441406, "Mean Q2": 134.80383319091797, "critic_loss": 40.73745043945313, "batch_reward": 1.653421875, "actor_loss": -136.39281469726564, "actor_target_entropy": -2.0, "actor_entropy": 0.39282040667533874, "alpha_loss": -0.05166411926597357, "alpha_value": 0.10350647620851725, "step": 15000}
{"duration": 142.86365032196045, "step": 15000}
{"episode_reward": 824.0, "episode": 61.0, "Q1 loss": 18.82359574890137, "Q2 loss": 18.859692584991453, "Mean Target Q": 137.65301165771484, "Mean Q1": 137.63909338378906, "Mean Q2": 137.64104779052735, "critic_loss": 37.68328835296631, "batch_reward": 1.6592890625, "actor_loss": -139.2059716796875, "actor_target_entropy": -2.0, "actor_entropy": 0.4214476801156998, "alpha_loss": -0.045044832311570644, "alpha_value": 0.10430451657273751, "duration": 95.67468214035034, "step": 15250}
{"episode_reward": 786.0, "episode": 62.0, "Q1 loss": 18.662994049072264, "Q2 loss": 18.690300586700438, "Mean Target Q": 141.53978271484374, "Mean Q1": 141.53112420654296, "Mean Q2": 141.53107781982422, "critic_loss": 37.35329473114014, "batch_reward": 1.70846875, "actor_loss": -143.20624450683593, "actor_target_entropy": -2.0, "actor_entropy": 0.39505159270763396, "alpha_loss": -0.04366001608222723, "alpha_value": 0.1050963277861248, "duration": 88.4148964881897, "step": 15500}
{"episode_reward": 977.0, "episode": 63.0, "Q1 loss": 17.174436485290528, "Q2 loss": 17.422075958251952, "Mean Target Q": 144.86440771484374, "Mean Q1": 144.8569270629883, "Mean Q2": 144.85831549072265, "critic_loss": 34.596512435913084, "batch_reward": 1.736453125, "actor_loss": -146.23391223144532, "actor_target_entropy": -2.0, "actor_entropy": 0.37308026289939883, "alpha_loss": -0.03711492667533457, "alpha_value": 0.10580421747779746, "duration": 82.06558036804199, "step": 15750}
{"episode_reward": 902.0, "episode": 64.0, "Q1 loss": 16.785397830963134, "Q2 loss": 16.854338745117186, "Mean Target Q": 148.35259942626953, "Mean Q1": 148.34036602783203, "Mean Q2": 148.33840911865235, "critic_loss": 33.63973656463623, "batch_reward": 1.76996875, "actor_loss": -149.87321765136718, "actor_target_entropy": -2.0, "actor_entropy": 0.4005571769475937, "alpha_loss": -0.03339351642597467, "alpha_value": 0.10648706197250718, "duration": 82.75048065185547, "step": 16000}
{"episode_reward": 882.0, "episode": 65.0, "Q1 loss": 17.443406200408937, "Q2 loss": 17.623468814849854, "Mean Target Q": 151.32281799316405, "Mean Q1": 151.31230407714844, "Mean Q2": 151.31194494628906, "critic_loss": 35.06687503051758, "batch_reward": 1.7906171875, "actor_loss": -152.42414770507813, "actor_target_entropy": -2.0, "actor_entropy": 0.4013234714269638, "alpha_loss": -0.027444625639356672, "alpha_value": 0.10710031686066407, "duration": 83.4562463760376, "step": 16250}
{"episode_reward": 882.0, "episode": 66.0, "Q1 loss": 17.934416564941408, "Q2 loss": 18.1295744972229, "Mean Target Q": 154.53303851318358, "Mean Q1": 154.5248996582031, "Mean Q2": 154.52462731933593, "critic_loss": 36.06399105072021, "batch_reward": 1.8099375, "actor_loss": -155.7306834716797, "actor_target_entropy": -2.0, "actor_entropy": 0.4257521759271622, "alpha_loss": -0.029033250361680984, "alpha_value": 0.1076599947443509, "duration": 151.72919511795044, "step": 16500}
{"episode_reward": 643.0, "episode": 67.0, "Q1 loss": 17.77900395965576, "Q2 loss": 17.689317276000978, "Mean Target Q": 157.39072680664063, "Mean Q1": 157.3806167602539, "Mean Q2": 157.38026165771484, "critic_loss": 35.46832122802734, "batch_reward": 1.829078125, "actor_loss": -158.80560498046876, "actor_target_entropy": -2.0, "actor_entropy": 0.3790805642604828, "alpha_loss": -0.028698033811524512, "alpha_value": 0.1083094899446106, "duration": 82.67705178260803, "step": 16750}
{"episode_reward": 813.0, "episode": 68.0, "Q1 loss": 16.717873008728027, "Q2 loss": 16.87696799468994, "Mean Target Q": 160.50139001464845, "Mean Q1": 160.49481396484376, "Mean Q2": 160.49566662597655, "critic_loss": 33.594841049194336, "batch_reward": 1.8577109375, "actor_loss": -161.86422143554688, "actor_target_entropy": -2.0, "actor_entropy": 0.3940723178386688, "alpha_loss": -0.030901677949354053, "alpha_value": 0.10904513255551664, "duration": 84.12839913368225, "step": 17000}
{"episode_reward": 891.0, "episode": 69.0, "Q1 loss": 16.991393409729003, "Q2 loss": 17.000091186523438, "Mean Target Q": 163.2826071166992, "Mean Q1": 163.2766448364258, "Mean Q2": 163.275712890625, "critic_loss": 33.99148464202881, "batch_reward": 1.8652734375, "actor_loss": -164.58947119140626, "actor_target_entropy": -2.0, "actor_entropy": 0.4593249069452286, "alpha_loss": -0.030220257855951786, "alpha_value": 0.10978347171621859, "duration": 83.4881820678711, "step": 17250}
{"episode_reward": 834.0, "episode": 70.0, "Q1 loss": 17.40637145614624, "Q2 loss": 17.34438814544678, "Mean Target Q": 166.66450415039063, "Mean Q1": 166.65270031738282, "Mean Q2": 166.65309411621095, "critic_loss": 34.75075963592529, "batch_reward": 1.89934375, "actor_loss": -168.08913684082032, "actor_target_entropy": -2.0, "actor_entropy": 0.38938956379890444, "alpha_loss": -0.02793326784297824, "alpha_value": 0.1105280325381369, "duration": 84.75346684455872, "step": 17500}
{"episode_reward": 833.0, "episode": 71.0, "Q1 loss": 16.962166061401366, "Q2 loss": 17.117943111419677, "Mean Target Q": 169.6154754638672, "Mean Q1": 169.61210693359374, "Mean Q2": 169.61214837646483, "critic_loss": 34.08010903930664, "batch_reward": 1.914234375, "actor_loss": -171.3763408203125, "actor_target_entropy": -2.0, "actor_entropy": 0.400113543510437, "alpha_loss": -0.027585941763594746, "alpha_value": 0.11124340070696648, "duration": 83.68851637840271, "step": 17750}
{"episode_reward": 863.0, "episode": 72.0, "Q1 loss": 15.567802898406983, "Q2 loss": 15.619890171051026, "Mean Target Q": 172.5954104614258, "Mean Q1": 172.58505596923828, "Mean Q2": 172.58621752929687, "critic_loss": 31.187693016052247, "batch_reward": 1.9358125, "actor_loss": -174.0872117919922, "actor_target_entropy": -2.0, "actor_entropy": 0.47021754693984985, "alpha_loss": -0.028724694872274994, "alpha_value": 0.11204576387868913, "duration": 148.4675488471985, "step": 18000}
{"episode_reward": 948.0, "episode": 73.0, "Q1 loss": 16.169130744934083, "Q2 loss": 16.285474140167235, "Mean Target Q": 175.75384387207032, "Mean Q1": 175.7480810546875, "Mean Q2": 175.74785766601562, "critic_loss": 32.45460483551025, "batch_reward": 1.971046875, "actor_loss": -177.25298876953124, "actor_target_entropy": -2.0, "actor_entropy": 0.5424795322418213, "alpha_loss": -0.02184772093221545, "alpha_value": 0.11279799849582889, "duration": 82.37381958961487, "step": 18250}
{"episode_reward": 867.0, "episode": 74.0, "Q1 loss": 14.777810703277588, "Q2 loss": 14.761845211029053, "Mean Target Q": 178.7359282836914, "Mean Q1": 178.727865234375, "Mean Q2": 178.7289110107422, "critic_loss": 29.539655906677247, "batch_reward": 1.98940625, "actor_loss": -180.00039526367186, "actor_target_entropy": -2.0, "actor_entropy": 0.5039123296737671, "alpha_loss": -0.016631947947666047, "alpha_value": 0.11334765947993751, "duration": 82.12692856788635, "step": 18500}
{"episode_reward": 930.0, "episode": 75.0, "Q1 loss": 14.42005156326294, "Q2 loss": 14.500805767059326, "Mean Target Q": 181.66384130859376, "Mean Q1": 181.65606048583984, "Mean Q2": 181.6557188720703, "critic_loss": 28.92085743713379, "batch_reward": 2.0107578125, "actor_loss": -183.19455834960937, "actor_target_entropy": -2.0, "actor_entropy": 0.5054358036518097, "alpha_loss": -0.01238616627920419, "alpha_value": 0.11379308808096691, "duration": 82.52433562278748, "step": 18750}
{"episode_reward": 860.0, "episode": 76.0, "Q1 loss": 14.340523620605468, "Q2 loss": 14.39111672592163, "Mean Target Q": 184.35333111572265, "Mean Q1": 184.34304620361328, "Mean Q2": 184.3437069091797, "critic_loss": 28.731640380859375, "batch_reward": 2.0270625, "actor_loss": -185.46289392089844, "actor_target_entropy": -2.0, "actor_entropy": 0.5586148934364319, "alpha_loss": -0.0090305303838104, "alpha_value": 0.11413869543611813, "duration": 82.62623238563538, "step": 19000}
{"episode_reward": 688.0, "episode": 77.0, "Q1 loss": 13.68248109817505, "Q2 loss": 13.758676670074463, "Mean Target Q": 187.04634674072267, "Mean Q1": 187.0389497680664, "Mean Q2": 187.03856072998047, "critic_loss": 27.44115779876709, "batch_reward": 2.04553125, "actor_loss": -188.39640234375, "actor_target_entropy": -2.0, "actor_entropy": 0.544214983701706, "alpha_loss": 0.0023375989217311144, "alpha_value": 0.11438333443461056, "duration": 90.46283316612244, "step": 19250}
{"episode_reward": 916.0, "episode": 78.0, "Q1 loss": 13.295287923812866, "Q2 loss": 13.298762784957885, "Mean Target Q": 189.46074896240233, "Mean Q1": 189.45648461914064, "Mean Q2": 189.45556286621093, "critic_loss": 26.594050758361817, "batch_reward": 2.051078125, "actor_loss": -190.8037989501953, "actor_target_entropy": -2.0, "actor_entropy": 0.5006283974647522, "alpha_loss": 0.004101014737039804, "alpha_value": 0.11421981637408957, "duration": 147.70917987823486, "step": 19500}
{"episode_reward": 876.0, "episode": 79.0, "Q1 loss": 12.92242194366455, "Q2 loss": 13.07734628868103, "Mean Target Q": 192.12431158447265, "Mean Q1": 192.11648175048828, "Mean Q2": 192.11800939941406, "critic_loss": 25.999768241882325, "batch_reward": 2.0751015625, "actor_loss": -193.5561072998047, "actor_target_entropy": -2.0, "actor_entropy": 0.48128607964515685, "alpha_loss": 0.011224481102079154, "alpha_value": 0.11390018481520797, "duration": 82.65450239181519, "step": 19750}
{"episode_reward": 815.0, "episode": 80.0, "Q1 loss": 13.616813623428344, "Q2 loss": 13.679198350906372, "Mean Target Q": 194.67457934570314, "Mean Q1": 194.66578125, "Mean Q2": 194.6653564453125, "critic_loss": 27.2960119972229, "batch_reward": 2.08796875, "actor_loss": -195.92700317382813, "actor_target_entropy": -2.0, "actor_entropy": 0.44509366106987, "alpha_loss": 0.0020404206104576586, "alpha_value": 0.11362133357956271, "step": 20000}
{"duration": 108.34090995788574, "step": 20000}
{"episode_reward": 878.0, "episode": 81.0, "Q1 loss": 12.878516189575196, "Q2 loss": 12.955179656982422, "Mean Target Q": 197.36419158935547, "Mean Q1": 197.35668951416017, "Mean Q2": 197.3574497680664, "critic_loss": 25.833695930480957, "batch_reward": 2.1101953125, "actor_loss": -198.88287255859376, "actor_target_entropy": -2.0, "actor_entropy": 0.4966159257888794, "alpha_loss": 0.008692820632830263, "alpha_value": 0.11345071601503649, "duration": 83.859135389328, "step": 20250}
{"episode_reward": 947.0, "episode": 82.0, "Q1 loss": 13.054879064559936, "Q2 loss": 13.22289748954773, "Mean Target Q": 199.97052032470702, "Mean Q1": 199.96707928466796, "Mean Q2": 199.96693383789062, "critic_loss": 26.277776512145998, "batch_reward": 2.132046875, "actor_loss": -201.27958142089844, "actor_target_entropy": -2.0, "actor_entropy": 0.47075374937057496, "alpha_loss": 0.0072027515396475795, "alpha_value": 0.1130850095877009, "duration": 82.98257160186768, "step": 20500}
{"episode_reward": 940.0, "episode": 83.0, "Q1 loss": 12.740907863616943, "Q2 loss": 12.745368309020996, "Mean Target Q": 202.9063306274414, "Mean Q1": 202.89697772216797, "Mean Q2": 202.8981655883789, "critic_loss": 25.486276165008544, "batch_reward": 2.163359375, "actor_loss": -204.18104931640624, "actor_target_entropy": -2.0, "actor_entropy": 0.4774010195732117, "alpha_loss": 0.004625933298841119, "alpha_value": 0.11278613544046256, "duration": 123.71921181678772, "step": 20750}
{"episode_reward": 931.0, "episode": 84.0, "Q1 loss": 11.901627132415772, "Q2 loss": 11.933109683990478, "Mean Target Q": 205.25014221191407, "Mean Q1": 205.24912591552734, "Mean Q2": 205.24796862792968, "critic_loss": 23.83473680114746, "batch_reward": 2.1689140625, "actor_loss": -206.56637658691406, "actor_target_entropy": -2.0, "actor_entropy": 0.5407519981861114, "alpha_loss": 0.011908519946038722, "alpha_value": 0.11241709833123503, "duration": 106.25832223892212, "step": 21000}
{"episode_reward": 987.0, "episode": 85.0, "Q1 loss": 11.658082923889161, "Q2 loss": 11.711134630203247, "Mean Target Q": 207.99539959716796, "Mean Q1": 207.98566278076171, "Mean Q2": 207.98711090087892, "critic_loss": 23.36921754837036, "batch_reward": 2.1988359375, "actor_loss": -209.1827635498047, "actor_target_entropy": -2.0, "actor_entropy": 0.5159121856689454, "alpha_loss": 0.015559198878705502, "alpha_value": 0.11169036092319044, "duration": 81.92477345466614, "step": 21250}
{"episode_reward": 989.0, "episode": 86.0, "Q1 loss": 11.158952529907227, "Q2 loss": 11.159294145584106, "Mean Target Q": 210.36611724853515, "Mean Q1": 210.3610347290039, "Mean Q2": 210.36150451660157, "critic_loss": 22.31824669265747, "batch_reward": 2.2083359375, "actor_loss": -211.83350732421874, "actor_target_entropy": -2.0, "actor_entropy": 0.47612030267715455, "alpha_loss": 0.016354283043183385, "alpha_value": 0.1108696011362567, "duration": 81.95165061950684, "step": 21500}
{"episode_reward": 888.0, "episode": 87.0, "Q1 loss": 10.998332033157348, "Q2 loss": 11.09812093925476, "Mean Target Q": 212.86393377685548, "Mean Q1": 212.85862438964844, "Mean Q2": 212.85892889404298, "critic_loss": 22.09645299911499, "batch_reward": 2.22109375, "actor_loss": -214.23161279296875, "actor_target_entropy": -2.0, "actor_entropy": 0.5251879799365997, "alpha_loss": 0.014128110240213572, "alpha_value": 0.11007223915686122, "duration": 82.8654191493988, "step": 21750}
{"episode_reward": 956.0, "episode": 88.0, "Q1 loss": 11.120746934890747, "Q2 loss": 11.169879123687744, "Mean Target Q": 215.2011563720703, "Mean Q1": 215.19511541748048, "Mean Q2": 215.19498919677736, "critic_loss": 22.290626068115234, "batch_reward": 2.228453125, "actor_loss": -216.65659338378907, "actor_target_entropy": -2.0, "actor_entropy": 0.4961502764225006, "alpha_loss": 0.0103211729247123, "alpha_value": 0.10939545871679192, "duration": 82.58127927780151, "step": 22000}
{"episode_reward": 942.0, "episode": 89.0, "Q1 loss": 10.673038347244262, "Q2 loss": 10.73443544960022, "Mean Target Q": 217.96853955078126, "Mean Q1": 217.964056640625, "Mean Q2": 217.96446301269532, "critic_loss": 21.40747379684448, "batch_reward": 2.2617578125, "actor_loss": -219.4207744140625, "actor_target_entropy": -2.0, "actor_entropy": 0.553928941488266, "alpha_loss": 0.014726717736572028, "alpha_value": 0.10878829864976455, "duration": 125.12566375732422, "step": 22250}
{"episode_reward": 950.0, "episode": 90.0, "Q1 loss": 10.410379135131835, "Q2 loss": 10.464104740142822, "Mean Target Q": 220.3774012451172, "Mean Q1": 220.37149676513673, "Mean Q2": 220.37159857177735, "critic_loss": 20.87448388290405, "batch_reward": 2.2708125, "actor_loss": -221.61515124511718, "actor_target_entropy": -2.0, "actor_entropy": 0.5280004103183746, "alpha_loss": 0.01760765598528087, "alpha_value": 0.10782358919773406, "duration": 101.43403387069702, "step": 22500}
{"episode_reward": 839.0, "episode": 91.0, "Q1 loss": 9.974373916625977, "Q2 loss": 9.94398113822937, "Mean Target Q": 222.62567669677733, "Mean Q1": 222.6165662841797, "Mean Q2": 222.61681854248047, "critic_loss": 19.918355102539063, "batch_reward": 2.2822265625, "actor_loss": -223.99294885253906, "actor_target_entropy": -2.0, "actor_entropy": 0.5509119417667389, "alpha_loss": 0.017910998973995448, "alpha_value": 0.1068784329352393, "duration": 82.16595959663391, "step": 22750}
{"episode_reward": 936.0, "episode": 92.0, "Q1 loss": 9.124888900756837, "Q2 loss": 9.199615358352661, "Mean Target Q": 225.16276354980468, "Mean Q1": 225.15987481689453, "Mean Q2": 225.1601957397461, "critic_loss": 18.324504261016845, "batch_reward": 2.304125, "actor_loss": -226.44065905761718, "actor_target_entropy": -2.0, "actor_entropy": 0.5273110225200653, "alpha_loss": 0.017152417065575718, "alpha_value": 0.10593203332341432, "duration": 85.73427295684814, "step": 23000}
{"episode_reward": 958.0, "episode": 93.0, "Q1 loss": 8.692436820983886, "Q2 loss": 8.845965497970582, "Mean Target Q": 227.74805700683595, "Mean Q1": 227.74244677734376, "Mean Q2": 227.74253106689454, "critic_loss": 17.538402332305907, "batch_reward": 2.3331328125, "actor_loss": -229.04401110839845, "actor_target_entropy": -2.0, "actor_entropy": 0.5000378208160401, "alpha_loss": 0.01777936638984829, "alpha_value": 0.1048747640146898, "duration": 84.86074328422546, "step": 23250}
{"episode_reward": 956.0, "episode": 94.0, "Q1 loss": 8.902838802337646, "Q2 loss": 8.959989883422852, "Mean Target Q": 229.958169921875, "Mean Q1": 229.95375079345703, "Mean Q2": 229.9546912841797, "critic_loss": 17.862828685760498, "batch_reward": 2.3450859375, "actor_loss": -231.1537265625, "actor_target_entropy": -2.0, "actor_entropy": 0.4951581783294678, "alpha_loss": 0.018776627900078894, "alpha_value": 0.10393057120227002, "duration": 85.97606587409973, "step": 23500}
{"episode_reward": 837.0, "episode": 95.0, "Q1 loss": 8.700931312561035, "Q2 loss": 8.724477910995484, "Mean Target Q": 231.98404205322265, "Mean Q1": 231.9783180541992, "Mean Q2": 231.9776817626953, "critic_loss": 17.42540923690796, "batch_reward": 2.3487890625, "actor_loss": -233.23667944335938, "actor_target_entropy": -2.0, "actor_entropy": 0.5401968765258789, "alpha_loss": 0.02203613626305014, "alpha_value": 0.10286163328355022, "duration": 157.34143900871277, "step": 23750}
{"episode_reward": 948.0, "episode": 96.0, "Q1 loss": 8.161558462142944, "Q2 loss": 8.275713899612427, "Mean Target Q": 234.3570557861328, "Mean Q1": 234.3536405029297, "Mean Q2": 234.35304357910155, "critic_loss": 16.437272354125977, "batch_reward": 2.3749921875, "actor_loss": -235.5313642578125, "actor_target_entropy": -2.0, "actor_entropy": 0.5585449376106262, "alpha_loss": 0.01944926668331027, "alpha_value": 0.10166991794578152, "duration": 85.51786422729492, "step": 24000}
{"episode_reward": 924.0, "episode": 97.0, "Q1 loss": 7.969809999465943, "Q2 loss": 7.9698882522583006, "Mean Target Q": 236.24640454101564, "Mean Q1": 236.24036779785158, "Mean Q2": 236.2409898071289, "critic_loss": 15.939698223114014, "batch_reward": 2.371453125, "actor_loss": -237.25090881347657, "actor_target_entropy": -2.0, "actor_entropy": 0.5451339495182037, "alpha_loss": 0.017599606720730663, "alpha_value": 0.10071558511762577, "duration": 86.37975072860718, "step": 24250}
{"episode_reward": 944.0, "episode": 98.0, "Q1 loss": 7.710901578903198, "Q2 loss": 7.769030319213867, "Mean Target Q": 238.3568267211914, "Mean Q1": 238.35337658691407, "Mean Q2": 238.35291326904297, "critic_loss": 15.479931938171386, "batch_reward": 2.3844609375, "actor_loss": -239.43799670410155, "actor_target_entropy": -2.0, "actor_entropy": 0.5527280349731445, "alpha_loss": 0.016072268974035978, "alpha_value": 0.0997885767215196, "duration": 85.58248496055603, "step": 24500}
{"episode_reward": 939.0, "episode": 99.0, "Q1 loss": 7.700552717208862, "Q2 loss": 7.782280595779419, "Mean Target Q": 240.8605484008789, "Mean Q1": 240.85459460449218, "Mean Q2": 240.85609362792968, "critic_loss": 15.482833305358886, "batch_reward": 2.4211640625, "actor_loss": -242.28346655273438, "actor_target_entropy": -2.0, "actor_entropy": 0.5583900279998779, "alpha_loss": 0.018739658929407595, "alpha_value": 0.09895484120051724, "duration": 84.69358682632446, "step": 24750}
{"episode_reward": 785.0, "episode": 100.0, "Q1 loss": 7.516015073776245, "Q2 loss": 7.6602660770416255, "Mean Target Q": 242.6360248413086, "Mean Q1": 242.63097967529296, "Mean Q2": 242.63137188720702, "critic_loss": 15.17628112411499, "batch_reward": 2.4109765625, "actor_loss": -243.69352697753905, "actor_target_entropy": -2.0, "actor_entropy": 0.6458158690929413, "alpha_loss": 0.01586542724445462, "alpha_value": 0.09800164685439818, "step": 25000}
{"duration": 129.53933763504028, "step": 25000}
{"episode_reward": 954.0, "episode": 101.0, "Q1 loss": 7.173099933624267, "Q2 loss": 7.210433124542236, "Mean Target Q": 244.72705609130858, "Mean Q1": 244.7233325805664, "Mean Q2": 244.723037109375, "critic_loss": 14.383533058166504, "batch_reward": 2.4304609375, "actor_loss": -245.8850655517578, "actor_target_entropy": -2.0, "actor_entropy": 0.5577741580009461, "alpha_loss": 0.012838434136472643, "alpha_value": 0.09719790247837469, "duration": 120.84598970413208, "step": 25250}
{"episode_reward": 941.0, "episode": 102.0, "Q1 loss": 7.167861667633057, "Q2 loss": 7.184688951492309, "Mean Target Q": 246.63795782470703, "Mean Q1": 246.6337424926758, "Mean Q2": 246.63292626953125, "critic_loss": 14.352550601959228, "batch_reward": 2.437265625, "actor_loss": -247.7856346435547, "actor_target_entropy": -2.0, "actor_entropy": 0.5907591965198516, "alpha_loss": 0.017795745247974993, "alpha_value": 0.09644286122743934, "duration": 84.78865146636963, "step": 25500}
{"episode_reward": 938.0, "episode": 103.0, "Q1 loss": 6.628118190765381, "Q2 loss": 6.669550130844116, "Mean Target Q": 248.70132153320313, "Mean Q1": 248.69869921875, "Mean Q2": 248.6986517944336, "critic_loss": 13.297668354034425, "batch_reward": 2.4566328125, "actor_loss": -249.86739624023437, "actor_target_entropy": -2.0, "actor_entropy": 0.5879355897903442, "alpha_loss": 0.0193149604909122, "alpha_value": 0.09541576130997338, "duration": 85.14460897445679, "step": 25750}
{"episode_reward": 915.0, "episode": 104.0, "Q1 loss": 6.703010929107666, "Q2 loss": 6.7406594257354735, "Mean Target Q": 250.2575396118164, "Mean Q1": 250.2508959350586, "Mean Q2": 250.25250524902344, "critic_loss": 13.443670356750488, "batch_reward": 2.453125, "actor_loss": -251.45607861328125, "actor_target_entropy": -2.0, "actor_entropy": 0.4645624942779541, "alpha_loss": 0.02094473716802895, "alpha_value": 0.09437183843133189, "duration": 85.56471633911133, "step": 26000}
{"episode_reward": 983.0, "episode": 105.0, "Q1 loss": 6.403661515235901, "Q2 loss": 6.371624378204346, "Mean Target Q": 252.47484967041015, "Mean Q1": 252.47177471923828, "Mean Q2": 252.47127990722657, "critic_loss": 12.775285865783692, "batch_reward": 2.484640625, "actor_loss": -253.60112329101563, "actor_target_entropy": -2.0, "actor_entropy": 0.48781487536430357, "alpha_loss": 0.021097449482418595, "alpha_value": 0.09329592866712762, "duration": 84.03643894195557, "step": 26250}
{"episode_reward": 937.0, "episode": 106.0, "Q1 loss": 6.118831205368042, "Q2 loss": 6.1972684516906735, "Mean Target Q": 254.32911486816405, "Mean Q1": 254.32598822021484, "Mean Q2": 254.32513397216798, "critic_loss": 12.316099700927735, "batch_reward": 2.496265625, "actor_loss": -255.4150361328125, "actor_target_entropy": -2.0, "actor_entropy": 0.49712276363372804, "alpha_loss": 0.02185932750441134, "alpha_value": 0.09220926551465614, "duration": 135.4564118385315, "step": 26500}
{"episode_reward": 957.0, "episode": 107.0, "Q1 loss": 5.906378384590149, "Q2 loss": 6.020669472694397, "Mean Target Q": 256.1973825073242, "Mean Q1": 256.19327288818357, "Mean Q2": 256.1943457641602, "critic_loss": 11.927047845840454, "batch_reward": 2.5078671875, "actor_loss": -257.28494348144534, "actor_target_entropy": -2.0, "actor_entropy": 0.4829325966835022, "alpha_loss": 0.021204134359955787, "alpha_value": 0.09109553679216816, "duration": 100.96680521965027, "step": 26750}
{"episode_reward": 953.0, "episode": 108.0, "Q1 loss": 6.022742264747619, "Q2 loss": 6.028804547309876, "Mean Target Q": 258.06071044921873, "Mean Q1": 258.0583355102539, "Mean Q2": 258.05842889404295, "critic_loss": 12.051546808242797, "batch_reward": 2.524375, "actor_loss": -259.32865588378905, "actor_target_entropy": -2.0, "actor_entropy": 0.5858151853084564, "alpha_loss": 0.02523101817816496, "alpha_value": 0.09004986366212267, "duration": 87.57783961296082, "step": 27000}
{"episode_reward": 950.0, "episode": 109.0, "Q1 loss": 5.893116901397705, "Q2 loss": 5.879295525550842, "Mean Target Q": 259.7886120605469, "Mean Q1": 259.783646484375, "Mean Q2": 259.78368963623046, "critic_loss": 11.772412427902221, "batch_reward": 2.5306015625, "actor_loss": -260.87181274414064, "actor_target_entropy": -2.0, "actor_entropy": 0.4894098489284515, "alpha_loss": 0.024249198423698545, "alpha_value": 0.0888765670072172, "duration": 86.11795330047607, "step": 27250}
{"episode_reward": 953.0, "episode": 110.0, "Q1 loss": 5.597237670898438, "Q2 loss": 5.679036703109741, "Mean Target Q": 261.55345922851564, "Mean Q1": 261.5498413085937, "Mean Q2": 261.54928759765625, "critic_loss": 11.276274366378784, "batch_reward": 2.5435, "actor_loss": -262.6393352050781, "actor_target_entropy": -2.0, "actor_entropy": 0.5301059536933899, "alpha_loss": 0.03125448730960488, "alpha_value": 0.0876162254523007, "duration": 93.98079943656921, "step": 27500}
{"episode_reward": 952.0, "episode": 111.0, "Q1 loss": 5.94707049369812, "Q2 loss": 5.944066060066223, "Mean Target Q": 263.23381494140625, "Mean Q1": 263.22793383789065, "Mean Q2": 263.2284411621094, "critic_loss": 11.89113655090332, "batch_reward": 2.5526171875, "actor_loss": -264.29786938476565, "actor_target_entropy": -2.0, "actor_entropy": 0.5389787333011627, "alpha_loss": 0.03197215420752764, "alpha_value": 0.08624938378054348, "duration": 89.63350892066956, "step": 27750}
{"episode_reward": 950.0, "episode": 112.0, "Q1 loss": 5.72606356048584, "Q2 loss": 5.766226684570312, "Mean Target Q": 265.0500642089844, "Mean Q1": 265.04786499023436, "Mean Q2": 265.04820861816404, "critic_loss": 11.492290237426758, "batch_reward": 2.5667734375, "actor_loss": -265.995046875, "actor_target_entropy": -2.0, "actor_entropy": 0.478584534406662, "alpha_loss": 0.02362559145502746, "alpha_value": 0.08514387405760404, "duration": 156.07533621788025, "step": 28000}
{"episode_reward": 912.0, "episode": 113.0, "Q1 loss": 5.8088625621795655, "Q2 loss": 5.806562107086181, "Mean Target Q": 266.67133459472655, "Mean Q1": 266.6691923828125, "Mean Q2": 266.66822497558593, "critic_loss": 11.615424655914307, "batch_reward": 2.565640625, "actor_loss": -267.66680981445313, "actor_target_entropy": -2.0, "actor_entropy": 0.42266509795188906, "alpha_loss": 0.026258392116054893, "alpha_value": 0.08416272341270917, "duration": 87.86475539207458, "step": 28250}
{"episode_reward": 923.0, "episode": 114.0, "Q1 loss": 5.65562403011322, "Q2 loss": 5.699725399971008, "Mean Target Q": 268.28372692871096, "Mean Q1": 268.2802916259766, "Mean Q2": 268.28122119140625, "critic_loss": 11.355349411010742, "batch_reward": 2.578875, "actor_loss": -269.2933181152344, "actor_target_entropy": -2.0, "actor_entropy": 0.44040859377384184, "alpha_loss": 0.020276014510542156, "alpha_value": 0.0831987706055559, "duration": 87.42598032951355, "step": 28500}
{"episode_reward": 662.0, "episode": 115.0, "Q1 loss": 5.735450937271118, "Q2 loss": 5.689395404815674, "Mean Target Q": 270.09552124023435, "Mean Q1": 270.09107531738283, "Mean Q2": 270.09130432128904, "critic_loss": 11.42484631729126, "batch_reward": 2.5946875, "actor_loss": -271.09667333984373, "actor_target_entropy": -2.0, "actor_entropy": 0.40485469162464144, "alpha_loss": 0.025599744020029902, "alpha_value": 0.08230553746271543, "duration": 85.63489365577698, "step": 28750}
{"episode_reward": 896.0, "episode": 116.0, "Q1 loss": 5.514688723564148, "Q2 loss": 5.514957777023316, "Mean Target Q": 271.5568494873047, "Mean Q1": 271.5538905029297, "Mean Q2": 271.5548026123047, "critic_loss": 11.02964652633667, "batch_reward": 2.59246875, "actor_loss": -272.46520141601565, "actor_target_entropy": -2.0, "actor_entropy": 0.44818068838119507, "alpha_loss": 0.023125895055010916, "alpha_value": 0.08138494783541111, "duration": 85.97227454185486, "step": 29000}
{"episode_reward": 938.0, "episode": 117.0, "Q1 loss": 5.248199317932129, "Q2 loss": 5.283922087669373, "Mean Target Q": 273.1484207763672, "Mean Q1": 273.1448591308594, "Mean Q2": 273.1441326904297, "critic_loss": 10.532121393203735, "batch_reward": 2.605703125, "actor_loss": -274.1444521484375, "actor_target_entropy": -2.0, "actor_entropy": 0.5146960239410401, "alpha_loss": 0.027977824684232475, "alpha_value": 0.08040425679067748, "duration": 129.80920672416687, "step": 29250}
{"episode_reward": 946.0, "episode": 118.0, "Q1 loss": 5.170163679122925, "Q2 loss": 5.155441804885864, "Mean Target Q": 274.7334130859375, "Mean Q1": 274.7312004394531, "Mean Q2": 274.7318123779297, "critic_loss": 10.325605480194092, "batch_reward": 2.6158828125, "actor_loss": -275.74064453125, "actor_target_entropy": -2.0, "actor_entropy": 0.4430360202789307, "alpha_loss": 0.029399205073714255, "alpha_value": 0.079328768796077, "duration": 113.72723603248596, "step": 29500}
{"episode_reward": 892.0, "episode": 119.0, "Q1 loss": 4.953376305580139, "Q2 loss": 4.927527353286743, "Mean Target Q": 276.258513671875, "Mean Q1": 276.2552556152344, "Mean Q2": 276.2564523925781, "critic_loss": 9.880903699874878, "batch_reward": 2.6324921875, "actor_loss": -277.15432153320313, "actor_target_entropy": -2.0, "actor_entropy": 0.4364938706159592, "alpha_loss": 0.0264260881729424, "alpha_value": 0.0783274620370189, "duration": 87.19816827774048, "step": 29750}
{"episode_reward": 979.0, "episode": 120.0, "Q1 loss": 4.9625606517791745, "Q2 loss": 4.919544727325439, "Mean Target Q": 277.5077590332031, "Mean Q1": 277.50518994140623, "Mean Q2": 277.50457385253907, "critic_loss": 9.882105367660522, "batch_reward": 2.6322109375, "actor_loss": -278.36828784179687, "actor_target_entropy": -2.0, "actor_entropy": 0.4752836278676987, "alpha_loss": 0.02560470716841519, "alpha_value": 0.07743306133944491, "step": 30000}
{"duration": 111.45654439926147, "step": 30000}
{"episode_reward": 989.0, "episode": 121.0, "Q1 loss": 4.669034309387207, "Q2 loss": 4.704809255599976, "Mean Target Q": 278.77780895996096, "Mean Q1": 278.77287182617187, "Mean Q2": 278.7737937011719, "critic_loss": 9.373843587875367, "batch_reward": 2.637609375, "actor_loss": -279.72011181640624, "actor_target_entropy": -2.0, "actor_entropy": 0.5356155784130097, "alpha_loss": 0.03086222442239523, "alpha_value": 0.07643035840552853, "duration": 86.9572160243988, "step": 30250}
{"episode_reward": 957.0, "episode": 122.0, "Q1 loss": 4.734933814048767, "Q2 loss": 4.728905350685119, "Mean Target Q": 280.12734814453125, "Mean Q1": 280.1254215087891, "Mean Q2": 280.1250703125, "critic_loss": 9.463839162826538, "batch_reward": 2.651359375, "actor_loss": -280.9389001464844, "actor_target_entropy": -2.0, "actor_entropy": 0.4450644326210022, "alpha_loss": 0.030351468969136475, "alpha_value": 0.07538404554840715, "duration": 105.3567841053009, "step": 30500}
{"episode_reward": 919.0, "episode": 123.0, "Q1 loss": 4.654209606170654, "Q2 loss": 4.625576175689697, "Mean Target Q": 281.4110032958984, "Mean Q1": 281.4084465332031, "Mean Q2": 281.4085047607422, "critic_loss": 9.279785772323608, "batch_reward": 2.6598984375, "actor_loss": -282.1607084960938, "actor_target_entropy": -2.0, "actor_entropy": 0.43853601765632627, "alpha_loss": 0.0283655671030283, "alpha_value": 0.07444701089364139, "duration": 134.19316363334656, "step": 30750}
{"episode_reward": 947.0, "episode": 124.0, "Q1 loss": 4.76731405544281, "Q2 loss": 4.723735347747803, "Mean Target Q": 282.5554644775391, "Mean Q1": 282.55336865234375, "Mean Q2": 282.55290466308594, "critic_loss": 9.491049407958984, "batch_reward": 2.66046875, "actor_loss": -283.42503491210937, "actor_target_entropy": -2.0, "actor_entropy": 0.41774867761135104, "alpha_loss": 0.02535190599411726, "alpha_value": 0.07358650382871293, "duration": 85.08369874954224, "step": 31000}
{"episode_reward": 832.0, "episode": 125.0, "Q1 loss": 4.8403113784790035, "Q2 loss": 4.825970253944397, "Mean Target Q": 283.92880822753904, "Mean Q1": 283.9258221435547, "Mean Q2": 283.9258782958984, "critic_loss": 9.666281618118287, "batch_reward": 2.6766015625, "actor_loss": -284.72207006835936, "actor_target_entropy": -2.0, "actor_entropy": 0.42944770193099974, "alpha_loss": 0.023852632576599718, "alpha_value": 0.0727968367503529, "duration": 92.7941792011261, "step": 31250}
{"episode_reward": 964.0, "episode": 126.0, "Q1 loss": 4.964127325057984, "Q2 loss": 4.937155479431152, "Mean Target Q": 285.13810021972654, "Mean Q1": 285.13579736328126, "Mean Q2": 285.13683166503904, "critic_loss": 9.901282800674439, "batch_reward": 2.6813515625, "actor_loss": -286.15137475585936, "actor_target_entropy": -2.0, "actor_entropy": 0.36324551129341126, "alpha_loss": 0.02039606747031212, "alpha_value": 0.0720786034884814, "duration": 88.43072533607483, "step": 31500}
{"episode_reward": 948.0, "episode": 127.0, "Q1 loss": 4.804121424674988, "Q2 loss": 4.792947457313538, "Mean Target Q": 286.3691702880859, "Mean Q1": 286.36731176757814, "Mean Q2": 286.3666490478516, "critic_loss": 9.59706887626648, "batch_reward": 2.68959375, "actor_loss": -287.21510498046877, "actor_target_entropy": -2.0, "actor_entropy": 0.38111952233314517, "alpha_loss": 0.022061128195375205, "alpha_value": 0.07137086667083951, "duration": 87.80713224411011, "step": 31750}
{"episode_reward": 989.0, "episode": 128.0, "Q1 loss": 4.58905908203125, "Q2 loss": 4.563013292312622, "Mean Target Q": 287.8603006591797, "Mean Q1": 287.85715991210935, "Mean Q2": 287.8583375244141, "critic_loss": 9.152072383880615, "batch_reward": 2.7136171875, "actor_loss": -288.7930439453125, "actor_target_entropy": -2.0, "actor_entropy": 0.4581421694755554, "alpha_loss": 0.025978095130994916, "alpha_value": 0.07062088119764935, "duration": 127.87834525108337, "step": 32000}
{"episode_reward": 981.0, "episode": 129.0, "Q1 loss": 4.828783748626709, "Q2 loss": 4.864882668495178, "Mean Target Q": 288.79768591308596, "Mean Q1": 288.7952752685547, "Mean Q2": 288.7943074951172, "critic_loss": 9.693666418075562, "batch_reward": 2.70303125, "actor_loss": -289.72552978515625, "actor_target_entropy": -2.0, "actor_entropy": 0.4163853701353073, "alpha_loss": 0.021493048222735525, "alpha_value": 0.06985612494188373, "duration": 103.27443408966064, "step": 32250}
{"episode_reward": 804.0, "episode": 130.0, "Q1 loss": 4.800588831901551, "Q2 loss": 4.832057055473328, "Mean Target Q": 290.06315783691406, "Mean Q1": 290.06121057128905, "Mean Q2": 290.06140771484377, "critic_loss": 9.632645877838135, "batch_reward": 2.7162109375, "actor_loss": -290.90394873046876, "actor_target_entropy": -2.0, "actor_entropy": 0.3952332273721695, "alpha_loss": 0.0257640337087214, "alpha_value": 0.06910194133000414, "duration": 87.54134774208069, "step": 32500}
{"episode_reward": 988.0, "episode": 131.0, "Q1 loss": 4.924420127868652, "Q2 loss": 4.963754002571106, "Mean Target Q": 291.3434860839844, "Mean Q1": 291.3415520019531, "Mean Q2": 291.34105041503904, "critic_loss": 9.888174129486083, "batch_reward": 2.73428125, "actor_loss": -292.19999584960937, "actor_target_entropy": -2.0, "actor_entropy": 0.43735599088668825, "alpha_loss": 0.0224192001465708, "alpha_value": 0.06834120665700129, "duration": 85.02349829673767, "step": 32750}
{"episode_reward": 972.0, "episode": 132.0, "Q1 loss": 4.978786406517028, "Q2 loss": 4.96987149810791, "Mean Target Q": 292.3125938720703, "Mean Q1": 292.3087099609375, "Mean Q2": 292.3092130126953, "critic_loss": 9.948657907485963, "batch_reward": 2.72834375, "actor_loss": -293.2175603027344, "actor_target_entropy": -2.0, "actor_entropy": 0.4225343332290649, "alpha_loss": 0.020280079811811447, "alpha_value": 0.06764013776101183, "duration": 86.33030652999878, "step": 33000}
{"episode_reward": 805.0, "episode": 133.0, "Q1 loss": 5.1827137069702145, "Q2 loss": 5.179220321655273, "Mean Target Q": 293.4822325439453, "Mean Q1": 293.4794826660156, "Mean Q2": 293.4801483154297, "critic_loss": 10.361934024810791, "batch_reward": 2.737140625, "actor_loss": -294.31995434570314, "actor_target_entropy": -2.0, "actor_entropy": 0.3723305283486843, "alpha_loss": 0.0176918663457036, "alpha_value": 0.06704030048467854, "duration": 85.69046092033386, "step": 33250}
{"episode_reward": 946.0, "episode": 134.0, "Q1 loss": 4.5137544479370115, "Q2 loss": 4.502668315887451, "Mean Target Q": 294.6943455810547, "Mean Q1": 294.69172094726565, "Mean Q2": 294.69163208007814, "critic_loss": 9.016422733306884, "batch_reward": 2.744890625, "actor_loss": -295.567732421875, "actor_target_entropy": -2.0, "actor_entropy": 0.4029888460636139, "alpha_loss": 0.022554085008800028, "alpha_value": 0.06638742818541908, "duration": 142.7466356754303, "step": 33500}
{"episode_reward": 943.0, "episode": 135.0, "Q1 loss": 4.665826977729798, "Q2 loss": 4.712386408805847, "Mean Target Q": 295.9225260009766, "Mean Q1": 295.9217607421875, "Mean Q2": 295.9214611816406, "critic_loss": 9.378213409423829, "batch_reward": 2.755625, "actor_loss": -296.7823232421875, "actor_target_entropy": -2.0, "actor_entropy": 0.348448281288147, "alpha_loss": 0.018869776106439532, "alpha_value": 0.06569650533496406, "duration": 85.74565148353577, "step": 33750}
{"episode_reward": 902.0, "episode": 136.0, "Q1 loss": 4.571200157165527, "Q2 loss": 4.584222988128662, "Mean Target Q": 296.9810074462891, "Mean Q1": 296.97748693847655, "Mean Q2": 296.97793310546876, "critic_loss": 9.155423118591308, "batch_reward": 2.7616953125, "actor_loss": -297.6877126464844, "actor_target_entropy": -2.0, "actor_entropy": 0.39378526449203494, "alpha_loss": 0.02099143266864121, "alpha_value": 0.06505280932533711, "duration": 86.83426451683044, "step": 34000}
{"episode_reward": 953.0, "episode": 137.0, "Q1 loss": 4.616091476440429, "Q2 loss": 4.581299860000611, "Mean Target Q": 298.21988671875, "Mean Q1": 298.2184808349609, "Mean Q2": 298.2186514892578, "critic_loss": 9.197391324996948, "batch_reward": 2.7724765625, "actor_loss": -299.08073486328124, "actor_target_entropy": -2.0, "actor_entropy": 0.36650467562675476, "alpha_loss": 0.02021308797225356, "alpha_value": 0.06432434612848639, "duration": 87.78036999702454, "step": 34250}
{"episode_reward": 944.0, "episode": 138.0, "Q1 loss": 4.471670259475708, "Q2 loss": 4.534713554382324, "Mean Target Q": 299.41123303222656, "Mean Q1": 299.40707824707033, "Mean Q2": 299.40708435058593, "critic_loss": 9.00638381576538, "batch_reward": 2.7772890625, "actor_loss": -300.3336484375, "actor_target_entropy": -2.0, "actor_entropy": 0.41799846804142, "alpha_loss": 0.01819154475815594, "alpha_value": 0.06371829605534671, "duration": 86.91599750518799, "step": 34500}
{"episode_reward": 958.0, "episode": 139.0, "Q1 loss": 4.486441864013672, "Q2 loss": 4.458888537406922, "Mean Target Q": 300.66386682128905, "Mean Q1": 300.6628530273438, "Mean Q2": 300.66240283203126, "critic_loss": 8.945330400466919, "batch_reward": 2.7939921875, "actor_loss": -301.5275036621094, "actor_target_entropy": -2.0, "actor_entropy": 0.45951165616512296, "alpha_loss": 0.019199690023437143, "alpha_value": 0.06312966640498403, "duration": 111.41802215576172, "step": 34750}
{"episode_reward": 950.0, "episode": 140.0, "Q1 loss": 4.418149875640869, "Q2 loss": 4.444537986755371, "Mean Target Q": 301.7433442382812, "Mean Q1": 301.7404561767578, "Mean Q2": 301.7410418701172, "critic_loss": 8.862687873840333, "batch_reward": 2.794484375, "actor_loss": -302.5546394042969, "actor_target_entropy": -2.0, "actor_entropy": 0.3964884639978409, "alpha_loss": 0.01929777040425688, "alpha_value": 0.06247074376870265, "step": 35000}
{"duration": 142.94292068481445, "step": 35000}
{"episode_reward": 889.0, "episode": 141.0, "Q1 loss": 4.396409427642822, "Q2 loss": 4.426018683433533, "Mean Target Q": 302.624373046875, "Mean Q1": 302.6240198974609, "Mean Q2": 302.62339782714844, "critic_loss": 8.822428123474122, "batch_reward": 2.79065625, "actor_loss": -303.4876333007812, "actor_target_entropy": -2.0, "actor_entropy": 0.38849226689338684, "alpha_loss": 0.022960248500108717, "alpha_value": 0.06178244515017637, "duration": 85.52674555778503, "step": 35250}
{"episode_reward": 948.0, "episode": 142.0, "Q1 loss": 4.352859706878662, "Q2 loss": 4.345962519645691, "Mean Target Q": 303.8002303466797, "Mean Q1": 303.7977060546875, "Mean Q2": 303.7982412109375, "critic_loss": 8.698822219848633, "batch_reward": 2.8044375, "actor_loss": -304.675646484375, "actor_target_entropy": -2.0, "actor_entropy": 0.3692889468967915, "alpha_loss": 0.021533116452395917, "alpha_value": 0.061051688298265835, "duration": 86.65732932090759, "step": 35500}
{"episode_reward": 990.0, "episode": 143.0, "Q1 loss": 4.439613285064698, "Q2 loss": 4.428049519538879, "Mean Target Q": 304.73540209960936, "Mean Q1": 304.7333229980469, "Mean Q2": 304.733626953125, "critic_loss": 8.867662788391113, "batch_reward": 2.8092890625, "actor_loss": -305.6725437011719, "actor_target_entropy": -2.0, "actor_entropy": 0.3677326319217682, "alpha_loss": 0.020268054274842143, "alpha_value": 0.06040101544762123, "duration": 83.16964221000671, "step": 35750}
{"episode_reward": 940.0, "episode": 144.0, "Q1 loss": 4.31669964313507, "Q2 loss": 4.379032554626465, "Mean Target Q": 305.6404963378906, "Mean Q1": 305.6360206298828, "Mean Q2": 305.63679235839845, "critic_loss": 8.695732196807862, "batch_reward": 2.80975, "actor_loss": -306.47675708007813, "actor_target_entropy": -2.0, "actor_entropy": 0.41460810112953184, "alpha_loss": 0.016945640561170875, "alpha_value": 0.05978301034544753, "duration": 85.3490297794342, "step": 36000}
{"episode_reward": 912.0, "episode": 145.0, "Q1 loss": 4.287476254463196, "Q2 loss": 4.325140768051147, "Mean Target Q": 306.7585339355469, "Mean Q1": 306.75897241210936, "Mean Q2": 306.75739013671875, "critic_loss": 8.612617027282715, "batch_reward": 2.816796875, "actor_loss": -307.54236645507814, "actor_target_entropy": -2.0, "actor_entropy": 0.40715419256687163, "alpha_loss": 0.017133770935237408, "alpha_value": 0.059235181726560115, "duration": 144.04726648330688, "step": 36250}
{"episode_reward": 885.0, "episode": 146.0, "Q1 loss": 4.332442494392395, "Q2 loss": 4.3608869934082035, "Mean Target Q": 307.8619039306641, "Mean Q1": 307.85933264160155, "Mean Q2": 307.86002075195313, "critic_loss": 8.693329475402832, "batch_reward": 2.8364140625, "actor_loss": -308.66845458984375, "actor_target_entropy": -2.0, "actor_entropy": 0.40609083211421965, "alpha_loss": 0.01784143182449043, "alpha_value": 0.05866746871926722, "duration": 88.75976276397705, "step": 36500}
{"episode_reward": 956.0, "episode": 147.0, "Q1 loss": 4.170683730125427, "Q2 loss": 4.118992979049683, "Mean Target Q": 308.84040197753905, "Mean Q1": 308.8370135498047, "Mean Q2": 308.83750244140623, "critic_loss": 8.289676727294921, "batch_reward": 2.840796875, "actor_loss": -309.6364692382812, "actor_target_entropy": -2.0, "actor_entropy": 0.3722945994138718, "alpha_loss": 0.020264818048104642, "alpha_value": 0.05802033648200922, "duration": 89.98659682273865, "step": 36750}
{"episode_reward": 982.0, "episode": 148.0, "Q1 loss": 4.156163066864013, "Q2 loss": 4.1850642461776735, "Mean Target Q": 309.76485095214844, "Mean Q1": 309.76512097167966, "Mean Q2": 309.7646136474609, "critic_loss": 8.341227310180663, "batch_reward": 2.8405703125, "actor_loss": -310.60675927734377, "actor_target_entropy": -2.0, "actor_entropy": 0.337083580493927, "alpha_loss": 0.018796704651787878, "alpha_value": 0.0574005993054734, "duration": 87.73716902732849, "step": 37000}
{"episode_reward": 954.0, "episode": 149.0, "Q1 loss": 4.1979200620651245, "Q2 loss": 4.179663104057312, "Mean Target Q": 310.7506677246094, "Mean Q1": 310.7477912597656, "Mean Q2": 310.74878967285156, "critic_loss": 8.377583164215087, "batch_reward": 2.845296875, "actor_loss": -311.48447387695313, "actor_target_entropy": -2.0, "actor_entropy": 0.285935597807169, "alpha_loss": 0.015450399679131806, "alpha_value": 0.056837352090151665, "duration": 87.34162616729736, "step": 37250}
{"episode_reward": 956.0, "episode": 150.0, "Q1 loss": 3.999533450126648, "Q2 loss": 4.028542317390442, "Mean Target Q": 311.8297314453125, "Mean Q1": 311.82696252441406, "Mean Q2": 311.8276925048828, "critic_loss": 8.028075784683228, "batch_reward": 2.861, "actor_loss": -312.50008642578126, "actor_target_entropy": -2.0, "actor_entropy": 0.3543195822238922, "alpha_loss": 0.015984257752075792, "alpha_value": 0.05632269715269814, "duration": 111.56114172935486, "step": 37500}
{"episode_reward": 990.0, "episode": 151.0, "Q1 loss": 4.120245789527893, "Q2 loss": 4.114381106376648, "Mean Target Q": 312.8000810546875, "Mean Q1": 312.79833435058595, "Mean Q2": 312.7981710205078, "critic_loss": 8.234626876831054, "batch_reward": 2.8615546875, "actor_loss": -313.5989143066406, "actor_target_entropy": -2.0, "actor_entropy": 0.3479871209859848, "alpha_loss": 0.01570899099484086, "alpha_value": 0.05578452521199865, "duration": 121.52114152908325, "step": 37750}
{"episode_reward": 888.0, "episode": 152.0, "Q1 loss": 3.963003499984741, "Q2 loss": 3.9608003406524657, "Mean Target Q": 313.6021672363281, "Mean Q1": 313.60047631835937, "Mean Q2": 313.5996236572266, "critic_loss": 7.9238038234710695, "batch_reward": 2.8575625, "actor_loss": -314.33444873046875, "actor_target_entropy": -2.0, "actor_entropy": 0.2871388951539993, "alpha_loss": 0.014019758198410273, "alpha_value": 0.05530992315909916, "duration": 85.51924133300781, "step": 38000}
{"episode_reward": 890.0, "episode": 153.0, "Q1 loss": 4.184422965049744, "Q2 loss": 4.132394563674927, "Mean Target Q": 314.6834614257813, "Mean Q1": 314.6828575439453, "Mean Q2": 314.6828344726562, "critic_loss": 8.316817531585693, "batch_reward": 2.8775, "actor_loss": -315.5271008300781, "actor_target_entropy": -2.0, "actor_entropy": 0.28170746518671513, "alpha_loss": 0.015389835284091533, "alpha_value": 0.05477594894958485, "duration": 88.95735239982605, "step": 38250}
{"episode_reward": 953.0, "episode": 154.0, "Q1 loss": 4.148277150154113, "Q2 loss": 4.174336591720581, "Mean Target Q": 315.5116433105469, "Mean Q1": 315.50696350097655, "Mean Q2": 315.5073309326172, "critic_loss": 8.322613725662231, "batch_reward": 2.8791796875, "actor_loss": -316.29396533203123, "actor_target_entropy": -2.0, "actor_entropy": 0.3411882498562336, "alpha_loss": 0.017852484682574866, "alpha_value": 0.05420093251257817, "duration": 86.37318515777588, "step": 38500}
{"episode_reward": 831.0, "episode": 155.0, "Q1 loss": 4.152437822341919, "Q2 loss": 4.194523150444031, "Mean Target Q": 316.1910732421875, "Mean Q1": 316.1899439697266, "Mean Q2": 316.18942431640625, "critic_loss": 8.346960981369019, "batch_reward": 2.8640859375, "actor_loss": -316.83547875976564, "actor_target_entropy": -2.0, "actor_entropy": 0.277171287804842, "alpha_loss": 0.01296783166565001, "alpha_value": 0.05368993480364814, "duration": 88.905348777771, "step": 38750}
{"episode_reward": 955.0, "episode": 156.0, "Q1 loss": 4.08244735622406, "Q2 loss": 3.9962362937927245, "Mean Target Q": 317.1323659667969, "Mean Q1": 317.13072009277346, "Mean Q2": 317.1320627441406, "critic_loss": 8.07868365097046, "batch_reward": 2.88325, "actor_loss": -317.8076921386719, "actor_target_entropy": -2.0, "actor_entropy": 0.2898443706035614, "alpha_loss": 0.015877485268749297, "alpha_value": 0.053174495246193214, "duration": 161.206937789917, "step": 39000}
{"episode_reward": 953.0, "episode": 157.0, "Q1 loss": 4.058909888267517, "Q2 loss": 4.048179479598999, "Mean Target Q": 318.12485375976564, "Mean Q1": 318.12196630859376, "Mean Q2": 318.1219216308594, "critic_loss": 8.107089380264282, "batch_reward": 2.89790625, "actor_loss": -318.83479418945313, "actor_target_entropy": -2.0, "actor_entropy": 0.2665891999974847, "alpha_loss": 0.016240338331088424, "alpha_value": 0.05260568510781085, "duration": 86.34695482254028, "step": 39250}
{"episode_reward": 948.0, "episode": 158.0, "Q1 loss": 3.9444322338104247, "Q2 loss": 3.933317702293396, "Mean Target Q": 319.06645581054687, "Mean Q1": 319.06585498046877, "Mean Q2": 319.0655289306641, "critic_loss": 7.877749965667725, "batch_reward": 2.9049453125, "actor_loss": -319.69080395507814, "actor_target_entropy": -2.0, "actor_entropy": 0.2945328091979027, "alpha_loss": 0.015594393346458674, "alpha_value": 0.05208874188561892, "duration": 87.29592967033386, "step": 39500}
{"episode_reward": 982.0, "episode": 159.0, "Q1 loss": 3.7050773839950564, "Q2 loss": 3.68799605846405, "Mean Target Q": 319.8592587890625, "Mean Q1": 319.8581466064453, "Mean Q2": 319.8584990234375, "critic_loss": 7.39307345199585, "batch_reward": 2.90634375, "actor_loss": -320.51607885742186, "actor_target_entropy": -2.0, "actor_entropy": 0.3414512552022934, "alpha_loss": 0.019651280063204466, "alpha_value": 0.05146184273297409, "duration": 88.87752938270569, "step": 39750}
{"episode_reward": 945.0, "episode": 160.0, "Q1 loss": 3.8142067613601687, "Q2 loss": 3.8078972845077517, "Mean Target Q": 320.7604929199219, "Mean Q1": 320.75744592285156, "Mean Q2": 320.7568475341797, "critic_loss": 7.622104055404663, "batch_reward": 2.9123671875, "actor_loss": -321.62435473632814, "actor_target_entropy": -2.0, "actor_entropy": 0.2717731805443764, "alpha_loss": 0.015173859562724828, "alpha_value": 0.050876375196495766, "step": 40000}
{"duration": 113.03696370124817, "step": 40000}
{"episode_reward": 901.0, "episode": 161.0, "Q1 loss": 3.6828503170013427, "Q2 loss": 3.7072203316688537, "Mean Target Q": 321.6120607910156, "Mean Q1": 321.6103333740234, "Mean Q2": 321.61103881835936, "critic_loss": 7.390070671081543, "batch_reward": 2.915109375, "actor_loss": -322.3774216308594, "actor_target_entropy": -2.0, "actor_entropy": 0.28451063476502897, "alpha_loss": 0.01664523487444967, "alpha_value": 0.05036863274613464, "duration": 122.04920363426208, "step": 40250}
{"episode_reward": 954.0, "episode": 162.0, "Q1 loss": 3.784228678703308, "Q2 loss": 3.7493062992095947, "Mean Target Q": 322.3886564941406, "Mean Q1": 322.38663342285156, "Mean Q2": 322.38727197265627, "critic_loss": 7.533534971237183, "batch_reward": 2.9101015625, "actor_loss": -323.162458984375, "actor_target_entropy": -2.0, "actor_entropy": 0.2047566673308611, "alpha_loss": 0.014506471306085587, "alpha_value": 0.049831929725397246, "duration": 156.45331859588623, "step": 40500}
{"episode_reward": 946.0, "episode": 163.0, "Q1 loss": 3.6402767772674562, "Q2 loss": 3.634340696334839, "Mean Target Q": 323.3403740234375, "Mean Q1": 323.33976220703124, "Mean Q2": 323.3382218017578, "critic_loss": 7.274617460250854, "batch_reward": 2.9232265625, "actor_loss": -324.0349853515625, "actor_target_entropy": -2.0, "actor_entropy": 0.2503454858660698, "alpha_loss": 0.014400884012691677, "alpha_value": 0.04933141029018245, "duration": 81.19688272476196, "step": 40750}
{"episode_reward": 937.0, "episode": 164.0, "Q1 loss": 3.4986873984336855, "Q2 loss": 3.496426294326782, "Mean Target Q": 324.11487341308595, "Mean Q1": 324.1124812011719, "Mean Q2": 324.112658203125, "critic_loss": 6.995113676071167, "batch_reward": 2.9254296875, "actor_loss": -324.87172998046873, "actor_target_entropy": -2.0, "actor_entropy": 0.2566227676048875, "alpha_loss": 0.015796447248198092, "alpha_value": 0.048836109286837334, "duration": 82.25696063041687, "step": 41000}
{"episode_reward": 989.0, "episode": 165.0, "Q1 loss": 3.5653088703155515, "Q2 loss": 3.5666353607177737, "Mean Target Q": 325.0838455810547, "Mean Q1": 325.082251953125, "Mean Q2": 325.0826749267578, "critic_loss": 7.131944242477417, "batch_reward": 2.9381484375, "actor_loss": -325.8256884765625, "actor_target_entropy": -2.0, "actor_entropy": 0.2919460173249245, "alpha_loss": 0.013164254666306078, "alpha_value": 0.04831344004637732, "duration": 81.97148633003235, "step": 41250}
{"episode_reward": 943.0, "episode": 166.0, "Q1 loss": 3.6174091453552246, "Q2 loss": 3.6295647535324096, "Mean Target Q": 325.9786036376953, "Mean Q1": 325.97710900878906, "Mean Q2": 325.9766052246094, "critic_loss": 7.246973908424377, "batch_reward": 2.9451328125, "actor_loss": -326.68419677734374, "actor_target_entropy": -2.0, "actor_entropy": 0.2738554936349392, "alpha_loss": 0.014192638659849762, "alpha_value": 0.04786928856773333, "duration": 82.83632612228394, "step": 41500}
{"episode_reward": 954.0, "episode": 167.0, "Q1 loss": 3.5654696879386902, "Q2 loss": 3.579306758403778, "Mean Target Q": 326.628416015625, "Mean Q1": 326.62757409667967, "Mean Q2": 326.62830212402343, "critic_loss": 7.144776443481446, "batch_reward": 2.944578125, "actor_loss": -327.45104638671876, "actor_target_entropy": -2.0, "actor_entropy": 0.20898966878652572, "alpha_loss": 0.0139948086226359, "alpha_value": 0.04738984659863427, "duration": 83.1528913974762, "step": 41750}
{"episode_reward": 954.0, "episode": 168.0, "Q1 loss": 3.6185671424865724, "Q2 loss": 3.5425326175689698, "Mean Target Q": 327.42726208496094, "Mean Q1": 327.42621923828125, "Mean Q2": 327.4260715332031, "critic_loss": 7.161099760055542, "batch_reward": 2.9571484375, "actor_loss": -328.16817431640624, "actor_target_entropy": -2.0, "actor_entropy": 0.1816143682897091, "alpha_loss": 0.011321236540097743, "alpha_value": 0.046954530697324276, "duration": 81.64516139030457, "step": 42000}
{"episode_reward": 907.0, "episode": 169.0, "Q1 loss": 3.493542815208435, "Q2 loss": 3.5052366342544556, "Mean Target Q": 328.2144688720703, "Mean Q1": 328.211546875, "Mean Q2": 328.21164929199216, "critic_loss": 6.998779445648194, "batch_reward": 2.957921875, "actor_loss": -328.92681958007813, "actor_target_entropy": -2.0, "actor_entropy": 0.19851692397147416, "alpha_loss": 0.01407005539815873, "alpha_value": 0.04649764191090613, "duration": 83.59382247924805, "step": 42250}
{"episode_reward": 943.0, "episode": 170.0, "Q1 loss": 3.302095497608185, "Q2 loss": 3.277451733589172, "Mean Target Q": 328.97851831054686, "Mean Q1": 328.9784462890625, "Mean Q2": 328.9786512451172, "critic_loss": 6.579547241210937, "batch_reward": 2.960421875, "actor_loss": -329.6592585449219, "actor_target_entropy": -2.0, "actor_entropy": 0.1859066052362323, "alpha_loss": 0.013040726013015955, "alpha_value": 0.04605918128272959, "duration": 82.3105731010437, "step": 42500}
{"episode_reward": 946.0, "episode": 171.0, "Q1 loss": 3.4273601603507995, "Q2 loss": 3.4753162965774536, "Mean Target Q": 329.7470830078125, "Mean Q1": 329.74533532714844, "Mean Q2": 329.7450427246094, "critic_loss": 6.902676471710205, "batch_reward": 2.9636171875, "actor_loss": -330.47930444335935, "actor_target_entropy": -2.0, "actor_entropy": 0.220940473228693, "alpha_loss": 0.01249411531817168, "alpha_value": 0.045584395242935546, "duration": 83.49573111534119, "step": 42750}
{"episode_reward": 914.0, "episode": 172.0, "Q1 loss": 3.408194951057434, "Q2 loss": 3.3729575929641724, "Mean Target Q": 330.4931884765625, "Mean Q1": 330.49046936035154, "Mean Q2": 330.4905859375, "critic_loss": 6.781152547836304, "batch_reward": 2.9673359375, "actor_loss": -331.2710654296875, "actor_target_entropy": -2.0, "actor_entropy": 0.200308620326221, "alpha_loss": 0.012153524045832455, "alpha_value": 0.045161712855081936, "duration": 90.25464010238647, "step": 43000}
{"episode_reward": 960.0, "episode": 173.0, "Q1 loss": 3.290833545207977, "Q2 loss": 3.2988050498962402, "Mean Target Q": 331.2468516845703, "Mean Q1": 331.2460482177734, "Mean Q2": 331.24565087890625, "critic_loss": 6.5896385974884035, "batch_reward": 2.97446875, "actor_loss": -331.8990915527344, "actor_target_entropy": -2.0, "actor_entropy": 0.1693590512871742, "alpha_loss": 0.011727940332144499, "alpha_value": 0.04476427537587604, "duration": 83.25956225395203, "step": 43250}
{"episode_reward": 994.0, "episode": 174.0, "Q1 loss": 3.0645006680488587, "Q2 loss": 3.0332226824760435, "Mean Target Q": 332.00675366210936, "Mean Q1": 332.00517541503905, "Mean Q2": 332.00581469726563, "critic_loss": 6.097723363876343, "batch_reward": 2.9804140625, "actor_loss": -332.66731689453127, "actor_target_entropy": -2.0, "actor_entropy": 0.1482315875068307, "alpha_loss": 0.01247504283580929, "alpha_value": 0.04430099660392903, "duration": 83.17212629318237, "step": 43500}
{"episode_reward": 992.0, "episode": 175.0, "Q1 loss": 3.0642105836868287, "Q2 loss": 3.032726974487305, "Mean Target Q": 332.6693850097656, "Mean Q1": 332.6671414794922, "Mean Q2": 332.6666768798828, "critic_loss": 6.09693756198883, "batch_reward": 2.986109375, "actor_loss": -333.21825390625, "actor_target_entropy": -2.0, "actor_entropy": 0.14300674387067555, "alpha_loss": 0.012223525209352375, "alpha_value": 0.04388586827471761, "duration": 85.24636745452881, "step": 43750}
{"episode_reward": 985.0, "episode": 176.0, "Q1 loss": 3.015597352981567, "Q2 loss": 2.973090418815613, "Mean Target Q": 333.4687401123047, "Mean Q1": 333.46783068847657, "Mean Q2": 333.46818688964845, "critic_loss": 5.988687788009644, "batch_reward": 2.9958984375, "actor_loss": -334.1340219726562, "actor_target_entropy": -2.0, "actor_entropy": 0.2075975011512637, "alpha_loss": 0.014724621545057744, "alpha_value": 0.043388784190938987, "duration": 84.01904010772705, "step": 44000}
{"episode_reward": 952.0, "episode": 177.0, "Q1 loss": 3.0404784293174743, "Q2 loss": 3.042710416793823, "Mean Target Q": 334.08560131835935, "Mean Q1": 334.0836640625, "Mean Q2": 334.08394555664063, "critic_loss": 6.083188850402832, "batch_reward": 2.9938671875, "actor_loss": -334.68950903320314, "actor_target_entropy": -2.0, "actor_entropy": 0.26069079065322875, "alpha_loss": 0.014209359743632376, "alpha_value": 0.04288584321499082, "duration": 82.9454505443573, "step": 44250}
{"episode_reward": 946.0, "episode": 178.0, "Q1 loss": 3.0183227462768554, "Q2 loss": 3.020228823184967, "Mean Target Q": 334.7317641601563, "Mean Q1": 334.73038232421874, "Mean Q2": 334.7307185058594, "critic_loss": 6.0385515499114994, "batch_reward": 3.0012734375, "actor_loss": -335.53193603515626, "actor_target_entropy": -2.0, "actor_entropy": 0.19374270616471767, "alpha_loss": 0.013554044397547841, "alpha_value": 0.042385889562660824, "duration": 83.82915377616882, "step": 44500}
{"episode_reward": 1000.0, "episode": 179.0, "Q1 loss": 3.1507848982810973, "Q2 loss": 3.1890657243728637, "Mean Target Q": 335.4110078125, "Mean Q1": 335.4099919433594, "Mean Q2": 335.4094005126953, "critic_loss": 6.339850629806518, "batch_reward": 3.0054921875, "actor_loss": -336.1731550292969, "actor_target_entropy": -2.0, "actor_entropy": 0.2839549862146378, "alpha_loss": 0.010673853306565433, "alpha_value": 0.041984491234262046, "duration": 84.67503523826599, "step": 44750}
{"episode_reward": 883.0, "episode": 180.0, "Q1 loss": 3.0369672451019287, "Q2 loss": 3.0529319105148316, "Mean Target Q": 336.01718029785155, "Mean Q1": 336.0161936035156, "Mean Q2": 336.01606384277346, "critic_loss": 6.08989914894104, "batch_reward": 3.005265625, "actor_loss": -336.7011103515625, "actor_target_entropy": -2.0, "actor_entropy": 0.20712204614281654, "alpha_loss": 0.011864714017137885, "alpha_value": 0.04161662388858774, "step": 45000}
{"duration": 107.88829469680786, "step": 45000}
{"episode_reward": 956.0, "episode": 181.0, "Q1 loss": 3.1464484519958495, "Q2 loss": 3.140064790725708, "Mean Target Q": 336.5921805419922, "Mean Q1": 336.5893310546875, "Mean Q2": 336.5895714111328, "critic_loss": 6.28651325416565, "batch_reward": 3.010203125, "actor_loss": -337.2516125488281, "actor_target_entropy": -2.0, "actor_entropy": 0.2032793769314885, "alpha_loss": 0.01260688488325104, "alpha_value": 0.04118635893986991, "duration": 83.01958990097046, "step": 45250}
{"episode_reward": 946.0, "episode": 182.0, "Q1 loss": 3.122706099510193, "Q2 loss": 3.1501036443710326, "Mean Target Q": 337.21948510742186, "Mean Q1": 337.2169903564453, "Mean Q2": 337.21785778808595, "critic_loss": 6.27280973815918, "batch_reward": 3.0157421875, "actor_loss": -337.9115368652344, "actor_target_entropy": -2.0, "actor_entropy": 0.23528351593017577, "alpha_loss": 0.01267751053161919, "alpha_value": 0.040738488547704774, "duration": 83.29901576042175, "step": 45500}
{"episode_reward": 947.0, "episode": 183.0, "Q1 loss": 2.883942039489746, "Q2 loss": 2.8948798089027403, "Mean Target Q": 337.971697265625, "Mean Q1": 337.97216784667967, "Mean Q2": 337.9713739013672, "critic_loss": 5.7788218421936035, "batch_reward": 3.0286640625, "actor_loss": -338.6004484863281, "actor_target_entropy": -2.0, "actor_entropy": 0.1968449338003993, "alpha_loss": 0.01283860656619072, "alpha_value": 0.04028799863711098, "duration": 84.49105191230774, "step": 45750}
{"episode_reward": 991.0, "episode": 184.0, "Q1 loss": 2.8376190447807312, "Q2 loss": 2.771948492527008, "Mean Target Q": 338.6026097412109, "Mean Q1": 338.6018836669922, "Mean Q2": 338.60269189453123, "critic_loss": 5.60956752204895, "batch_reward": 3.032703125, "actor_loss": -339.26458544921877, "actor_target_entropy": -2.0, "actor_entropy": 0.17520001865178347, "alpha_loss": 0.012735125662758947, "alpha_value": 0.039847301120479614, "duration": 84.3359203338623, "step": 46000}
{"episode_reward": 895.0, "episode": 185.0, "Q1 loss": 2.8918568768501283, "Q2 loss": 2.8955979022979736, "Mean Target Q": 339.02603564453125, "Mean Q1": 339.0237724609375, "Mean Q2": 339.02375830078125, "critic_loss": 5.787454794883728, "batch_reward": 3.024015625, "actor_loss": -339.72171362304687, "actor_target_entropy": -2.0, "actor_entropy": 0.21911160146445036, "alpha_loss": 0.011033823503181339, "alpha_value": 0.03943378048521192, "duration": 84.71428370475769, "step": 46250}
{"episode_reward": 951.0, "episode": 186.0, "Q1 loss": 2.9692405762672425, "Q2 loss": 2.94782683801651, "Mean Target Q": 339.72075622558594, "Mean Q1": 339.7201322021484, "Mean Q2": 339.71966223144534, "critic_loss": 5.917067401885986, "batch_reward": 3.0351875, "actor_loss": -340.45922583007814, "actor_target_entropy": -2.0, "actor_entropy": 0.2333287367671728, "alpha_loss": 0.011565464250743389, "alpha_value": 0.0390677205215628, "duration": 83.89726543426514, "step": 46500}
{"episode_reward": 948.0, "episode": 187.0, "Q1 loss": 2.891701928138733, "Q2 loss": 2.8972572641372683, "Mean Target Q": 340.3315378417969, "Mean Q1": 340.329197265625, "Mean Q2": 340.32946911621093, "critic_loss": 5.788959210395813, "batch_reward": 3.033984375, "actor_loss": -341.0967421875, "actor_target_entropy": -2.0, "actor_entropy": 0.18828378715366126, "alpha_loss": 0.011517275041900575, "alpha_value": 0.03866196688471716, "duration": 84.5254135131836, "step": 46750}
{"episode_reward": 950.0, "episode": 188.0, "Q1 loss": 2.9432410168647767, "Q2 loss": 2.892843523979187, "Mean Target Q": 340.9092390136719, "Mean Q1": 340.9093424072266, "Mean Q2": 340.9092193603516, "critic_loss": 5.8360845518112185, "batch_reward": 3.0403359375, "actor_loss": -341.49334204101564, "actor_target_entropy": -2.0, "actor_entropy": 0.19471886116266252, "alpha_loss": 0.009346572483889759, "alpha_value": 0.038301944166901636, "duration": 82.6703770160675, "step": 47000}
{"episode_reward": 934.0, "episode": 189.0, "Q1 loss": 2.749841724395752, "Q2 loss": 2.7611308097839355, "Mean Target Q": 341.4418996582031, "Mean Q1": 341.4399696044922, "Mean Q2": 341.4396468505859, "critic_loss": 5.510972531318664, "batch_reward": 3.0425390625, "actor_loss": -342.0551281738281, "actor_target_entropy": -2.0, "actor_entropy": 0.13527140859514475, "alpha_loss": 0.009250158020295203, "alpha_value": 0.03799001592736301, "duration": 83.58225011825562, "step": 47250}
{"episode_reward": 942.0, "episode": 190.0, "Q1 loss": 2.6787095084190367, "Q2 loss": 2.669910392284393, "Mean Target Q": 342.04412512207034, "Mean Q1": 342.0419407958984, "Mean Q2": 342.04233056640624, "critic_loss": 5.348619891166687, "batch_reward": 3.04271875, "actor_loss": -342.73175, "actor_target_entropy": -2.0, "actor_entropy": 0.23221179173886777, "alpha_loss": 0.011900299669243395, "alpha_value": 0.03759708951394134, "duration": 90.30322670936584, "step": 47500}
{"episode_reward": 993.0, "episode": 191.0, "Q1 loss": 2.7178073563575746, "Q2 loss": 2.7152658014297484, "Mean Target Q": 342.72931677246095, "Mean Q1": 342.7282894287109, "Mean Q2": 342.72840979003905, "critic_loss": 5.433073175430298, "batch_reward": 3.0546640625, "actor_loss": -343.4397390136719, "actor_target_entropy": -2.0, "actor_entropy": 0.21242576088011264, "alpha_loss": 0.010786048036068679, "alpha_value": 0.03720359938200055, "duration": 84.27966046333313, "step": 47750}
{"episode_reward": 960.0, "episode": 192.0, "Q1 loss": 2.641435241699219, "Q2 loss": 2.560297335624695, "Mean Target Q": 343.2098892822266, "Mean Q1": 343.20897241210935, "Mean Q2": 343.2086599121094, "critic_loss": 5.201732581138611, "batch_reward": 3.0578125, "actor_loss": -343.7963112792969, "actor_target_entropy": -2.0, "actor_entropy": 0.23812576156109572, "alpha_loss": 0.009989174341782928, "alpha_value": 0.03683303610086822, "duration": 84.17912554740906, "step": 48000}
{"episode_reward": 958.0, "episode": 193.0, "Q1 loss": 2.702027506351471, "Q2 loss": 2.704577461719513, "Mean Target Q": 343.7752247314453, "Mean Q1": 343.77381640625, "Mean Q2": 343.77385986328125, "critic_loss": 5.4066049728393555, "batch_reward": 3.06265625, "actor_loss": -344.36754174804685, "actor_target_entropy": -2.0, "actor_entropy": 0.1759385340064764, "alpha_loss": 0.006530052172020078, "alpha_value": 0.03655268979857723, "duration": 84.76026797294617, "step": 48250}
{"episode_reward": 956.0, "episode": 194.0, "Q1 loss": 2.740080425739288, "Q2 loss": 2.7474238386154175, "Mean Target Q": 344.42526818847654, "Mean Q1": 344.4232314453125, "Mean Q2": 344.4234547119141, "critic_loss": 5.487504271507263, "batch_reward": 3.07271875, "actor_loss": -344.9299904785156, "actor_target_entropy": -2.0, "actor_entropy": 0.14280104657262563, "alpha_loss": 0.008358301240717993, "alpha_value": 0.03627192254217084, "duration": 83.52718377113342, "step": 48500}
{"episode_reward": 888.0, "episode": 195.0, "Q1 loss": 2.596332895278931, "Q2 loss": 2.5490696392059324, "Mean Target Q": 344.8582938232422, "Mean Q1": 344.8573005371094, "Mean Q2": 344.8579136962891, "critic_loss": 5.145402523994446, "batch_reward": 3.0665078125, "actor_loss": -345.5794252929687, "actor_target_entropy": -2.0, "actor_entropy": 0.16780027873069048, "alpha_loss": 0.010638881186954677, "alpha_value": 0.03593005263174989, "duration": 83.79630279541016, "step": 48750}
{"episode_reward": 946.0, "episode": 196.0, "Q1 loss": 2.6855823817253115, "Q2 loss": 2.688097294807434, "Mean Target Q": 345.3752922363281, "Mean Q1": 345.37486682128906, "Mean Q2": 345.3746774902344, "critic_loss": 5.373679665565491, "batch_reward": 3.071015625, "actor_loss": -345.99930786132813, "actor_target_entropy": -2.0, "actor_entropy": 0.20076887020468712, "alpha_loss": 0.009309825127013028, "alpha_value": 0.0355866100792798, "duration": 82.60612654685974, "step": 49000}
{"episode_reward": 938.0, "episode": 197.0, "Q1 loss": 2.786083850860596, "Q2 loss": 2.760982457637787, "Mean Target Q": 346.08573498535156, "Mean Q1": 346.083904296875, "Mean Q2": 346.08384399414064, "critic_loss": 5.547066318511963, "batch_reward": 3.08428125, "actor_loss": -346.5547294921875, "actor_target_entropy": -2.0, "actor_entropy": 0.17286596352607012, "alpha_loss": 0.008520881842821836, "alpha_value": 0.03523834308118522, "duration": 83.37650632858276, "step": 49250}
{"episode_reward": 806.0, "episode": 198.0, "Q1 loss": 2.796513273715973, "Q2 loss": 2.777672482013702, "Mean Target Q": 346.4674455566406, "Mean Q1": 346.46599853515625, "Mean Q2": 346.4656079101562, "critic_loss": 5.574185749053955, "batch_reward": 3.0779375, "actor_loss": -347.0284873046875, "actor_target_entropy": -2.0, "actor_entropy": 0.1797064192891121, "alpha_loss": 0.009431562472134829, "alpha_value": 0.034914499665237755, "duration": 82.8828661441803, "step": 49500}
{"episode_reward": 938.0, "episode": 199.0, "Q1 loss": 2.5874851808547974, "Q2 loss": 2.5791967902183535, "Mean Target Q": 347.0232956542969, "Mean Q1": 347.02217041015626, "Mean Q2": 347.0230654296875, "critic_loss": 5.166681975364685, "batch_reward": 3.0816875, "actor_loss": -347.53909375, "actor_target_entropy": -2.0, "actor_entropy": 0.151478088863194, "alpha_loss": 0.008950256826821715, "alpha_value": 0.034572175672968004, "duration": 82.22286033630371, "step": 49750}
{"episode_reward": 953.0, "episode": 200.0, "Q1 loss": 2.5124129934310915, "Q2 loss": 2.5427371516227724, "Mean Target Q": 347.6100723876953, "Mean Q1": 347.6092268066406, "Mean Q2": 347.6079311523437, "critic_loss": 5.055150148391724, "batch_reward": 3.089859375, "actor_loss": -348.11990625, "actor_target_entropy": -2.0, "actor_entropy": 0.09313788321614265, "alpha_loss": 0.00952040637936443, "alpha_value": 0.034214327335565745, "step": 50000}
{"duration": 108.2284426689148, "step": 50000}
{"episode_reward": 882.0, "episode": 201.0, "Q1 loss": 2.9143724279403687, "Q2 loss": 2.875735860347748, "Mean Target Q": 347.9336791992188, "Mean Q1": 347.93226721191405, "Mean Q2": 347.9329366455078, "critic_loss": 5.790108292579651, "batch_reward": 3.0817734375, "actor_loss": -348.5633708496094, "actor_target_entropy": -2.0, "actor_entropy": 0.014516163393855095, "alpha_loss": 0.007658708096016199, "alpha_value": 0.033916429917372704, "duration": 88.57888412475586, "step": 50250}
{"episode_reward": 984.0, "episode": 202.0, "Q1 loss": 2.7463797159194945, "Q2 loss": 2.7419711112976075, "Mean Target Q": 348.4932548828125, "Mean Q1": 348.49203002929687, "Mean Q2": 348.49173693847655, "critic_loss": 5.488350817680359, "batch_reward": 3.088328125, "actor_loss": -349.05602758789064, "actor_target_entropy": -2.0, "actor_entropy": 0.11688458304852248, "alpha_loss": 0.008488814549520612, "alpha_value": 0.033608421832328934, "duration": 84.45206499099731, "step": 50500}
{"episode_reward": 957.0, "episode": 203.0, "Q1 loss": 2.5522373657226565, "Q2 loss": 2.5441216311454773, "Mean Target Q": 349.0616942138672, "Mean Q1": 349.0619774169922, "Mean Q2": 349.0629149169922, "critic_loss": 5.096358994483948, "batch_reward": 3.0966796875, "actor_loss": -349.58455029296874, "actor_target_entropy": -2.0, "actor_entropy": 0.09448456002771854, "alpha_loss": 0.0070968096726574, "alpha_value": 0.033308999992187255, "duration": 82.77750563621521, "step": 50750}
{"episode_reward": 956.0, "episode": 204.0, "Q1 loss": 2.7484548420906068, "Q2 loss": 2.7352051157951354, "Mean Target Q": 349.5209119873047, "Mean Q1": 349.51802502441404, "Mean Q2": 349.5171743164062, "critic_loss": 5.483659955978394, "batch_reward": 3.0992265625, "actor_loss": -350.13811157226564, "actor_target_entropy": -2.0, "actor_entropy": 0.06478025580942631, "alpha_loss": 0.010567994171287865, "alpha_value": 0.032978381897173586, "duration": 84.11753988265991, "step": 51000}
{"episode_reward": 939.0, "episode": 205.0, "Q1 loss": 2.737704456806183, "Q2 loss": 2.7261507925987245, "Mean Target Q": 349.9327087402344, "Mean Q1": 349.93177966308593, "Mean Q2": 349.93231127929687, "critic_loss": 5.4638552560806275, "batch_reward": 3.09471875, "actor_loss": -350.4687219238281, "actor_target_entropy": -2.0, "actor_entropy": -0.11507435313612223, "alpha_loss": 0.0037489816742017866, "alpha_value": 0.03271545532466109, "duration": 83.99654412269592, "step": 51250}
{"episode_reward": 911.0, "episode": 206.0, "Q1 loss": 2.8233869347572327, "Q2 loss": 2.7701003780364992, "Mean Target Q": 350.4452149658203, "Mean Q1": 350.44329455566407, "Mean Q2": 350.4439227294922, "critic_loss": 5.593487288475036, "batch_reward": 3.105609375, "actor_loss": -350.9893703613281, "actor_target_entropy": -2.0, "actor_entropy": 0.009961234852671624, "alpha_loss": 0.007252450352534651, "alpha_value": 0.03249524307718175, "duration": 84.524569272995, "step": 51500}
{"episode_reward": 937.0, "episode": 207.0, "Q1 loss": 2.591761059761047, "Q2 loss": 2.583923756599426, "Mean Target Q": 350.8670997314453, "Mean Q1": 350.8658946533203, "Mean Q2": 350.8650852050781, "critic_loss": 5.1756848001480105, "batch_reward": 3.1105625, "actor_loss": -351.46622021484376, "actor_target_entropy": -2.0, "actor_entropy": 0.05243644953891635, "alpha_loss": 0.008321952424012125, "alpha_value": 0.03220354001342114, "duration": 84.49516463279724, "step": 51750}
{"episode_reward": 988.0, "episode": 208.0, "Q1 loss": 2.572083252906799, "Q2 loss": 2.5788159494400023, "Mean Target Q": 351.32656970214845, "Mean Q1": 351.32716040039065, "Mean Q2": 351.3275684814453, "critic_loss": 5.15089920425415, "batch_reward": 3.1148359375, "actor_loss": -351.9708713378906, "actor_target_entropy": -2.0, "actor_entropy": -0.012438021272420883, "alpha_loss": 0.0050832962291315195, "alpha_value": 0.031941451236064386, "duration": 91.87408065795898, "step": 52000}
{"episode_reward": 949.0, "episode": 209.0, "Q1 loss": 2.534086810588837, "Q2 loss": 2.558767556667328, "Mean Target Q": 351.68780908203127, "Mean Q1": 351.68583520507815, "Mean Q2": 351.68598413085937, "critic_loss": 5.092854350090027, "batch_reward": 3.1108046875, "actor_loss": -352.21699462890626, "actor_target_entropy": -2.0, "actor_entropy": 0.11924044094979763, "alpha_loss": 0.00488056705147028, "alpha_value": 0.03174725995574414, "duration": 84.2762930393219, "step": 52250}
{"episode_reward": 931.0, "episode": 210.0, "Q1 loss": 2.6795488896369934, "Q2 loss": 2.715070013046265, "Mean Target Q": 352.2081579589844, "Mean Q1": 352.2080050048828, "Mean Q2": 352.2071701660156, "critic_loss": 5.394618893623352, "batch_reward": 3.1212109375, "actor_loss": -352.7305166015625, "actor_target_entropy": -2.0, "actor_entropy": 0.1311482048779726, "alpha_loss": 0.006278981237206608, "alpha_value": 0.03152721755151691, "duration": 85.23488545417786, "step": 52500}
{"episode_reward": 897.0, "episode": 211.0, "Q1 loss": 2.7396209392547606, "Q2 loss": 2.6944454598426817, "Mean Target Q": 352.5780017089844, "Mean Q1": 352.57622216796875, "Mean Q2": 352.5767003173828, "critic_loss": 5.434066395759583, "batch_reward": 3.122484375, "actor_loss": -353.16319360351565, "actor_target_entropy": -2.0, "actor_entropy": -0.02501147136464715, "alpha_loss": 0.003533510530833155, "alpha_value": 0.031329736189593026, "duration": 84.59717440605164, "step": 52750}
{"episode_reward": 946.0, "episode": 212.0, "Q1 loss": 2.6179084601402285, "Q2 loss": 2.590129106044769, "Mean Target Q": 352.98936669921875, "Mean Q1": 352.9865333251953, "Mean Q2": 352.98618115234376, "critic_loss": 5.208037569999695, "batch_reward": 3.1279921875, "actor_loss": -353.59732446289064, "actor_target_entropy": -2.0, "actor_entropy": 0.06573386651277542, "alpha_loss": 0.005859709415351972, "alpha_value": 0.03112617401011998, "duration": 85.12882781028748, "step": 53000}
{"episode_reward": 987.0, "episode": 213.0, "Q1 loss": 2.5899042410850526, "Q2 loss": 2.598706857204437, "Mean Target Q": 353.39998669433595, "Mean Q1": 353.40078332519533, "Mean Q2": 353.40100732421877, "critic_loss": 5.188611084938049, "batch_reward": 3.1295234375, "actor_loss": -353.9224279785156, "actor_target_entropy": -2.0, "actor_entropy": 0.01954676453769207, "alpha_loss": 0.007643794180825353, "alpha_value": 0.03083610515960346, "duration": 85.52216362953186, "step": 53250}
{"episode_reward": 923.0, "episode": 214.0, "Q1 loss": 2.705567089557648, "Q2 loss": 2.703001303195953, "Mean Target Q": 353.69258972167967, "Mean Q1": 353.69087280273436, "Mean Q2": 353.69105627441405, "critic_loss": 5.408568385124206, "batch_reward": 3.1235390625, "actor_loss": -354.27839086914065, "actor_target_entropy": -2.0, "actor_entropy": 0.010965469755232334, "alpha_loss": 0.00886545545887202, "alpha_value": 0.03045620103943737, "duration": 84.2775411605835, "step": 53500}
{"episode_reward": 881.0, "episode": 215.0, "Q1 loss": 2.888471390247345, "Q2 loss": 2.8706046962738037, "Mean Target Q": 354.0505115966797, "Mean Q1": 354.0489725341797, "Mean Q2": 354.0486248779297, "critic_loss": 5.759076097488403, "batch_reward": 3.1277734375, "actor_loss": -354.5265283203125, "actor_target_entropy": -2.0, "actor_entropy": -0.036338112995028496, "alpha_loss": 0.005949663273990154, "alpha_value": 0.03017593919716963, "duration": 84.23562479019165, "step": 53750}
{"episode_reward": 928.0, "episode": 216.0, "Q1 loss": 2.8461710600852967, "Q2 loss": 2.8703284087181093, "Mean Target Q": 354.49902099609375, "Mean Q1": 354.4970703125, "Mean Q2": 354.4977989501953, "critic_loss": 5.7164994802474975, "batch_reward": 3.13478125, "actor_loss": -355.04606103515624, "actor_target_entropy": -2.0, "actor_entropy": -0.03526464295387268, "alpha_loss": 0.0063778052791021765, "alpha_value": 0.029881971422531583, "duration": 84.08899641036987, "step": 54000}
{"episode_reward": 990.0, "episode": 217.0, "Q1 loss": 2.7691873302459715, "Q2 loss": 2.780853909492493, "Mean Target Q": 354.9099133300781, "Mean Q1": 354.9095166015625, "Mean Q2": 354.90936474609373, "critic_loss": 5.550041241645813, "batch_reward": 3.1412109375, "actor_loss": -355.3681745605469, "actor_target_entropy": -2.0, "actor_entropy": -0.006050738722085953, "alpha_loss": 0.0038889192461501808, "alpha_value": 0.02970987558766234, "duration": 84.90058302879333, "step": 54250}
{"episode_reward": 988.0, "episode": 218.0, "Q1 loss": 2.761203966140747, "Q2 loss": 2.7442044472694396, "Mean Target Q": 355.2313142089844, "Mean Q1": 355.22868408203124, "Mean Q2": 355.2293603515625, "critic_loss": 5.505408415794372, "batch_reward": 3.1414375, "actor_loss": -355.73815283203123, "actor_target_entropy": -2.0, "actor_entropy": -0.028913757219910623, "alpha_loss": 0.005827328557148576, "alpha_value": 0.029468387362615055, "duration": 84.05004835128784, "step": 54500}
{"episode_reward": 947.0, "episode": 219.0, "Q1 loss": 2.708010901927948, "Q2 loss": 2.6760281267166137, "Mean Target Q": 355.67152990722656, "Mean Q1": 355.67227734375, "Mean Q2": 355.67170666503904, "critic_loss": 5.384039029121399, "batch_reward": 3.14840625, "actor_loss": -356.1656643066406, "actor_target_entropy": -2.0, "actor_entropy": -0.06651726277917623, "alpha_loss": 0.005545796468853951, "alpha_value": 0.02925547507716329, "duration": 84.7604591846466, "step": 54750}
{"episode_reward": 959.0, "episode": 220.0, "Q1 loss": 2.553270691394806, "Q2 loss": 2.548653001308441, "Mean Target Q": 356.0342041015625, "Mean Q1": 356.03253051757815, "Mean Q2": 356.03293701171873, "critic_loss": 5.101923685073853, "batch_reward": 3.1525859375, "actor_loss": -356.5374968261719, "actor_target_entropy": -2.0, "actor_entropy": -0.026679908230900766, "alpha_loss": 0.005778796709142625, "alpha_value": 0.029035440579290077, "step": 55000}
{"duration": 108.07659840583801, "step": 55000}
{"episode_reward": 948.0, "episode": 221.0, "Q1 loss": 2.5587443079948424, "Q2 loss": 2.551826877593994, "Mean Target Q": 356.3995200195312, "Mean Q1": 356.39946032714846, "Mean Q2": 356.39883642578127, "critic_loss": 5.110571174621582, "batch_reward": 3.15121875, "actor_loss": -356.90919799804686, "actor_target_entropy": -2.0, "actor_entropy": -0.030080245517194272, "alpha_loss": 0.0051408636043779555, "alpha_value": 0.028771218952409496, "duration": 83.51651787757874, "step": 55250}
{"episode_reward": 976.0, "episode": 222.0, "Q1 loss": 2.364583948135376, "Q2 loss": 2.3346053042411805, "Mean Target Q": 356.81454736328124, "Mean Q1": 356.8123039550781, "Mean Q2": 356.81294360351563, "critic_loss": 4.699189251899719, "batch_reward": 3.1558984375, "actor_loss": -357.3541953125, "actor_target_entropy": -2.0, "actor_entropy": -0.055484052032232285, "alpha_loss": 0.005018995477585122, "alpha_value": 0.028560667040471135, "duration": 83.52982425689697, "step": 55500}
{"episode_reward": 954.0, "episode": 223.0, "Q1 loss": 2.4457171149253845, "Q2 loss": 2.418343551158905, "Mean Target Q": 357.3246396484375, "Mean Q1": 357.32455126953124, "Mean Q2": 357.32462854003904, "critic_loss": 4.864060653686524, "batch_reward": 3.1606328125, "actor_loss": -357.8456728515625, "actor_target_entropy": -2.0, "actor_entropy": 0.044197676829993725, "alpha_loss": 0.006592262354679405, "alpha_value": 0.028287292491748304, "duration": 83.76490688323975, "step": 55750}
{"episode_reward": 954.0, "episode": 224.0, "Q1 loss": 2.3817041096687315, "Q2 loss": 2.345167989730835, "Mean Target Q": 357.68927075195313, "Mean Q1": 357.6876773681641, "Mean Q2": 357.6873940429688, "critic_loss": 4.726872111320495, "batch_reward": 3.1607890625, "actor_loss": -358.25094165039064, "actor_target_entropy": -2.0, "actor_entropy": -0.0057442903444170955, "alpha_loss": 0.005754011909943074, "alpha_value": 0.028026137338825856, "duration": 169.71412539482117, "step": 56000}
{"episode_reward": 954.0, "episode": 225.0, "Q1 loss": 2.5032233324050903, "Q2 loss": 2.475188391685486, "Mean Target Q": 358.01189001464843, "Mean Q1": 358.01064221191405, "Mean Q2": 358.0108350830078, "critic_loss": 4.978411712646484, "batch_reward": 3.1565546875, "actor_loss": -358.50786181640626, "actor_target_entropy": -2.0, "actor_entropy": -0.13650943261012435, "alpha_loss": 0.0044599841162562374, "alpha_value": 0.027788841982721663, "duration": 229.90191650390625, "step": 56250}
{"episode_reward": 950.0, "episode": 226.0, "Q1 loss": 2.446427809715271, "Q2 loss": 2.425496284484863, "Mean Target Q": 358.4557337646484, "Mean Q1": 358.45445532226563, "Mean Q2": 358.4547109375, "critic_loss": 4.871924080848694, "batch_reward": 3.1672265625, "actor_loss": -358.90755126953127, "actor_target_entropy": -2.0, "actor_entropy": -0.12114086555689574, "alpha_loss": 0.004800778010394424, "alpha_value": 0.027569092332318284, "duration": 83.67109632492065, "step": 56500}
{"episode_reward": 987.0, "episode": 227.0, "Q1 loss": 2.7769324989318847, "Q2 loss": 2.7843125920295715, "Mean Target Q": 358.7668088378906, "Mean Q1": 358.76668615722656, "Mean Q2": 358.7661984863281, "critic_loss": 5.561245098114013, "batch_reward": 3.1682265625, "actor_loss": -359.3675446777344, "actor_target_entropy": -2.0, "actor_entropy": -0.18990345293283462, "alpha_loss": 0.00410044765600469, "alpha_value": 0.02736270054588096, "duration": 84.12935304641724, "step": 56750}
{"episode_reward": 989.0, "episode": 228.0, "Q1 loss": 2.4480699830055235, "Q2 loss": 2.4698047194480894, "Mean Target Q": 359.15330895996095, "Mean Q1": 359.1522393798828, "Mean Q2": 359.15261486816405, "critic_loss": 4.917874711036682, "batch_reward": 3.171796875, "actor_loss": -359.6176379394531, "actor_target_entropy": -2.0, "actor_entropy": -0.12719292324781417, "alpha_loss": 0.0033903758195228876, "alpha_value": 0.02720620051560664, "duration": 84.13719320297241, "step": 57000}
{"episode_reward": 955.0, "episode": 229.0, "Q1 loss": 2.501533181667328, "Q2 loss": 2.474991303443909, "Mean Target Q": 359.55129833984375, "Mean Q1": 359.55079040527346, "Mean Q2": 359.5500061035156, "critic_loss": 4.976524495124817, "batch_reward": 3.178921875, "actor_loss": -360.0067077636719, "actor_target_entropy": -2.0, "actor_entropy": -0.06504034163057804, "alpha_loss": 0.0049275244411546735, "alpha_value": 0.02699190729111309, "duration": 85.40883183479309, "step": 57250}
{"episode_reward": 956.0, "episode": 230.0, "Q1 loss": 2.5890549893379213, "Q2 loss": 2.565235781908035, "Mean Target Q": 359.7598526611328, "Mean Q1": 359.75846130371093, "Mean Q2": 359.7593070068359, "critic_loss": 5.154290758132935, "batch_reward": 3.171171875, "actor_loss": -360.2637526855469, "actor_target_entropy": -2.0, "actor_entropy": -0.10158362023532391, "alpha_loss": 0.003981507433578372, "alpha_value": 0.026795817057292906, "duration": 86.12944102287292, "step": 57500}
{"episode_reward": 960.0, "episode": 231.0, "Q1 loss": 2.4232380928993225, "Q2 loss": 2.4437314820289613, "Mean Target Q": 360.1492999267578, "Mean Q1": 360.1475512695313, "Mean Q2": 360.14702526855467, "critic_loss": 4.866969577789306, "batch_reward": 3.1823359375, "actor_loss": -360.63629833984373, "actor_target_entropy": -2.0, "actor_entropy": -0.11179310046881437, "alpha_loss": 0.004126701045781374, "alpha_value": 0.026583365989583545, "duration": 84.2082953453064, "step": 57750}
{"episode_reward": 956.0, "episode": 232.0, "Q1 loss": 2.4150354180336, "Q2 loss": 2.3534404187202456, "Mean Target Q": 360.6231540527344, "Mean Q1": 360.6219244384766, "Mean Q2": 360.62307690429685, "critic_loss": 4.768475838661194, "batch_reward": 3.19078125, "actor_loss": -361.0648359375, "actor_target_entropy": -2.0, "actor_entropy": -0.097167595397681, "alpha_loss": 0.004467765218578279, "alpha_value": 0.026374903589346826, "duration": 84.78751230239868, "step": 58000}
{"episode_reward": 954.0, "episode": 233.0, "Q1 loss": 2.303136824607849, "Q2 loss": 2.2982597002983094, "Mean Target Q": 360.93039453125, "Mean Q1": 360.9300234375, "Mean Q2": 360.9298420410156, "critic_loss": 4.601396524429322, "batch_reward": 3.194390625, "actor_loss": -361.5580305175781, "actor_target_entropy": -2.0, "actor_entropy": -0.010673464931547642, "alpha_loss": 0.005533699328778311, "alpha_value": 0.02611716353672246, "duration": 86.07926487922668, "step": 58250}
{"episode_reward": 988.0, "episode": 234.0, "Q1 loss": 2.2778158597946168, "Q2 loss": 2.2961712975502016, "Mean Target Q": 361.1975928955078, "Mean Q1": 361.1966148681641, "Mean Q2": 361.19606677246094, "critic_loss": 4.5739871702194215, "batch_reward": 3.1909140625, "actor_loss": -361.81161572265626, "actor_target_entropy": -2.0, "actor_entropy": -0.1488753034695983, "alpha_loss": 0.004179103774600662, "alpha_value": 0.025921810054289215, "duration": 85.00833463668823, "step": 58500}
{"episode_reward": 954.0, "episode": 235.0, "Q1 loss": 2.345970905780792, "Q2 loss": 2.3141526312828065, "Mean Target Q": 361.56513610839846, "Mean Q1": 361.5644591064453, "Mean Q2": 361.56539685058596, "critic_loss": 4.6601235294342045, "batch_reward": 3.194, "actor_loss": -362.1661047363281, "actor_target_entropy": -2.0, "actor_entropy": -0.11270861398428679, "alpha_loss": 0.004879989681532607, "alpha_value": 0.025684130841794554, "duration": 86.67679071426392, "step": 58750}
{"episode_reward": 957.0, "episode": 236.0, "Q1 loss": 2.456207314014435, "Q2 loss": 2.455157989501953, "Mean Target Q": 362.0153526611328, "Mean Q1": 362.01365686035155, "Mean Q2": 362.012767578125, "critic_loss": 4.911365303993225, "batch_reward": 3.1996171875, "actor_loss": -362.54424853515627, "actor_target_entropy": -2.0, "actor_entropy": -0.1982740120217204, "alpha_loss": 0.00400004247110337, "alpha_value": 0.02548225296814894, "duration": 85.32519769668579, "step": 59000}
{"episode_reward": 853.0, "episode": 237.0, "Q1 loss": 2.4855536322593688, "Q2 loss": 2.4385728931427, "Mean Target Q": 362.23562060546874, "Mean Q1": 362.2357264404297, "Mean Q2": 362.2357479248047, "critic_loss": 4.924126534461975, "batch_reward": 3.1963515625, "actor_loss": -362.8089375, "actor_target_entropy": -2.0, "actor_entropy": -0.201968487970531, "alpha_loss": 0.002294220102950931, "alpha_value": 0.02530559818343038, "duration": 85.50635480880737, "step": 59250}
{"episode_reward": 954.0, "episode": 238.0, "Q1 loss": 2.5977290291786193, "Q2 loss": 2.5491768684387206, "Mean Target Q": 362.5652482910156, "Mean Q1": 362.5632692871094, "Mean Q2": 362.56329418945313, "critic_loss": 5.146905903816223, "batch_reward": 3.1953515625, "actor_loss": -363.02548291015626, "actor_target_entropy": -2.0, "actor_entropy": -0.11036757177114487, "alpha_loss": 0.0037889460339210926, "alpha_value": 0.02519320638251066, "duration": 83.8431875705719, "step": 59500}
{"episode_reward": 956.0, "episode": 239.0, "Q1 loss": 2.2461401553153992, "Q2 loss": 2.210802459716797, "Mean Target Q": 362.92611462402346, "Mean Q1": 362.9247015380859, "Mean Q2": 362.92509350585937, "critic_loss": 4.456942608833313, "batch_reward": 3.20553125, "actor_loss": -363.44274536132815, "actor_target_entropy": -2.0, "actor_entropy": -0.15492903110384942, "alpha_loss": 0.0033328343897592277, "alpha_value": 0.024986784208114836, "duration": 85.34849500656128, "step": 59750}
{"episode_reward": 991.0, "episode": 240.0, "Q1 loss": 2.110868559360504, "Q2 loss": 2.080328847408295, "Mean Target Q": 363.373134765625, "Mean Q1": 363.3725682373047, "Mean Q2": 363.3723720703125, "critic_loss": 4.19119739818573, "batch_reward": 3.214375, "actor_loss": -363.75580444335935, "actor_target_entropy": -2.0, "actor_entropy": -0.13826795786619187, "alpha_loss": 0.001486525336280465, "alpha_value": 0.02485986565259283, "step": 60000}
{"duration": 109.01117300987244, "step": 60000}
{"episode_reward": 991.0, "episode": 241.0, "Q1 loss": 2.1033090739250184, "Q2 loss": 2.0856754040718077, "Mean Target Q": 363.6455598144531, "Mean Q1": 363.64633935546874, "Mean Q2": 363.64662707519534, "critic_loss": 4.1889844789505, "batch_reward": 3.2111875, "actor_loss": -364.15681494140625, "actor_target_entropy": -2.0, "actor_entropy": -0.0967810623049736, "alpha_loss": 0.003634928401326761, "alpha_value": 0.024742397985820296, "duration": 91.78557991981506, "step": 60250}
{"episode_reward": 955.0, "episode": 242.0, "Q1 loss": 2.1276457271575926, "Q2 loss": 2.1056762018203736, "Mean Target Q": 363.99761328125, "Mean Q1": 363.9955379638672, "Mean Q2": 363.9954278564453, "critic_loss": 4.233321932315826, "batch_reward": 3.2177890625, "actor_loss": -364.40644970703124, "actor_target_entropy": -2.0, "actor_entropy": -0.08202708529680967, "alpha_loss": 0.004242153517901898, "alpha_value": 0.024506341571308415, "duration": 83.80112171173096, "step": 60500}
{"episode_reward": 951.0, "episode": 243.0, "Q1 loss": 2.1667969403266905, "Q2 loss": 2.1483456597328185, "Mean Target Q": 364.1980382080078, "Mean Q1": 364.19660095214846, "Mean Q2": 364.19706384277345, "critic_loss": 4.315142593383789, "batch_reward": 3.209140625, "actor_loss": -364.75384619140624, "actor_target_entropy": -2.0, "actor_entropy": -0.13242114993929863, "alpha_loss": 0.0036431665965355933, "alpha_value": 0.024325641340154566, "duration": 87.02528119087219, "step": 60750}
{"episode_reward": 992.0, "episode": 244.0, "Q1 loss": 2.1106122217178345, "Q2 loss": 2.1006299934387207, "Mean Target Q": 364.5814835205078, "Mean Q1": 364.5805085449219, "Mean Q2": 364.57991650390625, "critic_loss": 4.211242221355438, "batch_reward": 3.221328125, "actor_loss": -364.9895920410156, "actor_target_entropy": -2.0, "actor_entropy": -0.11926326136291027, "alpha_loss": 0.0028378380155190826, "alpha_value": 0.024158456945026597, "duration": 147.07896971702576, "step": 61000}
{"episode_reward": 994.0, "episode": 245.0, "Q1 loss": 2.1034461402893068, "Q2 loss": 2.092215198278427, "Mean Target Q": 364.82664880371095, "Mean Q1": 364.82537634277344, "Mean Q2": 364.8258760986328, "critic_loss": 4.195661343574524, "batch_reward": 3.221671875, "actor_loss": -365.2686120605469, "actor_target_entropy": -2.0, "actor_entropy": -0.1571059421300888, "alpha_loss": 0.0021397196655161678, "alpha_value": 0.024005024152897816, "duration": 119.10359334945679, "step": 61250}
{"episode_reward": 954.0, "episode": 246.0, "Q1 loss": 2.0548362061977388, "Q2 loss": 2.0369893341064453, "Mean Target Q": 365.0108220214844, "Mean Q1": 365.00983850097657, "Mean Q2": 365.00953161621095, "critic_loss": 4.091825543403625, "batch_reward": 3.2135234375, "actor_loss": -365.4177097167969, "actor_target_entropy": -2.0, "actor_entropy": -0.12250818791240453, "alpha_loss": 0.00241340984008275, "alpha_value": 0.023896740207474027, "duration": 85.47547483444214, "step": 61500}
{"episode_reward": 955.0, "episode": 247.0, "Q1 loss": 2.2347294363975525, "Q2 loss": 2.1909275774955748, "Mean Target Q": 365.39222741699217, "Mean Q1": 365.3919381103516, "Mean Q2": 365.39240173339846, "critic_loss": 4.425657006263733, "batch_reward": 3.2213359375, "actor_loss": -365.8524338378906, "actor_target_entropy": -2.0, "actor_entropy": -0.09480446258932353, "alpha_loss": 0.00046025187708437443, "alpha_value": 0.023812525247196466, "duration": 86.65614056587219, "step": 61750}
{"episode_reward": 958.0, "episode": 248.0, "Q1 loss": 2.1238207187652587, "Q2 loss": 2.114489456176758, "Mean Target Q": 365.84543383789065, "Mean Q1": 365.84359301757814, "Mean Q2": 365.8438809814453, "critic_loss": 4.238310173988342, "batch_reward": 3.2325859375, "actor_loss": -366.40372802734373, "actor_target_entropy": -2.0, "actor_entropy": -0.07280208586156368, "alpha_loss": 0.0015417595161125064, "alpha_value": 0.02373242823864871, "duration": 89.04334163665771, "step": 62000}
{"episode_reward": 988.0, "episode": 249.0, "Q1 loss": 2.057465448141098, "Q2 loss": 2.0240030465126035, "Mean Target Q": 365.9979686279297, "Mean Q1": 365.9980020751953, "Mean Q2": 365.99743017578123, "critic_loss": 4.081468502998352, "batch_reward": 3.2218046875, "actor_loss": -366.4643283691406, "actor_target_entropy": -2.0, "actor_entropy": -0.06990280408412218, "alpha_loss": 0.00289097561663948, "alpha_value": 0.023630022919415926, "duration": 89.01414489746094, "step": 62250}
{"episode_reward": 956.0, "episode": 250.0, "Q1 loss": 1.893081910610199, "Q2 loss": 1.8924372820854187, "Mean Target Q": 366.41874353027345, "Mean Q1": 366.41905212402344, "Mean Q2": 366.4190592041016, "critic_loss": 3.785519190311432, "batch_reward": 3.2350078125, "actor_loss": -366.8693291015625, "actor_target_entropy": -2.0, "actor_entropy": -0.17091291046887636, "alpha_loss": 0.00041568420967087147, "alpha_value": 0.02353198115795068, "duration": 87.27714896202087, "step": 62500}
{"episode_reward": 958.0, "episode": 251.0, "Q1 loss": 1.9049369852542877, "Q2 loss": 1.9113481674194337, "Mean Target Q": 366.7320313720703, "Mean Q1": 366.73024975585935, "Mean Q2": 366.7306550292969, "critic_loss": 3.8162851457595823, "batch_reward": 3.2356796875, "actor_loss": -367.1450368652344, "actor_target_entropy": -2.0, "actor_entropy": -0.0628774253949523, "alpha_loss": 0.0035379520477727055, "alpha_value": 0.023416538882022695, "duration": 84.38718724250793, "step": 62750}
{"episode_reward": 996.0, "episode": 252.0, "Q1 loss": 2.016512590408325, "Q2 loss": 2.0025915393829345, "Mean Target Q": 366.97137841796877, "Mean Q1": 366.97070275878906, "Mean Q2": 366.9704807128906, "critic_loss": 4.0191041288375855, "batch_reward": 3.240625, "actor_loss": -367.3471833496094, "actor_target_entropy": -2.0, "actor_entropy": -0.11838483414053917, "alpha_loss": 0.0017375318222912029, "alpha_value": 0.023250251242905407, "duration": 88.07172799110413, "step": 63000}
{"episode_reward": 989.0, "episode": 253.0, "Q1 loss": 2.1529373083114622, "Q2 loss": 2.1177314586639406, "Mean Target Q": 367.23070837402344, "Mean Q1": 367.2299027099609, "Mean Q2": 367.23014099121093, "critic_loss": 4.270668766021728, "batch_reward": 3.243984375, "actor_loss": -367.65248974609375, "actor_target_entropy": -2.0, "actor_entropy": -0.10353873596340418, "alpha_loss": 0.0026593765313737095, "alpha_value": 0.023106511729271254, "duration": 88.99459600448608, "step": 63250}
{"episode_reward": 983.0, "episode": 254.0, "Q1 loss": 2.0397626371383666, "Q2 loss": 2.012898447036743, "Mean Target Q": 367.36247326660157, "Mean Q1": 367.36200390625, "Mean Q2": 367.36198205566404, "critic_loss": 4.052661093711853, "batch_reward": 3.2385625, "actor_loss": -367.9633527832031, "actor_target_entropy": -2.0, "actor_entropy": -0.14445844838768243, "alpha_loss": 0.004485371850896627, "alpha_value": 0.022920399073072476, "duration": 89.69808578491211, "step": 63500}
{"episode_reward": 958.0, "episode": 255.0, "Q1 loss": 2.0477461104393004, "Q2 loss": 2.0274911422729494, "Mean Target Q": 367.72583068847655, "Mean Q1": 367.7242907714844, "Mean Q2": 367.7246500244141, "critic_loss": 4.075237257003784, "batch_reward": 3.2475703125, "actor_loss": -368.163388671875, "actor_target_entropy": -2.0, "actor_entropy": -0.0710506729632616, "alpha_loss": 0.004488050483632833, "alpha_value": 0.022658401941748237, "duration": 86.62927055358887, "step": 63750}
{"episode_reward": 953.0, "episode": 256.0, "Q1 loss": 2.0546091074943544, "Q2 loss": 2.037369663000107, "Mean Target Q": 367.966634765625, "Mean Q1": 367.96536779785157, "Mean Q2": 367.9650549316406, "critic_loss": 4.0919787774086, "batch_reward": 3.2471328125, "actor_loss": -368.51769311523435, "actor_target_entropy": -2.0, "actor_entropy": -0.14495984655618668, "alpha_loss": 0.0027473039634060116, "alpha_value": 0.022474055980210947, "duration": 87.61844778060913, "step": 64000}
{"episode_reward": 954.0, "episode": 257.0, "Q1 loss": 2.015457627058029, "Q2 loss": 1.968776494026184, "Mean Target Q": 368.20811157226564, "Mean Q1": 368.2070037841797, "Mean Q2": 368.20686499023435, "critic_loss": 3.984234127998352, "batch_reward": 3.2486328125, "actor_loss": -368.7133889160156, "actor_target_entropy": -2.0, "actor_entropy": -0.09642610766738653, "alpha_loss": 0.0035081987108569594, "alpha_value": 0.02229367224947356, "duration": 90.46960926055908, "step": 64250}
{"episode_reward": 988.0, "episode": 258.0, "Q1 loss": 1.9438520350456239, "Q2 loss": 1.9342963318824768, "Mean Target Q": 368.5177926025391, "Mean Q1": 368.5174362792969, "Mean Q2": 368.51779736328126, "critic_loss": 3.8781483573913573, "batch_reward": 3.2523828125, "actor_loss": -368.97777465820315, "actor_target_entropy": -2.0, "actor_entropy": -0.1279413469955325, "alpha_loss": 0.003726573634892702, "alpha_value": 0.022064773469161764, "duration": 89.30036807060242, "step": 64500}
{"episode_reward": 940.0, "episode": 259.0, "Q1 loss": 1.9072663493156432, "Q2 loss": 1.912628350019455, "Mean Target Q": 368.74464306640624, "Mean Q1": 368.74273486328127, "Mean Q2": 368.74241027832034, "critic_loss": 3.8198946943283083, "batch_reward": 3.249296875, "actor_loss": -369.2132822265625, "actor_target_entropy": -2.0, "actor_entropy": -0.15011182457581163, "alpha_loss": 0.0027425984549336135, "alpha_value": 0.02189364694809608, "duration": 87.14298939704895, "step": 64750}
{"episode_reward": 956.0, "episode": 260.0, "Q1 loss": 1.9381700298786164, "Q2 loss": 1.9278281354904174, "Mean Target Q": 369.06660388183593, "Mean Q1": 369.0673653564453, "Mean Q2": 369.06746020507813, "critic_loss": 3.8659981632232667, "batch_reward": 3.25784375, "actor_loss": -369.47577954101564, "actor_target_entropy": -2.0, "actor_entropy": -0.07941614878922701, "alpha_loss": 0.0016522717180196195, "alpha_value": 0.021781634889695627, "step": 65000}
{"duration": 114.69771456718445, "step": 65000}
{"episode_reward": 958.0, "episode": 261.0, "Q1 loss": 1.898116285800934, "Q2 loss": 1.8877794318199157, "Mean Target Q": 369.33870629882813, "Mean Q1": 369.3361414794922, "Mean Q2": 369.33631713867186, "critic_loss": 3.7858957147598264, "batch_reward": 3.263859375, "actor_loss": -369.7810004882812, "actor_target_entropy": -2.0, "actor_entropy": -0.07089823041856289, "alpha_loss": 0.002270594768691808, "alpha_value": 0.021655232190571225, "duration": 88.23136520385742, "step": 65250}
{"episode_reward": 954.0, "episode": 262.0, "Q1 loss": 1.8849856009483337, "Q2 loss": 1.8989094061851501, "Mean Target Q": 369.56063806152343, "Mean Q1": 369.56154736328125, "Mean Q2": 369.5617097167969, "critic_loss": 3.7838949971199036, "batch_reward": 3.2622578125, "actor_loss": -370.0836027832031, "actor_target_entropy": -2.0, "actor_entropy": -0.1447605362534523, "alpha_loss": 0.001964278978994116, "alpha_value": 0.021565076622287887, "duration": 91.67554831504822, "step": 65500}
{"episode_reward": 956.0, "episode": 263.0, "Q1 loss": 1.8253265895843507, "Q2 loss": 1.7760892217159272, "Mean Target Q": 369.79042102050784, "Mean Q1": 369.7897053222656, "Mean Q2": 369.7895743408203, "critic_loss": 3.601415802001953, "batch_reward": 3.2630625, "actor_loss": -370.2621071777344, "actor_target_entropy": -2.0, "actor_entropy": -0.08507233632355929, "alpha_loss": 0.0024740589004941284, "alpha_value": 0.021421164314458507, "duration": 88.46872472763062, "step": 65750}
{"episode_reward": 984.0, "episode": 264.0, "Q1 loss": 1.7311421153545379, "Q2 loss": 1.7049403195381165, "Mean Target Q": 370.1511190185547, "Mean Q1": 370.1504020996094, "Mean Q2": 370.1512735595703, "critic_loss": 3.436082432746887, "batch_reward": 3.2738984375, "actor_loss": -370.6127197265625, "actor_target_entropy": -2.0, "actor_entropy": -0.20852421713620425, "alpha_loss": 0.00269342435663566, "alpha_value": 0.021280752724288213, "duration": 90.02784585952759, "step": 66000}
{"episode_reward": 988.0, "episode": 265.0, "Q1 loss": 1.6357548711299896, "Q2 loss": 1.640522463798523, "Mean Target Q": 370.28433947753905, "Mean Q1": 370.2822559814453, "Mean Q2": 370.28182263183595, "critic_loss": 3.2762773327827452, "batch_reward": 3.2678515625, "actor_loss": -370.67586840820314, "actor_target_entropy": -2.0, "actor_entropy": -0.2180316606760025, "alpha_loss": 0.0035739700604462995, "alpha_value": 0.02108309630189031, "duration": 92.34522533416748, "step": 66250}
{"episode_reward": 948.0, "episode": 266.0, "Q1 loss": 1.7844258618354798, "Q2 loss": 1.796850045442581, "Mean Target Q": 370.53590441894534, "Mean Q1": 370.5353842773437, "Mean Q2": 370.5360625, "critic_loss": 3.5812759079933167, "batch_reward": 3.2702734375, "actor_loss": -371.03654541015624, "actor_target_entropy": -2.0, "actor_entropy": -0.08136908145993949, "alpha_loss": 0.004673815294168889, "alpha_value": 0.020849567286930538, "duration": 91.72829008102417, "step": 66500}
{"episode_reward": 959.0, "episode": 267.0, "Q1 loss": 1.7448498888015747, "Q2 loss": 1.7562047529220581, "Mean Target Q": 370.67527978515625, "Mean Q1": 370.67545837402344, "Mean Q2": 370.67430126953127, "critic_loss": 3.501054635047913, "batch_reward": 3.2658671875, "actor_loss": -371.1522106933594, "actor_target_entropy": -2.0, "actor_entropy": -0.15148857345432043, "alpha_loss": 0.0014911639862693845, "alpha_value": 0.020684618878815862, "duration": 90.42285561561584, "step": 66750}
{"episode_reward": 988.0, "episode": 268.0, "Q1 loss": 1.932502863883972, "Q2 loss": 1.922914662361145, "Mean Target Q": 370.9926701660156, "Mean Q1": 370.99070666503906, "Mean Q2": 370.99134362792967, "critic_loss": 3.855417516708374, "batch_reward": 3.2744296875, "actor_loss": -371.49562329101565, "actor_target_entropy": -2.0, "actor_entropy": -0.13740409574657678, "alpha_loss": 0.002394453253597021, "alpha_value": 0.02059922154318207, "duration": 85.60927248001099, "step": 67000}
{"episode_reward": 868.0, "episode": 269.0, "Q1 loss": 1.9242277607917786, "Q2 loss": 1.8967156517505646, "Mean Target Q": 371.1697694091797, "Mean Q1": 371.1689177246094, "Mean Q2": 371.1691162109375, "critic_loss": 3.820943419933319, "batch_reward": 3.2722421875, "actor_loss": -371.5311513671875, "actor_target_entropy": -2.0, "actor_entropy": -0.031079458750784397, "alpha_loss": 0.0032359883659519254, "alpha_value": 0.02042396051143022, "duration": 87.55947232246399, "step": 67250}
{"episode_reward": 928.0, "episode": 270.0, "Q1 loss": 1.7935695548057555, "Q2 loss": 1.802177806377411, "Mean Target Q": 371.4302725830078, "Mean Q1": 371.4298919677734, "Mean Q2": 371.4291203613281, "critic_loss": 3.5957473621368408, "batch_reward": 3.27784375, "actor_loss": -371.87084741210936, "actor_target_entropy": -2.0, "actor_entropy": -0.11617149137705564, "alpha_loss": 0.002763173022074625, "alpha_value": 0.020277528607742318, "duration": 86.50984263420105, "step": 67500}
{"episode_reward": 934.0, "episode": 271.0, "Q1 loss": 1.8303542919158935, "Q2 loss": 1.81262127327919, "Mean Target Q": 371.69820959472656, "Mean Q1": 371.69720361328126, "Mean Q2": 371.69799536132814, "critic_loss": 3.642975562572479, "batch_reward": 3.285359375, "actor_loss": -372.16233520507814, "actor_target_entropy": -2.0, "actor_entropy": -0.1673995453566313, "alpha_loss": 0.0016079016488511116, "alpha_value": 0.020141631045074136, "duration": 87.19517302513123, "step": 67750}
{"episode_reward": 949.0, "episode": 272.0, "Q1 loss": 1.8008783211708068, "Q2 loss": 1.8277654402256012, "Mean Target Q": 371.86816247558596, "Mean Q1": 371.8665411376953, "Mean Q2": 371.8662214355469, "critic_loss": 3.6286437644958496, "batch_reward": 3.279375, "actor_loss": -372.25681762695314, "actor_target_entropy": -2.0, "actor_entropy": -0.12297983603924513, "alpha_loss": 0.0037197542020585388, "alpha_value": 0.020006890648837282, "duration": 87.69104814529419, "step": 68000}
{"episode_reward": 990.0, "episode": 273.0, "Q1 loss": 1.9607547857761383, "Q2 loss": 1.9409322381019591, "Mean Target Q": 372.12912109375, "Mean Q1": 372.1298004150391, "Mean Q2": 372.1300090332031, "critic_loss": 3.9016870217323305, "batch_reward": 3.2873046875, "actor_loss": -372.5184914550781, "actor_target_entropy": -2.0, "actor_entropy": -0.1924485452696681, "alpha_loss": 0.0017113543561426922, "alpha_value": 0.019843778437794998, "duration": 87.54002737998962, "step": 68250}
{"episode_reward": 948.0, "episode": 274.0, "Q1 loss": 1.9910897636413574, "Q2 loss": 1.9768154420852662, "Mean Target Q": 372.20791735839845, "Mean Q1": 372.2068525390625, "Mean Q2": 372.2070686035156, "critic_loss": 3.9679052081108095, "batch_reward": 3.286078125, "actor_loss": -372.6639375, "actor_target_entropy": -2.0, "actor_entropy": -0.32857316720485685, "alpha_loss": 0.0008164915585657581, "alpha_value": 0.019764224104761384, "duration": 93.04531955718994, "step": 68500}
{"episode_reward": 956.0, "episode": 275.0, "Q1 loss": 1.8825824582576751, "Q2 loss": 1.8607451622486115, "Mean Target Q": 372.46229602050784, "Mean Q1": 372.46079431152344, "Mean Q2": 372.4606281738281, "critic_loss": 3.743327629566193, "batch_reward": 3.2911328125, "actor_loss": -372.85328344726565, "actor_target_entropy": -2.0, "actor_entropy": -0.2132319560982287, "alpha_loss": 3.001792402938008e-05, "alpha_value": 0.019749049834428278, "duration": 84.10691952705383, "step": 68750}
{"episode_reward": 950.0, "episode": 276.0, "Q1 loss": 1.8847107126712799, "Q2 loss": 1.850237678527832, "Mean Target Q": 372.6168988037109, "Mean Q1": 372.6176804199219, "Mean Q2": 372.61764135742186, "critic_loss": 3.7349483985900878, "batch_reward": 3.2900546875, "actor_loss": -373.07052099609376, "actor_target_entropy": -2.0, "actor_entropy": -0.16995984046906232, "alpha_loss": 0.001842625782592222, "alpha_value": 0.01968942382727823, "duration": 83.83303666114807, "step": 69000}
{"episode_reward": 940.0, "episode": 277.0, "Q1 loss": 1.8732540805339812, "Q2 loss": 1.8207258377075195, "Mean Target Q": 372.82926086425783, "Mean Q1": 372.82721740722656, "Mean Q2": 372.8276848144531, "critic_loss": 3.6939799060821534, "batch_reward": 3.296171875, "actor_loss": -373.2352976074219, "actor_target_entropy": -2.0, "actor_entropy": -0.15793950877338647, "alpha_loss": 0.001704756871331483, "alpha_value": 0.019575651342066732, "duration": 83.83421921730042, "step": 69250}
{"episode_reward": 957.0, "episode": 278.0, "Q1 loss": 1.8256855993270873, "Q2 loss": 1.8543792080879211, "Mean Target Q": 372.91950622558596, "Mean Q1": 372.91926733398435, "Mean Q2": 372.91900170898435, "critic_loss": 3.680064809799194, "batch_reward": 3.2903515625, "actor_loss": -373.3364052734375, "actor_target_entropy": -2.0, "actor_entropy": -0.10349540299922227, "alpha_loss": -0.000594971799524501, "alpha_value": 0.019564059359576436, "duration": 83.36314511299133, "step": 69500}
{"episode_reward": 947.0, "episode": 279.0, "Q1 loss": 1.8309323117733, "Q2 loss": 1.8274960522651673, "Mean Target Q": 373.054578125, "Mean Q1": 373.053306640625, "Mean Q2": 373.05268017578123, "critic_loss": 3.6584283628463745, "batch_reward": 3.288234375, "actor_loss": -373.522513671875, "actor_target_entropy": -2.0, "actor_entropy": -0.1688030985891819, "alpha_loss": 0.0015664457166567444, "alpha_value": 0.019528835983912547, "duration": 84.57757115364075, "step": 69750}
{"episode_reward": 955.0, "episode": 280.0, "Q1 loss": 1.8342013220787048, "Q2 loss": 1.8520698246955871, "Mean Target Q": 373.33945666503905, "Mean Q1": 373.3396016845703, "Mean Q2": 373.3401212158203, "critic_loss": 3.686271146774292, "batch_reward": 3.294484375, "actor_loss": -373.7637565917969, "actor_target_entropy": -2.0, "actor_entropy": -0.17249309932440518, "alpha_loss": 0.0006469995966181159, "alpha_value": 0.01944582674676144, "step": 70000}
{"duration": 108.01794862747192, "step": 70000}
{"episode_reward": 950.0, "episode": 281.0, "Q1 loss": 1.8529808402061463, "Q2 loss": 1.834539752960205, "Mean Target Q": 373.6144599609375, "Mean Q1": 373.6134309082031, "Mean Q2": 373.6141455078125, "critic_loss": 3.687520610332489, "batch_reward": 3.297609375, "actor_loss": -374.003337890625, "actor_target_entropy": -2.0, "actor_entropy": -0.22846750691533088, "alpha_loss": -9.798540978226811e-05, "alpha_value": 0.01943665300051111, "duration": 83.80284142494202, "step": 70250}
{"episode_reward": 969.0, "episode": 282.0, "Q1 loss": 1.869967903137207, "Q2 loss": 1.8812817597389222, "Mean Target Q": 373.77227587890627, "Mean Q1": 373.7712666015625, "Mean Q2": 373.771107421875, "critic_loss": 3.751249658584595, "batch_reward": 3.3006484375, "actor_loss": -374.29411279296875, "actor_target_entropy": -2.0, "actor_entropy": -0.15114333956688641, "alpha_loss": 0.000762969312025234, "alpha_value": 0.019405351295103315, "duration": 83.353280544281, "step": 70500}
{"episode_reward": 955.0, "episode": 283.0, "Q1 loss": 1.738508755683899, "Q2 loss": 1.7253639857769012, "Mean Target Q": 373.9789815673828, "Mean Q1": 373.97865368652344, "Mean Q2": 373.978404296875, "critic_loss": 3.4638727464675902, "batch_reward": 3.2993828125, "actor_loss": -374.39177001953124, "actor_target_entropy": -2.0, "actor_entropy": -0.09509718491137027, "alpha_loss": 0.0028950720308348536, "alpha_value": 0.01932252941228261, "duration": 84.16384291648865, "step": 70750}
{"episode_reward": 957.0, "episode": 284.0, "Q1 loss": 1.7947170412540436, "Q2 loss": 1.795741084098816, "Mean Target Q": 374.1836791992188, "Mean Q1": 374.18203198242185, "Mean Q2": 374.18190002441406, "critic_loss": 3.590458137989044, "batch_reward": 3.3033671875, "actor_loss": -374.58627783203127, "actor_target_entropy": -2.0, "actor_entropy": -0.21911616979539394, "alpha_loss": 0.0009713949318975211, "alpha_value": 0.019190228437680965, "duration": 84.61340641975403, "step": 71000}
{"episode_reward": 951.0, "episode": 285.0, "Q1 loss": 1.7041363279819488, "Q2 loss": 1.6797670164108276, "Mean Target Q": 374.4013713378906, "Mean Q1": 374.4015296630859, "Mean Q2": 374.4017083740234, "critic_loss": 3.3839033575057984, "batch_reward": 3.3051640625, "actor_loss": -374.7700510253906, "actor_target_entropy": -2.0, "actor_entropy": -0.33247560454905034, "alpha_loss": -0.0006854102429933846, "alpha_value": 0.019154447402071034, "duration": 84.77345991134644, "step": 71250}
{"episode_reward": 942.0, "episode": 286.0, "Q1 loss": 1.7495078740119934, "Q2 loss": 1.7398801894187927, "Mean Target Q": 374.49033459472656, "Mean Q1": 374.48900280761717, "Mean Q2": 374.4888936767578, "critic_loss": 3.4893880643844604, "batch_reward": 3.3003359375, "actor_loss": -374.98481323242186, "actor_target_entropy": -2.0, "actor_entropy": -0.18598323736339809, "alpha_loss": 0.002144080131081864, "alpha_value": 0.019111636589819764, "duration": 84.94549655914307, "step": 71500}
{"episode_reward": 959.0, "episode": 287.0, "Q1 loss": 1.8580009508132935, "Q2 loss": 1.8489988582134247, "Mean Target Q": 374.7459833984375, "Mean Q1": 374.7451389160156, "Mean Q2": 374.7449797363281, "critic_loss": 3.706999801158905, "batch_reward": 3.3103984375, "actor_loss": -375.13126123046874, "actor_target_entropy": -2.0, "actor_entropy": -0.22327670431137084, "alpha_loss": 0.0012050960892811418, "alpha_value": 0.01902424600058253, "duration": 84.74858283996582, "step": 71750}
{"episode_reward": 949.0, "episode": 288.0, "Q1 loss": 1.8093735485076905, "Q2 loss": 1.7947746605873107, "Mean Target Q": 374.93267919921874, "Mean Q1": 374.9318259277344, "Mean Q2": 374.9322315673828, "critic_loss": 3.6041482071876527, "batch_reward": 3.3099140625, "actor_loss": -375.36974780273437, "actor_target_entropy": -2.0, "actor_entropy": -0.21209884813427926, "alpha_loss": 0.0016392024832312018, "alpha_value": 0.01889890957521525, "duration": 84.34019756317139, "step": 72000}
{"episode_reward": 955.0, "episode": 289.0, "Q1 loss": 1.7916452128887177, "Q2 loss": 1.7665569322109222, "Mean Target Q": 375.23156909179687, "Mean Q1": 375.23146337890626, "Mean Q2": 375.23098010253904, "critic_loss": 3.558202153682709, "batch_reward": 3.314328125, "actor_loss": -375.62128247070314, "actor_target_entropy": -2.0, "actor_entropy": -0.22269677107036115, "alpha_loss": 0.0014625379100907593, "alpha_value": 0.018838885309640244, "duration": 85.59772610664368, "step": 72250}
{"episode_reward": 945.0, "episode": 290.0, "Q1 loss": 1.8258455543518066, "Q2 loss": 1.808541910648346, "Mean Target Q": 375.35768408203126, "Mean Q1": 375.3566994628906, "Mean Q2": 375.3572463378906, "critic_loss": 3.634387450695038, "batch_reward": 3.31003125, "actor_loss": -375.84162353515626, "actor_target_entropy": -2.0, "actor_entropy": -0.2663682204037905, "alpha_loss": 0.0012265023635700345, "alpha_value": 0.01873983991334302, "duration": 84.90565705299377, "step": 72500}
{"episode_reward": 958.0, "episode": 291.0, "Q1 loss": 1.6789374220371247, "Q2 loss": 1.663177688598633, "Mean Target Q": 375.5289688720703, "Mean Q1": 375.5287709960937, "Mean Q2": 375.5286253662109, "critic_loss": 3.3421151156425477, "batch_reward": 3.3140703125, "actor_loss": -375.93788916015626, "actor_target_entropy": -2.0, "actor_entropy": -0.280363907456398, "alpha_loss": 0.001697879353305325, "alpha_value": 0.018629894576614237, "duration": 86.64315342903137, "step": 72750}
{"episode_reward": 989.0, "episode": 292.0, "Q1 loss": 1.669223821401596, "Q2 loss": 1.6729947826862335, "Mean Target Q": 375.6861800537109, "Mean Q1": 375.68414636230466, "Mean Q2": 375.6838603515625, "critic_loss": 3.3422186074256897, "batch_reward": 3.317796875, "actor_loss": -376.1322297363281, "actor_target_entropy": -2.0, "actor_entropy": -0.14141857473552227, "alpha_loss": 0.002394138818141073, "alpha_value": 0.018485614230389507, "duration": 91.23342823982239, "step": 73000}
{"episode_reward": 911.0, "episode": 293.0, "Q1 loss": 1.713443293571472, "Q2 loss": 1.6881802401542663, "Mean Target Q": 375.90071838378907, "Mean Q1": 375.90085961914065, "Mean Q2": 375.90124011230466, "critic_loss": 3.4016235280036926, "batch_reward": 3.3231875, "actor_loss": -376.22821166992185, "actor_target_entropy": -2.0, "actor_entropy": -0.24150461887568236, "alpha_loss": 0.000269113639369607, "alpha_value": 0.01840531825692281, "duration": 85.11648368835449, "step": 73250}
{"episode_reward": 980.0, "episode": 294.0, "Q1 loss": 1.723244621038437, "Q2 loss": 1.7241833591461182, "Mean Target Q": 376.0589006347656, "Mean Q1": 376.0577021484375, "Mean Q2": 376.05711376953127, "critic_loss": 3.4474279828071595, "batch_reward": 3.323671875, "actor_loss": -376.53022705078126, "actor_target_entropy": -2.0, "actor_entropy": -0.22148094084113837, "alpha_loss": 0.0018022537264041603, "alpha_value": 0.01835066735919773, "duration": 86.41841077804565, "step": 73500}
{"episode_reward": 997.0, "episode": 295.0, "Q1 loss": 1.6273655529022217, "Q2 loss": 1.6295114872455596, "Mean Target Q": 376.18785583496094, "Mean Q1": 376.1877551269531, "Mean Q2": 376.18860998535155, "critic_loss": 3.2568770403862, "batch_reward": 3.3236015625, "actor_loss": -376.5696931152344, "actor_target_entropy": -2.0, "actor_entropy": -0.24332664553076028, "alpha_loss": 0.0011199547281721606, "alpha_value": 0.018238078152583836, "duration": 84.80708265304565, "step": 73750}
{"episode_reward": 959.0, "episode": 296.0, "Q1 loss": 1.735152449131012, "Q2 loss": 1.7322602422237396, "Mean Target Q": 376.36178637695315, "Mean Q1": 376.36131372070315, "Mean Q2": 376.3612332763672, "critic_loss": 3.467412693977356, "batch_reward": 3.3299140625, "actor_loss": -376.71650732421875, "actor_target_entropy": -2.0, "actor_entropy": -0.19943751517683267, "alpha_loss": 0.002243717267177999, "alpha_value": 0.018138188321881864, "duration": 85.53032517433167, "step": 74000}
{"episode_reward": 870.0, "episode": 297.0, "Q1 loss": 1.5748518958091735, "Q2 loss": 1.592016102552414, "Mean Target Q": 376.4854990234375, "Mean Q1": 376.4834256591797, "Mean Q2": 376.48361401367185, "critic_loss": 3.1668680095672608, "batch_reward": 3.3275234375, "actor_loss": -376.925681640625, "actor_target_entropy": -2.0, "actor_entropy": -0.18193390895426273, "alpha_loss": 0.0022070415501948444, "alpha_value": 0.017965851224302717, "duration": 86.98512291908264, "step": 74250}
{"episode_reward": 997.0, "episode": 298.0, "Q1 loss": 1.6858706982135774, "Q2 loss": 1.6479345207214355, "Mean Target Q": 376.5780285644531, "Mean Q1": 376.57843505859375, "Mean Q2": 376.57797436523435, "critic_loss": 3.333805221557617, "batch_reward": 3.3242265625, "actor_loss": -376.90680444335936, "actor_target_entropy": -2.0, "actor_entropy": -0.15662822365015744, "alpha_loss": 0.00321572821540758, "alpha_value": 0.017823080649941637, "duration": 86.49365067481995, "step": 74500}
{"episode_reward": 948.0, "episode": 299.0, "Q1 loss": 1.822363706111908, "Q2 loss": 1.7988634397983552, "Mean Target Q": 376.7151677246094, "Mean Q1": 376.71388525390626, "Mean Q2": 376.71406127929686, "critic_loss": 3.621227146148682, "batch_reward": 3.3265390625, "actor_loss": -377.162458984375, "actor_target_entropy": -2.0, "actor_entropy": -0.2221622955650091, "alpha_loss": 0.0028032850159797817, "alpha_value": 0.017642635103130317, "duration": 85.13712763786316, "step": 74750}
{"episode_reward": 947.0, "episode": 300.0, "Q1 loss": 1.8554877717494964, "Q2 loss": 1.8439489281177521, "Mean Target Q": 376.8912565917969, "Mean Q1": 376.8913250732422, "Mean Q2": 376.8914173583984, "critic_loss": 3.699436690330505, "batch_reward": 3.33221875, "actor_loss": -377.2961379394531, "actor_target_entropy": -2.0, "actor_entropy": -0.26867678470909595, "alpha_loss": 0.0023839202830567958, "alpha_value": 0.017501718174976567, "step": 75000}
{"duration": 110.29276371002197, "step": 75000}
{"episode_reward": 988.0, "episode": 301.0, "Q1 loss": 1.6548822519779205, "Q2 loss": 1.6343546895980834, "Mean Target Q": 377.00706896972656, "Mean Q1": 377.0062645263672, "Mean Q2": 377.00613256835936, "critic_loss": 3.2892369456291197, "batch_reward": 3.3310546875, "actor_loss": -377.46208837890623, "actor_target_entropy": -2.0, "actor_entropy": -0.27838887787610295, "alpha_loss": 0.0018409951406065375, "alpha_value": 0.017344367584646133, "duration": 85.41458034515381, "step": 75250}
{"episode_reward": 986.0, "episode": 302.0, "Q1 loss": 1.7619717421531678, "Q2 loss": 1.761625449180603, "Mean Target Q": 377.14935986328123, "Mean Q1": 377.1486492919922, "Mean Q2": 377.1487530517578, "critic_loss": 3.523597188949585, "batch_reward": 3.3274765625, "actor_loss": -377.55448828125, "actor_target_entropy": -2.0, "actor_entropy": -0.17243041295558215, "alpha_loss": 0.002172109174542129, "alpha_value": 0.017238033644814063, "duration": 86.00672483444214, "step": 75500}
{"episode_reward": 952.0, "episode": 303.0, "Q1 loss": 1.7784052183628083, "Q2 loss": 1.7115392506122589, "Mean Target Q": 377.36530700683596, "Mean Q1": 377.36372387695315, "Mean Q2": 377.3639647216797, "critic_loss": 3.4899444651603697, "batch_reward": 3.3389921875, "actor_loss": -377.7895078125, "actor_target_entropy": -2.0, "actor_entropy": -0.2626296146735549, "alpha_loss": 0.0016340185611043126, "alpha_value": 0.017154954148372804, "duration": 84.60251688957214, "step": 75750}
{"episode_reward": 989.0, "episode": 304.0, "Q1 loss": 1.7854209957122802, "Q2 loss": 1.7891851360797881, "Mean Target Q": 377.5706026611328, "Mean Q1": 377.57120581054687, "Mean Q2": 377.5709417724609, "critic_loss": 3.5746061253547667, "batch_reward": 3.340734375, "actor_loss": -377.9502014160156, "actor_target_entropy": -2.0, "actor_entropy": -0.17336017507314683, "alpha_loss": 0.0024409231033641846, "alpha_value": 0.01701875041507057, "duration": 84.83803820610046, "step": 76000}
{"episode_reward": 910.0, "episode": 305.0, "Q1 loss": 1.8092329063415526, "Q2 loss": 1.8060786986351014, "Mean Target Q": 377.5635231933594, "Mean Q1": 377.56142761230467, "Mean Q2": 377.56174267578126, "critic_loss": 3.61531161403656, "batch_reward": 3.331359375, "actor_loss": -377.9823586425781, "actor_target_entropy": -2.0, "actor_entropy": -0.24618458383530378, "alpha_loss": 0.0021511993138119577, "alpha_value": 0.016902100831156504, "duration": 83.45230054855347, "step": 76250}
{"episode_reward": 954.0, "episode": 306.0, "Q1 loss": 1.6863980715274811, "Q2 loss": 1.6841704139709472, "Mean Target Q": 377.7572423095703, "Mean Q1": 377.75639416503907, "Mean Q2": 377.75592138671874, "critic_loss": 3.3705684909820555, "batch_reward": 3.3390703125, "actor_loss": -378.2217177734375, "actor_target_entropy": -2.0, "actor_entropy": -0.1927132721915841, "alpha_loss": 0.002113100745715201, "alpha_value": 0.01677526777325094, "duration": 84.72216820716858, "step": 76500}
{"episode_reward": 990.0, "episode": 307.0, "Q1 loss": 1.8404618995189668, "Q2 loss": 1.8323129415512085, "Mean Target Q": 377.7746520996094, "Mean Q1": 377.77535339355467, "Mean Q2": 377.7752977294922, "critic_loss": 3.672774835586548, "batch_reward": 3.334125, "actor_loss": -378.07750048828126, "actor_target_entropy": -2.0, "actor_entropy": -0.27468631461262705, "alpha_loss": 0.0010653246899601071, "alpha_value": 0.01667799479603631, "duration": 84.35554504394531, "step": 76750}
{"episode_reward": 950.0, "episode": 308.0, "Q1 loss": 1.838185017824173, "Q2 loss": 1.8451382451057434, "Mean Target Q": 377.9976676025391, "Mean Q1": 377.9958077392578, "Mean Q2": 377.99632263183594, "critic_loss": 3.6833232731819154, "batch_reward": 3.3412890625, "actor_loss": -378.3987023925781, "actor_target_entropy": -2.0, "actor_entropy": -0.2927882933020592, "alpha_loss": 0.0011151466383598745, "alpha_value": 0.01660701996003345, "duration": 83.17833733558655, "step": 77000}
{"episode_reward": 987.0, "episode": 309.0, "Q1 loss": 1.6835516500473022, "Q2 loss": 1.642500741004944, "Mean Target Q": 378.1280908203125, "Mean Q1": 378.1273779296875, "Mean Q2": 378.12741674804687, "critic_loss": 3.3260523867607117, "batch_reward": 3.341078125, "actor_loss": -378.56043310546875, "actor_target_entropy": -2.0, "actor_entropy": -0.28822816345095637, "alpha_loss": 0.0011232935783918947, "alpha_value": 0.016549907076586785, "duration": 85.5224711894989, "step": 77250}
{"episode_reward": 957.0, "episode": 310.0, "Q1 loss": 1.6545283753871918, "Q2 loss": 1.652951943397522, "Mean Target Q": 378.34212255859376, "Mean Q1": 378.3419747314453, "Mean Q2": 378.34180517578125, "critic_loss": 3.3074803075790404, "batch_reward": 3.3479609375, "actor_loss": -378.83472509765625, "actor_target_entropy": -2.0, "actor_entropy": -0.3086846389770508, "alpha_loss": 0.001237660891842097, "alpha_value": 0.01646338904505141, "duration": 90.96638345718384, "step": 77500}
{"episode_reward": 956.0, "episode": 311.0, "Q1 loss": 1.7641996350288391, "Q2 loss": 1.7557034502029418, "Mean Target Q": 378.5363602294922, "Mean Q1": 378.5360168457031, "Mean Q2": 378.53656079101563, "critic_loss": 3.5199030876159667, "batch_reward": 3.3493203125, "actor_loss": -378.9562663574219, "actor_target_entropy": -2.0, "actor_entropy": -0.27137514398992063, "alpha_loss": 0.0007626219103112817, "alpha_value": 0.01642119898693861, "duration": 84.4158706665039, "step": 77750}
{"episode_reward": 953.0, "episode": 312.0, "Q1 loss": 1.6637460939884186, "Q2 loss": 1.6490870823860169, "Mean Target Q": 378.6660072021484, "Mean Q1": 378.66478942871095, "Mean Q2": 378.66447387695314, "critic_loss": 3.3128331770896913, "batch_reward": 3.351703125, "actor_loss": -379.083650390625, "actor_target_entropy": -2.0, "actor_entropy": -0.23684053718298673, "alpha_loss": -2.2404830204322936e-05, "alpha_value": 0.016393834614668822, "duration": 83.30610847473145, "step": 78000}
{"episode_reward": 957.0, "episode": 313.0, "Q1 loss": 1.7387822427749633, "Q2 loss": 1.7106598937511444, "Mean Target Q": 378.8367110595703, "Mean Q1": 378.8358607177734, "Mean Q2": 378.8354361572266, "critic_loss": 3.449442138195038, "batch_reward": 3.357421875, "actor_loss": -379.192080078125, "actor_target_entropy": -2.0, "actor_entropy": -0.27939870988577603, "alpha_loss": 0.0008705756340641528, "alpha_value": 0.016364044102286153, "duration": 83.24054479598999, "step": 78250}
{"episode_reward": 960.0, "episode": 314.0, "Q1 loss": 1.5738604488372803, "Q2 loss": 1.5854377579689025, "Mean Target Q": 378.8530369873047, "Mean Q1": 378.8522401123047, "Mean Q2": 378.8522503662109, "critic_loss": 3.159298204898834, "batch_reward": 3.3510703125, "actor_loss": -379.20996411132813, "actor_target_entropy": -2.0, "actor_entropy": -0.3211573100686073, "alpha_loss": 0.0008881369989831001, "alpha_value": 0.016324135403377554, "duration": 84.70820760726929, "step": 78500}
{"episode_reward": 959.0, "episode": 315.0, "Q1 loss": 1.6847612376213075, "Q2 loss": 1.637225960969925, "Mean Target Q": 379.0921221923828, "Mean Q1": 379.09146459960937, "Mean Q2": 379.0915676269531, "critic_loss": 3.3219872040748597, "batch_reward": 3.36246875, "actor_loss": -379.4903759765625, "actor_target_entropy": -2.0, "actor_entropy": -0.3584157830476761, "alpha_loss": 0.00019994701014366002, "alpha_value": 0.01627781713802787, "duration": 84.23786354064941, "step": 78750}
{"episode_reward": 958.0, "episode": 316.0, "Q1 loss": 1.630641107559204, "Q2 loss": 1.6214458582401277, "Mean Target Q": 379.0881900634766, "Mean Q1": 379.0890035400391, "Mean Q2": 379.08918591308594, "critic_loss": 3.2520869665145873, "batch_reward": 3.3594921875, "actor_loss": -379.48725537109374, "actor_target_entropy": -2.0, "actor_entropy": -0.2433533113077283, "alpha_loss": 0.00196204294427298, "alpha_value": 0.016208363516894444, "duration": 83.55842685699463, "step": 79000}
{"episode_reward": 953.0, "episode": 317.0, "Q1 loss": 1.6872455344200135, "Q2 loss": 1.6614108905792235, "Mean Target Q": 379.12053747558593, "Mean Q1": 379.11899279785155, "Mean Q2": 379.1189952392578, "critic_loss": 3.3486564245224, "batch_reward": 3.35553125, "actor_loss": -379.4076789550781, "actor_target_entropy": -2.0, "actor_entropy": -0.25789112331718206, "alpha_loss": 0.0014114051517099143, "alpha_value": 0.01609307621559475, "duration": 83.91313815116882, "step": 79250}
{"episode_reward": 947.0, "episode": 318.0, "Q1 loss": 1.6486505239009857, "Q2 loss": 1.6275478472709657, "Mean Target Q": 379.2682978515625, "Mean Q1": 379.2670739746094, "Mean Q2": 379.26714477539065, "critic_loss": 3.276198387145996, "batch_reward": 3.35803125, "actor_loss": -379.62956640625, "actor_target_entropy": -2.0, "actor_entropy": -0.26934274929761887, "alpha_loss": 0.0005127420034259558, "alpha_value": 0.016036680178415322, "duration": 84.44159150123596, "step": 79500}
{"episode_reward": 989.0, "episode": 319.0, "Q1 loss": 1.6650201003551484, "Q2 loss": 1.6135947110652924, "Mean Target Q": 379.40249169921873, "Mean Q1": 379.4018697509766, "Mean Q2": 379.4017548828125, "critic_loss": 3.278614814758301, "batch_reward": 3.35821875, "actor_loss": -379.78160815429686, "actor_target_entropy": -2.0, "actor_entropy": -0.2690848316252232, "alpha_loss": 0.0010524775166995824, "alpha_value": 0.015973662916747206, "duration": 83.36563396453857, "step": 79750}
{"episode_reward": 959.0, "episode": 320.0, "Q1 loss": 1.628762413263321, "Q2 loss": 1.6208221428394318, "Mean Target Q": 379.5791667480469, "Mean Q1": 379.5778681640625, "Mean Q2": 379.5783458251953, "critic_loss": 3.249584554672241, "batch_reward": 3.3600078125, "actor_loss": -380.030916015625, "actor_target_entropy": -2.0, "actor_entropy": -0.12069148011133075, "alpha_loss": 0.0019000948371831327, "alpha_value": 0.015890434775017708, "step": 80000}
{"duration": 108.16933584213257, "step": 80000}
{"episode_reward": 957.0, "episode": 321.0, "Q1 loss": 1.6388906719684602, "Q2 loss": 1.6422076885700225, "Mean Target Q": 379.6979837646484, "Mean Q1": 379.69803759765625, "Mean Q2": 379.69796252441404, "critic_loss": 3.281098365306854, "batch_reward": 3.36346875, "actor_loss": -380.0814677734375, "actor_target_entropy": -2.0, "actor_entropy": -0.18452766535431148, "alpha_loss": 0.0010340838083066047, "alpha_value": 0.015801839013610736, "duration": 83.77175855636597, "step": 80250}
{"episode_reward": 945.0, "episode": 322.0, "Q1 loss": 1.5624185581207275, "Q2 loss": 1.5393786160945893, "Mean Target Q": 379.8238809814453, "Mean Q1": 379.8240369873047, "Mean Q2": 379.82387829589845, "critic_loss": 3.1017971744537354, "batch_reward": 3.36259375, "actor_loss": -380.2166364746094, "actor_target_entropy": -2.0, "actor_entropy": -0.29430718064308165, "alpha_loss": 0.0006857574284076691, "alpha_value": 0.015720654290968014, "duration": 83.03159689903259, "step": 80500}
{"episode_reward": 989.0, "episode": 323.0, "Q1 loss": 1.6373367438316344, "Q2 loss": 1.6221756391525268, "Mean Target Q": 379.9680007324219, "Mean Q1": 379.9666519775391, "Mean Q2": 379.9667961425781, "critic_loss": 3.2595123782157898, "batch_reward": 3.36546875, "actor_loss": -380.2933671875, "actor_target_entropy": -2.0, "actor_entropy": -0.30103442434966565, "alpha_loss": 0.0003165193930035457, "alpha_value": 0.015715594943257492, "duration": 84.12797164916992, "step": 80750}
{"episode_reward": 997.0, "episode": 324.0, "Q1 loss": 1.6132603323459624, "Q2 loss": 1.599451489686966, "Mean Target Q": 380.07294775390625, "Mean Q1": 380.0731303710937, "Mean Q2": 380.0730310058594, "critic_loss": 3.2127118287086485, "batch_reward": 3.366421875, "actor_loss": -380.5340537109375, "actor_target_entropy": -2.0, "actor_entropy": -0.3065503186509013, "alpha_loss": 0.0013575619850307702, "alpha_value": 0.0156495821111514, "duration": 84.1689703464508, "step": 81000}
{"episode_reward": 993.0, "episode": 325.0, "Q1 loss": 1.4678819537162782, "Q2 loss": 1.4588979742527008, "Mean Target Q": 380.2301456298828, "Mean Q1": 380.2288785400391, "Mean Q2": 380.22870434570314, "critic_loss": 2.9267799253463744, "batch_reward": 3.36984375, "actor_loss": -380.55899389648437, "actor_target_entropy": -2.0, "actor_entropy": -0.3324597677588463, "alpha_loss": 0.0015576464645564556, "alpha_value": 0.015549723664267703, "duration": 83.81837868690491, "step": 81250}
{"episode_reward": 959.0, "episode": 326.0, "Q1 loss": 1.5069393434524536, "Q2 loss": 1.4958038623332977, "Mean Target Q": 380.28925866699217, "Mean Q1": 380.2888902587891, "Mean Q2": 380.2886666259766, "critic_loss": 3.0027432074546816, "batch_reward": 3.3698984375, "actor_loss": -380.65651123046877, "actor_target_entropy": -2.0, "actor_entropy": -0.18247461311519145, "alpha_loss": 0.0029209817305672916, "alpha_value": 0.015392775669693166, "duration": 84.5564935207367, "step": 81500}
{"episode_reward": 951.0, "episode": 327.0, "Q1 loss": 1.469120008945465, "Q2 loss": 1.4479432618618011, "Mean Target Q": 380.50724353027346, "Mean Q1": 380.50680993652344, "Mean Q2": 380.50804040527345, "critic_loss": 2.917063265323639, "batch_reward": 3.3778203125, "actor_loss": -380.891205078125, "actor_target_entropy": -2.0, "actor_entropy": -0.22114053967595101, "alpha_loss": 0.001933917767368257, "alpha_value": 0.01525764934536076, "duration": 84.97120308876038, "step": 81750}
{"episode_reward": 957.0, "episode": 328.0, "Q1 loss": 1.4395425004959106, "Q2 loss": 1.4137823615074159, "Mean Target Q": 380.5961018066406, "Mean Q1": 380.5960205078125, "Mean Q2": 380.5952424316406, "critic_loss": 2.8533248739242554, "batch_reward": 3.37503125, "actor_loss": -380.9126086425781, "actor_target_entropy": -2.0, "actor_entropy": -0.24460395477712155, "alpha_loss": 0.0006889759285841137, "alpha_value": 0.015166951878000504, "duration": 92.24874329566956, "step": 82000}
{"episode_reward": 903.0, "episode": 329.0, "Q1 loss": 1.4570181603431702, "Q2 loss": 1.4594340307712554, "Mean Target Q": 380.7046853027344, "Mean Q1": 380.7040637207031, "Mean Q2": 380.7040889892578, "critic_loss": 2.9164521961212158, "batch_reward": 3.377125, "actor_loss": -381.1202717285156, "actor_target_entropy": -2.0, "actor_entropy": -0.19896976517885923, "alpha_loss": 0.001444816351402551, "alpha_value": 0.01509718765852723, "duration": 85.83153486251831, "step": 82250}
{"episode_reward": 988.0, "episode": 330.0, "Q1 loss": 1.5064984023571015, "Q2 loss": 1.4864421315193177, "Mean Target Q": 380.7113835449219, "Mean Q1": 380.7090340576172, "Mean Q2": 380.70954467773436, "critic_loss": 2.9929405298233034, "batch_reward": 3.3741484375, "actor_loss": -381.1442648925781, "actor_target_entropy": -2.0, "actor_entropy": -0.4216228616833687, "alpha_loss": -0.00041213528800290077, "alpha_value": 0.015058313142569208, "duration": 83.42585015296936, "step": 82500}
{"episode_reward": 946.0, "episode": 331.0, "Q1 loss": 1.5255911242961884, "Q2 loss": 1.4872486510276794, "Mean Target Q": 380.8462030029297, "Mean Q1": 380.84634204101565, "Mean Q2": 380.8456821289063, "critic_loss": 3.0128397760391237, "batch_reward": 3.379046875, "actor_loss": -381.21655126953124, "actor_target_entropy": -2.0, "actor_entropy": -0.2709795646443963, "alpha_loss": 0.0021778894995804878, "alpha_value": 0.014994051694334978, "duration": 83.29282975196838, "step": 82750}
{"episode_reward": 961.0, "episode": 332.0, "Q1 loss": 1.5298188116550446, "Q2 loss": 1.5233390891551972, "Mean Target Q": 380.9389372558594, "Mean Q1": 380.9386984863281, "Mean Q2": 380.9392060546875, "critic_loss": 3.053157890319824, "batch_reward": 3.3797734375, "actor_loss": -381.22855004882814, "actor_target_entropy": -2.0, "actor_entropy": -0.27225784946233034, "alpha_loss": 0.0016369049132335931, "alpha_value": 0.014882106270842436, "duration": 82.89156246185303, "step": 83000}
{"episode_reward": 950.0, "episode": 333.0, "Q1 loss": 1.5047358603477479, "Q2 loss": 1.5076794557571411, "Mean Target Q": 380.9971923828125, "Mean Q1": 380.9970777587891, "Mean Q2": 380.9969554443359, "critic_loss": 3.012415312767029, "batch_reward": 3.3799140625, "actor_loss": -381.32879125976564, "actor_target_entropy": -2.0, "actor_entropy": -0.3901544248759747, "alpha_loss": -9.94574858341366e-05, "alpha_value": 0.01485024430327725, "duration": 82.07782483100891, "step": 83250}
{"episode_reward": 959.0, "episode": 334.0, "Q1 loss": 1.486614191532135, "Q2 loss": 1.4774168720245362, "Mean Target Q": 381.0391055908203, "Mean Q1": 381.0398416748047, "Mean Q2": 381.03962646484376, "critic_loss": 2.96403106546402, "batch_reward": 3.377921875, "actor_loss": -381.547712890625, "actor_target_entropy": -2.0, "actor_entropy": -0.39360063963383435, "alpha_loss": -0.0010013826508074998, "alpha_value": 0.014881167928686495, "duration": 83.25992012023926, "step": 83500}
{"episode_reward": 958.0, "episode": 335.0, "Q1 loss": 1.52668075299263, "Q2 loss": 1.506778983592987, "Mean Target Q": 381.2070496826172, "Mean Q1": 381.20497912597654, "Mean Q2": 381.2050573730469, "critic_loss": 3.0334597334861755, "batch_reward": 3.3852734375, "actor_loss": -381.6299755859375, "actor_target_entropy": -2.0, "actor_entropy": -0.2252685413993895, "alpha_loss": 0.0007190258807968349, "alpha_value": 0.01485776315294095, "duration": 83.30647706985474, "step": 83750}
{"episode_reward": 865.0, "episode": 336.0, "Q1 loss": 1.5308516638278962, "Q2 loss": 1.4891235258579254, "Mean Target Q": 381.3380048828125, "Mean Q1": 381.3366160888672, "Mean Q2": 381.3371978759766, "critic_loss": 3.019975176811218, "batch_reward": 3.386625, "actor_loss": -381.6807963867187, "actor_target_entropy": -2.0, "actor_entropy": -0.3436817613840103, "alpha_loss": -0.000943640292622149, "alpha_value": 0.014877907609393051, "duration": 84.40215611457825, "step": 84000}
{"episode_reward": 994.0, "episode": 337.0, "Q1 loss": 1.4367657163143157, "Q2 loss": 1.420389594078064, "Mean Target Q": 381.4020310058594, "Mean Q1": 381.4024705810547, "Mean Q2": 381.4021906738281, "critic_loss": 2.857155316352844, "batch_reward": 3.3831640625, "actor_loss": -381.82567651367185, "actor_target_entropy": -2.0, "actor_entropy": -0.35858122200518844, "alpha_loss": -0.0007747218227013946, "alpha_value": 0.014944256703622373, "duration": 83.67029690742493, "step": 84250}
{"episode_reward": 989.0, "episode": 338.0, "Q1 loss": 1.4501766171455384, "Q2 loss": 1.4219036543369292, "Mean Target Q": 381.4581844482422, "Mean Q1": 381.45818786621095, "Mean Q2": 381.4585925292969, "critic_loss": 2.872080265522003, "batch_reward": 3.3836484375, "actor_loss": -381.7783991699219, "actor_target_entropy": -2.0, "actor_entropy": -0.4045918901860714, "alpha_loss": -0.00012955139321275055, "alpha_value": 0.01498270882936558, "duration": 83.91019535064697, "step": 84500}
{"episode_reward": 951.0, "episode": 339.0, "Q1 loss": 1.4565522046089172, "Q2 loss": 1.4627227501869202, "Mean Target Q": 381.6098828125, "Mean Q1": 381.60765673828126, "Mean Q2": 381.60747546386716, "critic_loss": 2.91927494430542, "batch_reward": 3.3914375, "actor_loss": -381.97267431640626, "actor_target_entropy": -2.0, "actor_entropy": -0.34402459222078324, "alpha_loss": 0.001935714177438058, "alpha_value": 0.014905586258011706, "duration": 83.94316673278809, "step": 84750}
{"episode_reward": 986.0, "episode": 340.0, "Q1 loss": 1.5349129750728607, "Q2 loss": 1.5535862486362457, "Mean Target Q": 381.6323004150391, "Mean Q1": 381.632740234375, "Mean Q2": 381.6321612548828, "critic_loss": 3.0884992122650146, "batch_reward": 3.38725, "actor_loss": -382.0523254394531, "actor_target_entropy": -2.0, "actor_entropy": -0.292421478047967, "alpha_loss": 0.0018280496790539474, "alpha_value": 0.014781473656062133, "step": 85000}
{"duration": 108.96219420433044, "step": 85000}
{"episode_reward": 958.0, "episode": 341.0, "Q1 loss": 1.666919734477997, "Q2 loss": 1.6575642087459563, "Mean Target Q": 381.73602624511716, "Mean Q1": 381.7343931884766, "Mean Q2": 381.7346149902344, "critic_loss": 3.324483938694, "batch_reward": 3.39253125, "actor_loss": -382.06111157226565, "actor_target_entropy": -2.0, "actor_entropy": -0.2968527015224099, "alpha_loss": 0.002078148869331926, "alpha_value": 0.01468139082675308, "duration": 84.32830572128296, "step": 85250}
{"episode_reward": 955.0, "episode": 342.0, "Q1 loss": 1.4968735809326172, "Q2 loss": 1.5067874507904053, "Mean Target Q": 381.9036774902344, "Mean Q1": 381.9021904296875, "Mean Q2": 381.9023572998047, "critic_loss": 3.00366103219986, "batch_reward": 3.3988203125, "actor_loss": -382.32907299804685, "actor_target_entropy": -2.0, "actor_entropy": -0.3523061525225639, "alpha_loss": 0.0017722325536888093, "alpha_value": 0.0145617235444239, "duration": 83.323157787323, "step": 85500}
{"episode_reward": 957.0, "episode": 343.0, "Q1 loss": 1.5198966643810272, "Q2 loss": 1.5237848372459413, "Mean Target Q": 381.9244533691406, "Mean Q1": 381.92466845703126, "Mean Q2": 381.925029296875, "critic_loss": 3.0436815061569216, "batch_reward": 3.39796875, "actor_loss": -382.19858959960936, "actor_target_entropy": -2.0, "actor_entropy": -0.4181251629889011, "alpha_loss": 0.001491333983023651, "alpha_value": 0.014467816863790318, "duration": 84.60400199890137, "step": 85750}
{"episode_reward": 987.0, "episode": 344.0, "Q1 loss": 1.5379726023674012, "Q2 loss": 1.5151827340126038, "Mean Target Q": 382.08848803710936, "Mean Q1": 382.08960485839845, "Mean Q2": 382.08954748535155, "critic_loss": 3.0531553339958193, "batch_reward": 3.4048125, "actor_loss": -382.48196484375, "actor_target_entropy": -2.0, "actor_entropy": -0.22889383149147033, "alpha_loss": 0.002510058535495773, "alpha_value": 0.014338694699592787, "duration": 82.56087970733643, "step": 86000}
{"episode_reward": 901.0, "episode": 345.0, "Q1 loss": 1.5187370405197143, "Q2 loss": 1.5332845783233642, "Mean Target Q": 382.17545361328126, "Mean Q1": 382.17327270507815, "Mean Q2": 382.1728162841797, "critic_loss": 3.052021619796753, "batch_reward": 3.4055703125, "actor_loss": -382.5483447265625, "actor_target_entropy": -2.0, "actor_entropy": -0.37880738645792006, "alpha_loss": 0.0001326131564565003, "alpha_value": 0.014251540955968763, "duration": 82.77628326416016, "step": 86250}
{"episode_reward": 1000.0, "episode": 346.0, "Q1 loss": 1.518458412885666, "Q2 loss": 1.5228281898498535, "Mean Target Q": 382.2078837890625, "Mean Q1": 382.2081744384766, "Mean Q2": 382.2083913574219, "critic_loss": 3.0412866139411925, "batch_reward": 3.398203125, "actor_loss": -382.5081806640625, "actor_target_entropy": -2.0, "actor_entropy": -0.388578326523304, "alpha_loss": -0.0002913627990055829, "alpha_value": 0.014253792553888107, "duration": 91.4485559463501, "step": 86500}
{"episode_reward": 982.0, "episode": 347.0, "Q1 loss": 1.5363877584934236, "Q2 loss": 1.4952613747119903, "Mean Target Q": 382.2405168457031, "Mean Q1": 382.2386806640625, "Mean Q2": 382.23931970214846, "critic_loss": 3.0316491322517396, "batch_reward": 3.39603125, "actor_loss": -382.69177563476563, "actor_target_entropy": -2.0, "actor_entropy": -0.31564758011326194, "alpha_loss": 0.00058344292268157, "alpha_value": 0.014255698889679917, "duration": 86.2843873500824, "step": 86750}
{"episode_reward": 952.0, "episode": 348.0, "Q1 loss": 1.6574053304195404, "Q2 loss": 1.6440102112293244, "Mean Target Q": 382.3952294921875, "Mean Q1": 382.39439367675783, "Mean Q2": 382.39371960449216, "critic_loss": 3.301415544509888, "batch_reward": 3.402, "actor_loss": -382.7544262695312, "actor_target_entropy": -2.0, "actor_entropy": -0.3670484897196293, "alpha_loss": -2.1705176448449493e-05, "alpha_value": 0.014230222848768495, "duration": 85.97860097885132, "step": 87000}
{"episode_reward": 987.0, "episode": 349.0, "Q1 loss": 1.5463751680850983, "Q2 loss": 1.519693418264389, "Mean Target Q": 382.5187586669922, "Mean Q1": 382.5190421142578, "Mean Q2": 382.5190329589844, "critic_loss": 3.0660685997009276, "batch_reward": 3.4096171875, "actor_loss": -382.92796411132815, "actor_target_entropy": -2.0, "actor_entropy": -0.39421586620807647, "alpha_loss": 0.0008515726135810837, "alpha_value": 0.014214967738059256, "duration": 86.08106088638306, "step": 87250}
{"episode_reward": 986.0, "episode": 350.0, "Q1 loss": 1.4404337549209594, "Q2 loss": 1.454709315776825, "Mean Target Q": 382.5970635986328, "Mean Q1": 382.5977166748047, "Mean Q2": 382.5978845214844, "critic_loss": 2.89514306306839, "batch_reward": 3.4106171875, "actor_loss": -382.95843383789065, "actor_target_entropy": -2.0, "actor_entropy": -0.4652062290906906, "alpha_loss": -0.0010173900795634836, "alpha_value": 0.014204307427888365, "duration": 85.37985897064209, "step": 87500}
{"episode_reward": 954.0, "episode": 351.0, "Q1 loss": 1.3901854774951934, "Q2 loss": 1.3540724561214448, "Mean Target Q": 382.6956492919922, "Mean Q1": 382.69498193359374, "Mean Q2": 382.6947474365234, "critic_loss": 2.744257932662964, "batch_reward": 3.408765625, "actor_loss": -383.06806420898437, "actor_target_entropy": -2.0, "actor_entropy": -0.43034403359889983, "alpha_loss": -0.0005943770649610087, "alpha_value": 0.014276008840713904, "duration": 85.80592942237854, "step": 87750}
{"episode_reward": 997.0, "episode": 352.0, "Q1 loss": 1.6704916865825654, "Q2 loss": 1.6241740925312043, "Mean Target Q": 382.78577026367185, "Mean Q1": 382.7848325195313, "Mean Q2": 382.7852510986328, "critic_loss": 3.2946657743453978, "batch_reward": 3.4068828125, "actor_loss": -383.14646020507814, "actor_target_entropy": -2.0, "actor_entropy": -0.3515003827214241, "alpha_loss": 0.000413972323294729, "alpha_value": 0.014268665117463417, "duration": 86.35613751411438, "step": 88000}
{"episode_reward": 995.0, "episode": 353.0, "Q1 loss": 1.5422278797626496, "Q2 loss": 1.5187226538658143, "Mean Target Q": 382.81535681152343, "Mean Q1": 382.8143903808594, "Mean Q2": 382.8141121826172, "critic_loss": 3.0609505252838134, "batch_reward": 3.4051015625, "actor_loss": -383.21734497070315, "actor_target_entropy": -2.0, "actor_entropy": -0.2150961677953601, "alpha_loss": 0.001545525565627031, "alpha_value": 0.014217356987442817, "duration": 85.99887657165527, "step": 88250}
{"episode_reward": 955.0, "episode": 354.0, "Q1 loss": 1.4633265891075133, "Q2 loss": 1.4663739652633667, "Mean Target Q": 382.8740419921875, "Mean Q1": 382.8734908447266, "Mean Q2": 382.87363891601564, "critic_loss": 2.929700556755066, "batch_reward": 3.4053828125, "actor_loss": -383.2587060546875, "actor_target_entropy": -2.0, "actor_entropy": -0.26521068421006205, "alpha_loss": 0.0016572380044963212, "alpha_value": 0.014101580092683558, "duration": 86.0754759311676, "step": 88500}
{"episode_reward": 952.0, "episode": 355.0, "Q1 loss": 1.4556249051094055, "Q2 loss": 1.4242191097736359, "Mean Target Q": 383.11017517089846, "Mean Q1": 383.10964453125, "Mean Q2": 383.10914306640626, "critic_loss": 2.8798440165519716, "batch_reward": 3.415765625, "actor_loss": -383.43731567382815, "actor_target_entropy": -2.0, "actor_entropy": -0.3720091598033905, "alpha_loss": 0.0015488462060457096, "alpha_value": 0.01401079745993221, "duration": 85.10536885261536, "step": 88750}
{"episode_reward": 1000.0, "episode": 356.0, "Q1 loss": 1.4937726879119873, "Q2 loss": 1.4761435914039611, "Mean Target Q": 383.2091904296875, "Mean Q1": 383.2087904052734, "Mean Q2": 383.20937353515626, "critic_loss": 2.9699162821769716, "batch_reward": 3.4144921875, "actor_loss": -383.5733662109375, "actor_target_entropy": -2.0, "actor_entropy": -0.3503821130692959, "alpha_loss": 0.0013834543166449294, "alpha_value": 0.013916704182142196, "duration": 85.85491871833801, "step": 89000}
{"episode_reward": 989.0, "episode": 357.0, "Q1 loss": 1.4102755391597748, "Q2 loss": 1.423763839483261, "Mean Target Q": 383.1892760009766, "Mean Q1": 383.1886838378906, "Mean Q2": 383.1883956298828, "critic_loss": 2.8340393795967103, "batch_reward": 3.4112734375, "actor_loss": -383.6523134765625, "actor_target_entropy": -2.0, "actor_entropy": -0.3257869240045547, "alpha_loss": 0.0004689296481665224, "alpha_value": 0.013853290715423842, "duration": 84.30455279350281, "step": 89250}
{"episode_reward": 956.0, "episode": 358.0, "Q1 loss": 1.4748079187870025, "Q2 loss": 1.4536870589256288, "Mean Target Q": 383.36651818847656, "Mean Q1": 383.364900390625, "Mean Q2": 383.36541076660154, "critic_loss": 2.9284949865341185, "batch_reward": 3.4163671875, "actor_loss": -383.6861625976563, "actor_target_entropy": -2.0, "actor_entropy": -0.34592664408683776, "alpha_loss": 0.0008070236025378108, "alpha_value": 0.013825592656437657, "duration": 85.15211582183838, "step": 89500}
{"episode_reward": 952.0, "episode": 359.0, "Q1 loss": 1.5002840616703033, "Q2 loss": 1.4630230143070222, "Mean Target Q": 383.4642124023438, "Mean Q1": 383.4650908203125, "Mean Q2": 383.46478369140624, "critic_loss": 2.9633070673942568, "batch_reward": 3.418921875, "actor_loss": -383.831978515625, "actor_target_entropy": -2.0, "actor_entropy": -0.34509328949451445, "alpha_loss": 0.0016544674029573799, "alpha_value": 0.013739625813947632, "duration": 84.85361385345459, "step": 89750}
{"episode_reward": 950.0, "episode": 360.0, "Q1 loss": 1.4666337597370147, "Q2 loss": 1.4531243937015534, "Mean Target Q": 383.48978515625, "Mean Q1": 383.48881726074217, "Mean Q2": 383.48861743164065, "critic_loss": 2.919758161067963, "batch_reward": 3.417265625, "actor_loss": -383.7942277832031, "actor_target_entropy": -2.0, "actor_entropy": -0.3640520539879799, "alpha_loss": 0.000590675672981888, "alpha_value": 0.01366832595775849, "step": 90000}
{"duration": 109.64923191070557, "step": 90000}
{"episode_reward": 988.0, "episode": 361.0, "Q1 loss": 1.385917507648468, "Q2 loss": 1.3700798892974853, "Mean Target Q": 383.4493546142578, "Mean Q1": 383.4485518798828, "Mean Q2": 383.44822644042966, "critic_loss": 2.7559973950386047, "batch_reward": 3.407828125, "actor_loss": -383.7966462402344, "actor_target_entropy": -2.0, "actor_entropy": -0.3816442135423422, "alpha_loss": -0.00021652649913448841, "alpha_value": 0.013666192689246806, "duration": 84.89664459228516, "step": 90250}
{"episode_reward": 947.0, "episode": 362.0, "Q1 loss": 1.4155120935440064, "Q2 loss": 1.4100966818332672, "Mean Target Q": 383.7208486328125, "Mean Q1": 383.7200440673828, "Mean Q2": 383.7208034667969, "critic_loss": 2.8256087641716006, "batch_reward": 3.4238671875, "actor_loss": -384.09686254882814, "actor_target_entropy": -2.0, "actor_entropy": -0.2762660697922111, "alpha_loss": 0.0017088419410865754, "alpha_value": 0.013600147368848455, "duration": 86.37980365753174, "step": 90500}
{"episode_reward": 912.0, "episode": 363.0, "Q1 loss": 1.2847058987617492, "Q2 loss": 1.28273282289505, "Mean Target Q": 383.7811259765625, "Mean Q1": 383.7814228515625, "Mean Q2": 383.78142431640623, "critic_loss": 2.567438722610474, "batch_reward": 3.4209921875, "actor_loss": -384.1393718261719, "actor_target_entropy": -2.0, "actor_entropy": -0.38995142102241515, "alpha_loss": 0.00043094159220345317, "alpha_value": 0.013534061601837642, "duration": 93.36438965797424, "step": 90750}
{"episode_reward": 959.0, "episode": 364.0, "Q1 loss": 1.3954830440282822, "Q2 loss": 1.3865357027053833, "Mean Target Q": 383.77509008789065, "Mean Q1": 383.7747741699219, "Mean Q2": 383.7744421386719, "critic_loss": 2.782018742084503, "batch_reward": 3.417265625, "actor_loss": -384.0972902832031, "actor_target_entropy": -2.0, "actor_entropy": -0.32218182301521303, "alpha_loss": 0.0008591584193054587, "alpha_value": 0.01348554390171207, "duration": 84.18158149719238, "step": 91000}
{"episode_reward": 993.0, "episode": 365.0, "Q1 loss": 1.4162682495117187, "Q2 loss": 1.397300192594528, "Mean Target Q": 383.9944783935547, "Mean Q1": 383.993642578125, "Mean Q2": 383.99366748046873, "critic_loss": 2.8135684356689454, "batch_reward": 3.4251796875, "actor_loss": -384.3599763183594, "actor_target_entropy": -2.0, "actor_entropy": -0.2692049485370517, "alpha_loss": 0.001740421039517969, "alpha_value": 0.013403707822731066, "duration": 84.87329149246216, "step": 91250}
{"episode_reward": 944.0, "episode": 366.0, "Q1 loss": 1.4106762471199035, "Q2 loss": 1.38742889046669, "Mean Target Q": 384.14946813964843, "Mean Q1": 384.14801953125, "Mean Q2": 384.1482141113281, "critic_loss": 2.7981051325798036, "batch_reward": 3.433125, "actor_loss": -384.4156845703125, "actor_target_entropy": -2.0, "actor_entropy": -0.3095550117641687, "alpha_loss": 0.0007150399065576494, "alpha_value": 0.013338809369903594, "duration": 85.0752477645874, "step": 91500}
{"episode_reward": 998.0, "episode": 367.0, "Q1 loss": 1.2175931248664855, "Q2 loss": 1.2126439092159271, "Mean Target Q": 384.1675643310547, "Mean Q1": 384.16704162597654, "Mean Q2": 384.16751049804685, "critic_loss": 2.4302370371818545, "batch_reward": 3.43221875, "actor_loss": -384.4737548828125, "actor_target_entropy": -2.0, "actor_entropy": -0.4828519805073738, "alpha_loss": 0.0009819326640572398, "alpha_value": 0.013258468298599923, "duration": 85.2953884601593, "step": 91750}
{"episode_reward": 945.0, "episode": 368.0, "Q1 loss": 1.2796090717315673, "Q2 loss": 1.2696119401454926, "Mean Target Q": 384.22636303710937, "Mean Q1": 384.22666271972656, "Mean Q2": 384.22610070800783, "critic_loss": 2.5492210187911986, "batch_reward": 3.432140625, "actor_loss": -384.6087641601562, "actor_target_entropy": -2.0, "actor_entropy": -0.3396634834632277, "alpha_loss": 0.0008508736804360524, "alpha_value": 0.013222135343109545, "duration": 84.34658122062683, "step": 92000}
{"episode_reward": 957.0, "episode": 369.0, "Q1 loss": 1.3578411209583283, "Q2 loss": 1.3468410985469819, "Mean Target Q": 384.20595703125, "Mean Q1": 384.2051116943359, "Mean Q2": 384.2054616699219, "critic_loss": 2.704682218551636, "batch_reward": 3.4227734375, "actor_loss": -384.45798486328124, "actor_target_entropy": -2.0, "actor_entropy": -0.4198063069283962, "alpha_loss": -0.0007436152559239417, "alpha_value": 0.013210549718657965, "duration": 85.66325736045837, "step": 92250}
{"episode_reward": 977.0, "episode": 370.0, "Q1 loss": 1.2913538146018981, "Q2 loss": 1.2884066092967987, "Mean Target Q": 384.40124475097656, "Mean Q1": 384.4006103515625, "Mean Q2": 384.40094201660156, "critic_loss": 2.579760422706604, "batch_reward": 3.434265625, "actor_loss": -384.7696955566406, "actor_target_entropy": -2.0, "actor_entropy": -0.2650129819884896, "alpha_loss": 0.0008550314719323068, "alpha_value": 0.013211312326320265, "duration": 84.11353778839111, "step": 92500}
{"episode_reward": 960.0, "episode": 371.0, "Q1 loss": 1.2801965837478637, "Q2 loss": 1.2704066652059556, "Mean Target Q": 384.3547618408203, "Mean Q1": 384.3543273925781, "Mean Q2": 384.3538251953125, "critic_loss": 2.550603248119354, "batch_reward": 3.4285390625, "actor_loss": -384.6506237792969, "actor_target_entropy": -2.0, "actor_entropy": -0.3193044071793556, "alpha_loss": 0.0008430913873016835, "alpha_value": 0.013135716003274806, "duration": 85.37675070762634, "step": 92750}
{"episode_reward": 955.0, "episode": 372.0, "Q1 loss": 1.287346568107605, "Q2 loss": 1.2860643496513366, "Mean Target Q": 384.5740667724609, "Mean Q1": 384.57419384765626, "Mean Q2": 384.57424523925783, "critic_loss": 2.573410922527313, "batch_reward": 3.4372890625, "actor_loss": -384.9685852050781, "actor_target_entropy": -2.0, "actor_entropy": -0.37698425018787385, "alpha_loss": 0.00036088836775161325, "alpha_value": 0.013128681398924428, "duration": 85.00287747383118, "step": 93000}
{"episode_reward": 960.0, "episode": 373.0, "Q1 loss": 1.3121780784130097, "Q2 loss": 1.2859422225952148, "Mean Target Q": 384.63658813476565, "Mean Q1": 384.6353040771484, "Mean Q2": 384.6357708740234, "critic_loss": 2.598120304107666, "batch_reward": 3.4364921875, "actor_loss": -384.89738452148435, "actor_target_entropy": -2.0, "actor_entropy": -0.31777202686667444, "alpha_loss": 0.0003478916464373469, "alpha_value": 0.013091994243689717, "duration": 85.41629910469055, "step": 93250}
{"episode_reward": 958.0, "episode": 374.0, "Q1 loss": 1.2540250408649445, "Q2 loss": 1.2528583111763, "Mean Target Q": 384.6806866455078, "Mean Q1": 384.67945983886716, "Mean Q2": 384.67964208984375, "critic_loss": 2.506883354187012, "batch_reward": 3.437265625, "actor_loss": -385.00304541015623, "actor_target_entropy": -2.0, "actor_entropy": -0.38611081639677286, "alpha_loss": -0.0011947757260641083, "alpha_value": 0.013093534889315474, "duration": 85.29634380340576, "step": 93500}
{"episode_reward": 987.0, "episode": 375.0, "Q1 loss": 1.3345976643562316, "Q2 loss": 1.3351498243808746, "Mean Target Q": 384.6772358398438, "Mean Q1": 384.6766403808594, "Mean Q2": 384.67619104003904, "critic_loss": 2.669747489452362, "batch_reward": 3.4356484375, "actor_loss": -385.03241479492186, "actor_target_entropy": -2.0, "actor_entropy": -0.4186400297880173, "alpha_loss": 1.6775560448877514e-05, "alpha_value": 0.013168430740900176, "duration": 84.62039732933044, "step": 93750}
{"episode_reward": 985.0, "episode": 376.0, "Q1 loss": 1.4371744017601014, "Q2 loss": 1.4083914380073548, "Mean Target Q": 384.7648377685547, "Mean Q1": 384.7650223388672, "Mean Q2": 384.76502478027345, "critic_loss": 2.845565835237503, "batch_reward": 3.44125, "actor_loss": -385.01821264648436, "actor_target_entropy": -2.0, "actor_entropy": -0.30846481171250345, "alpha_loss": 0.0012174124133307487, "alpha_value": 0.013100772979422684, "duration": 84.86394381523132, "step": 94000}
{"episode_reward": 988.0, "episode": 377.0, "Q1 loss": 1.3440345680713655, "Q2 loss": 1.3425392570495605, "Mean Target Q": 384.7815368652344, "Mean Q1": 384.78084497070313, "Mean Q2": 384.78108276367186, "critic_loss": 2.686573823928833, "batch_reward": 3.437125, "actor_loss": -385.1284245605469, "actor_target_entropy": -2.0, "actor_entropy": -0.3747776610851288, "alpha_loss": 0.001005339649738744, "alpha_value": 0.013053540220203174, "duration": 84.96506786346436, "step": 94250}
{"episode_reward": 958.0, "episode": 378.0, "Q1 loss": 1.3027086344957353, "Q2 loss": 1.3278898406028747, "Mean Target Q": 384.7993878173828, "Mean Q1": 384.79885888671873, "Mean Q2": 384.79849450683594, "critic_loss": 2.6305984816551207, "batch_reward": 3.4328203125, "actor_loss": -385.12419604492186, "actor_target_entropy": -2.0, "actor_entropy": -0.30038652750849726, "alpha_loss": 0.000545944579062052, "alpha_value": 0.012984571248673565, "duration": 85.98410391807556, "step": 94500}
{"episode_reward": 1000.0, "episode": 379.0, "Q1 loss": 1.2992127375602722, "Q2 loss": 1.2926507829427718, "Mean Target Q": 384.95021875, "Mean Q1": 384.9497509765625, "Mean Q2": 384.9494326171875, "critic_loss": 2.591863523006439, "batch_reward": 3.443125, "actor_loss": -385.33789892578125, "actor_target_entropy": -2.0, "actor_entropy": -0.45588640666007996, "alpha_loss": 0.00016809340519830584, "alpha_value": 0.012934449713707039, "duration": 84.3296070098877, "step": 94750}
{"episode_reward": 948.0, "episode": 380.0, "Q1 loss": 1.3262894849777223, "Q2 loss": 1.303570931315422, "Mean Target Q": 385.08356713867187, "Mean Q1": 385.08286096191404, "Mean Q2": 385.08308471679686, "critic_loss": 2.6298604125976563, "batch_reward": 3.4450703125, "actor_loss": -385.36586889648436, "actor_target_entropy": -2.0, "actor_entropy": -0.5243643922805786, "alpha_loss": 2.0991066005080937e-05, "alpha_value": 0.012957678125718249, "step": 95000}
{"duration": 111.23654055595398, "step": 95000}
{"episode_reward": 943.0, "episode": 381.0, "Q1 loss": 1.3070308723449706, "Q2 loss": 1.2878657228946686, "Mean Target Q": 385.06688220214846, "Mean Q1": 385.0669395751953, "Mean Q2": 385.06727209472655, "critic_loss": 2.594896587371826, "batch_reward": 3.4448828125, "actor_loss": -385.41148022460936, "actor_target_entropy": -2.0, "actor_entropy": -0.3880870290994644, "alpha_loss": 0.0010421169840265066, "alpha_value": 0.012912429611545175, "duration": 90.37576413154602, "step": 95250}
{"episode_reward": 960.0, "episode": 382.0, "Q1 loss": 1.3542254333496093, "Q2 loss": 1.349010807991028, "Mean Target Q": 385.0663781738281, "Mean Q1": 385.06583459472654, "Mean Q2": 385.06555029296874, "critic_loss": 2.7032362432479857, "batch_reward": 3.4438203125, "actor_loss": -385.42814697265624, "actor_target_entropy": -2.0, "actor_entropy": -0.5616597740650177, "alpha_loss": -0.00011379822622984647, "alpha_value": 0.012883003534832233, "duration": 83.54146552085876, "step": 95500}
{"episode_reward": 960.0, "episode": 383.0, "Q1 loss": 1.3009966881275177, "Q2 loss": 1.2898122853040694, "Mean Target Q": 385.1888616943359, "Mean Q1": 385.18785278320314, "Mean Q2": 385.18857165527345, "critic_loss": 2.590808964252472, "batch_reward": 3.4438828125, "actor_loss": -385.488765625, "actor_target_entropy": -2.0, "actor_entropy": -0.3954973370283842, "alpha_loss": 0.000805980583652854, "alpha_value": 0.012870103401472004, "duration": 85.00679135322571, "step": 95750}
{"episode_reward": 987.0, "episode": 384.0, "Q1 loss": 1.2662883229255677, "Q2 loss": 1.2304973640441894, "Mean Target Q": 385.16654528808596, "Mean Q1": 385.1666420898438, "Mean Q2": 385.1660230712891, "critic_loss": 2.4967856907844546, "batch_reward": 3.4409609375, "actor_loss": -385.5664060058594, "actor_target_entropy": -2.0, "actor_entropy": -0.2783136258274317, "alpha_loss": 0.0016354852612130344, "alpha_value": 0.012778815054098181, "duration": 81.18762493133545, "step": 96000}
{"episode_reward": 896.0, "episode": 385.0, "Q1 loss": 1.4064224548339843, "Q2 loss": 1.4100959655046463, "Mean Target Q": 385.2827437744141, "Mean Q1": 385.2821240234375, "Mean Q2": 385.2824816894531, "critic_loss": 2.8165184235572815, "batch_reward": 3.44753125, "actor_loss": -385.5511083984375, "actor_target_entropy": -2.0, "actor_entropy": -0.44941193333640694, "alpha_loss": 0.00035109011142048983, "alpha_value": 0.012684511109081031, "duration": 81.56373977661133, "step": 96250}
{"episode_reward": 959.0, "episode": 386.0, "Q1 loss": 1.3287701555490494, "Q2 loss": 1.3243064730167389, "Mean Target Q": 385.36921875, "Mean Q1": 385.36855090332034, "Mean Q2": 385.3682945556641, "critic_loss": 2.653076626777649, "batch_reward": 3.450140625, "actor_loss": -385.740333984375, "actor_target_entropy": -2.0, "actor_entropy": -0.4382612787634134, "alpha_loss": 0.0004931780290789902, "alpha_value": 0.012708079330255612, "duration": 81.79543805122375, "step": 96500}
{"episode_reward": 958.0, "episode": 387.0, "Q1 loss": 1.273347055912018, "Q2 loss": 1.2410936758518218, "Mean Target Q": 385.40008776855467, "Mean Q1": 385.3997357177734, "Mean Q2": 385.40007727050784, "critic_loss": 2.5144407362937926, "batch_reward": 3.45075, "actor_loss": -385.6954191894531, "actor_target_entropy": -2.0, "actor_entropy": -0.3832042134404182, "alpha_loss": 0.000962680303840898, "alpha_value": 0.012638316852263676, "duration": 82.44696283340454, "step": 96750}
{"episode_reward": 992.0, "episode": 388.0, "Q1 loss": 1.3585645940303803, "Q2 loss": 1.3334603717327118, "Mean Target Q": 385.40536950683594, "Mean Q1": 385.4050830078125, "Mean Q2": 385.40505078125, "critic_loss": 2.692024974346161, "batch_reward": 3.44925, "actor_loss": -385.6846064453125, "actor_target_entropy": -2.0, "actor_entropy": -0.28926885733008384, "alpha_loss": 0.00023194705485366285, "alpha_value": 0.012601816645735155, "duration": 81.00623774528503, "step": 97000}
{"episode_reward": 907.0, "episode": 389.0, "Q1 loss": 1.3523298792839051, "Q2 loss": 1.355191928267479, "Mean Target Q": 385.35366259765624, "Mean Q1": 385.35262646484375, "Mean Q2": 385.35290966796873, "critic_loss": 2.707521800994873, "batch_reward": 3.44075, "actor_loss": -385.6617536621094, "actor_target_entropy": -2.0, "actor_entropy": -0.3296466230303049, "alpha_loss": -0.0005333530533825979, "alpha_value": 0.012613961822648226, "duration": 81.43074893951416, "step": 97250}
{"episode_reward": 950.0, "episode": 390.0, "Q1 loss": 1.3004483571052552, "Q2 loss": 1.2840372133255005, "Mean Target Q": 385.5047908935547, "Mean Q1": 385.5049460449219, "Mean Q2": 385.5049881591797, "critic_loss": 2.58448557639122, "batch_reward": 3.4480234375, "actor_loss": -385.8072458496094, "actor_target_entropy": -2.0, "actor_entropy": -0.3275924117863178, "alpha_loss": 0.0012182503645308317, "alpha_value": 0.012590366648292663, "duration": 81.08315515518188, "step": 97500}
{"episode_reward": 978.0, "episode": 391.0, "Q1 loss": 1.208627717614174, "Q2 loss": 1.231166642665863, "Mean Target Q": 385.5273483886719, "Mean Q1": 385.5268406982422, "Mean Q2": 385.5267490234375, "critic_loss": 2.4397943658828734, "batch_reward": 3.4469609375, "actor_loss": -385.807435546875, "actor_target_entropy": -2.0, "actor_entropy": -0.3683741000890732, "alpha_loss": 0.00047247067908756435, "alpha_value": 0.012520476229022954, "duration": 82.26243138313293, "step": 97750}
{"episode_reward": 946.0, "episode": 392.0, "Q1 loss": 1.284334290266037, "Q2 loss": 1.2776563413143158, "Mean Target Q": 385.61766064453127, "Mean Q1": 385.61625939941405, "Mean Q2": 385.616349609375, "critic_loss": 2.561990636348724, "batch_reward": 3.45515625, "actor_loss": -385.83067431640626, "actor_target_entropy": -2.0, "actor_entropy": -0.3397011340856552, "alpha_loss": 0.0014047539335442707, "alpha_value": 0.012477349682010135, "duration": 82.27817916870117, "step": 98000}
{"episode_reward": 992.0, "episode": 393.0, "Q1 loss": 1.4492411024570464, "Q2 loss": 1.4278969690799712, "Mean Target Q": 385.654845703125, "Mean Q1": 385.65467822265623, "Mean Q2": 385.65482666015623, "critic_loss": 2.877138066768646, "batch_reward": 3.451125, "actor_loss": -385.9480888671875, "actor_target_entropy": -2.0, "actor_entropy": -0.33685279804468155, "alpha_loss": 0.0007001023204065859, "alpha_value": 0.01240598333862371, "duration": 81.9617211818695, "step": 98250}
{"episode_reward": 958.0, "episode": 394.0, "Q1 loss": 1.2211114749908447, "Q2 loss": 1.2078865888118744, "Mean Target Q": 385.8176553955078, "Mean Q1": 385.81782055664064, "Mean Q2": 385.81681506347655, "critic_loss": 2.428998071193695, "batch_reward": 3.46059375, "actor_loss": -386.109263671875, "actor_target_entropy": -2.0, "actor_entropy": -0.4145707801580429, "alpha_loss": 0.0007311690978240221, "alpha_value": 0.01235217858154141, "duration": 81.1731321811676, "step": 98500}
{"episode_reward": 981.0, "episode": 395.0, "Q1 loss": 1.2588191688060761, "Q2 loss": 1.2543161599636077, "Mean Target Q": 385.7910057373047, "Mean Q1": 385.7893825683594, "Mean Q2": 385.7899893798828, "critic_loss": 2.513135322570801, "batch_reward": 3.4549453125, "actor_loss": -386.1541062011719, "actor_target_entropy": -2.0, "actor_entropy": -0.5102981052398682, "alpha_loss": 0.00012192858033813536, "alpha_value": 0.012327009304176994, "duration": 81.54274392127991, "step": 98750}
{"episode_reward": 979.0, "episode": 396.0, "Q1 loss": 1.435243273973465, "Q2 loss": 1.3895580399036407, "Mean Target Q": 385.88449291992185, "Mean Q1": 385.8850517578125, "Mean Q2": 385.88463781738284, "critic_loss": 2.8248013072013856, "batch_reward": 3.4588515625, "actor_loss": -386.23494555664064, "actor_target_entropy": -2.0, "actor_entropy": -0.44341600096225736, "alpha_loss": 0.0003632954047061503, "alpha_value": 0.012307833728938017, "duration": 82.82234811782837, "step": 99000}
{"episode_reward": 956.0, "episode": 397.0, "Q1 loss": 1.3804701565504074, "Q2 loss": 1.3772896139621735, "Mean Target Q": 385.96437939453125, "Mean Q1": 385.9639588623047, "Mean Q2": 385.9646649169922, "critic_loss": 2.7577597808837893, "batch_reward": 3.4612578125, "actor_loss": -386.2968266601562, "actor_target_entropy": -2.0, "actor_entropy": -0.4078601967692375, "alpha_loss": -0.00028998761461116373, "alpha_value": 0.012295315382487315, "duration": 81.6566789150238, "step": 99250}
{"episode_reward": 902.0, "episode": 398.0, "Q1 loss": 1.445712267637253, "Q2 loss": 1.4390834538936614, "Mean Target Q": 385.98534936523436, "Mean Q1": 385.9837912597656, "Mean Q2": 385.9835529785156, "critic_loss": 2.8847957158088686, "batch_reward": 3.460734375, "actor_loss": -386.2944401855469, "actor_target_entropy": -2.0, "actor_entropy": -0.45045597225427625, "alpha_loss": 0.00032552284142002466, "alpha_value": 0.012308580800963836, "duration": 80.83826613426208, "step": 99500}
{"episode_reward": 964.0, "episode": 399.0, "Q1 loss": 1.373252872467041, "Q2 loss": 1.373515795469284, "Mean Target Q": 386.0247283935547, "Mean Q1": 386.0251397705078, "Mean Q2": 386.0251800537109, "critic_loss": 2.7467686643600464, "batch_reward": 3.459890625, "actor_loss": -386.3820749511719, "actor_target_entropy": -2.0, "actor_entropy": -0.4011952426433563, "alpha_loss": -0.0007299542889231816, "alpha_value": 0.012320602959692659, "duration": 88.16958332061768, "step": 99750}
{"episode_reward": 989.0, "episode": 400.0, "Q1 loss": 1.3056389392619152, "Q2 loss": 1.2767806354775486, "Mean Target Q": 386.07230778583084, "Mean Q1": 386.07135953481895, "Mean Q2": 386.07167905018514, "critic_loss": 2.58241957497884, "batch_reward": 3.460890436746988, "actor_loss": -386.382376953125, "actor_target_entropy": -2.0, "actor_entropy": -0.426771165959537, "alpha_loss": 0.0002741116608958691, "alpha_value": 0.01232130443986887, "step": 99999}
